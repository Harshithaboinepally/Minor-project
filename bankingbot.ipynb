{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UtJnnZ8KZ0tq",
        "outputId": "0ec4ecf8-be5e-4e4e-dcfb-591aa5b71308"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[31mERROR: Could not find a version that satisfies the requirement tensorflow==2.1.0 (from versions: 2.8.0rc0, 2.8.0rc1, 2.8.0, 2.8.1, 2.8.2, 2.8.3, 2.8.4, 2.9.0rc0, 2.9.0rc1, 2.9.0rc2, 2.9.0, 2.9.1, 2.9.2, 2.9.3, 2.10.0rc0, 2.10.0rc1, 2.10.0rc2, 2.10.0rc3, 2.10.0, 2.10.1, 2.11.0rc0, 2.11.0rc1, 2.11.0rc2, 2.11.0, 2.11.1, 2.12.0rc0, 2.12.0rc1, 2.12.0, 2.12.1, 2.13.0rc0, 2.13.0rc1, 2.13.0rc2, 2.13.0, 2.13.1, 2.14.0rc0, 2.14.0rc1, 2.14.0)\u001b[0m\u001b[31m\n",
            "\u001b[0m\u001b[31mERROR: No matching distribution found for tensorflow==2.1.0\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ],
      "source": [
        "!pip install tensorflow==2.1.0"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bpiv9H1pctBi"
      },
      "source": [
        "Importing libraries"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install tflearn"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vYErM8cVBp8S",
        "outputId": "ff1a7f36-b441-4388-fc6c-d683a6af8d88"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting tflearn\n",
            "  Downloading tflearn-0.5.0.tar.gz (107 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m107.3/107.3 kB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from tflearn) (1.23.5)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from tflearn) (1.16.0)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.10/dist-packages (from tflearn) (9.4.0)\n",
            "Building wheels for collected packages: tflearn\n",
            "  Building wheel for tflearn (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for tflearn: filename=tflearn-0.5.0-py3-none-any.whl size=127283 sha256=46efbeb0abeca173b7f5af5625d7f86440309706920b508c64bc04b7699155d0\n",
            "  Stored in directory: /root/.cache/pip/wheels/55/fb/7b/e06204a0ceefa45443930b9a250cb5ebe31def0e4e8245a465\n",
            "Successfully built tflearn\n",
            "Installing collected packages: tflearn\n",
            "Successfully installed tflearn-0.5.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 546
        },
        "id": "L2_-IIsXZw41",
        "outputId": "f2c4f9d6-b634-4b60-b782-f5c119c4018b"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "ImportError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-9-75bc011c6ad0>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mtflearn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mv1\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mrandom\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tflearn/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;31m# Predefined ops\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnormalization\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmetrics\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mactivations\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tflearn/layers/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mnormalization\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mbatch_normalization\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlocal_response_normalization\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mestimator\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mregression\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mrecurrent\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mlstm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgru\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msimple_rnn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbidirectional_rnn\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m     \u001b[0mBasicRNNCell\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mBasicLSTMCell\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mGRUCell\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0membedding_ops\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0membedding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tflearn/layers/recurrent.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontrib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mops\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcore_rnn_cell\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutil\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnest\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mis_sequence\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mImportError\u001b[0m: cannot import name 'is_sequence' from 'tensorflow.python.util.nest' (/usr/local/lib/python3.10/dist-packages/tensorflow/python/util/nest.py)",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ],
      "source": [
        "import os\n",
        "import tflearn\n",
        "import tensorflow.compat.v1 as tf\n",
        "import numpy\n",
        "import random\n",
        "\n",
        "import nltk\n",
        "from nltk.stem.lancaster import LancasterStemmer\n",
        "stemmer = LancasterStemmer()\n",
        "\n",
        "import json\n",
        "with open('/content/drive/MyDrive/Colab Notebooks/Bankbot/intents.json') as file:\n",
        "    data = json.load(file)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zVclt2Crb-ja"
      },
      "outputs": [],
      "source": [
        "nltk.download('punkt')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "testing_data = []\n",
        "for intent in data['intents']:\n",
        "    for pattern in intent['patterns']:\n",
        "        testing_data.append((pattern, intent['tag']))"
      ],
      "metadata": {
        "id": "P1ZDTZBCAAVe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4GoWIDdwcxEz"
      },
      "source": [
        "Extracting data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rXNmsooobugl"
      },
      "outputs": [],
      "source": [
        "words = []\n",
        "labels = []\n",
        "docs_x = []\n",
        "docs_y = []\n",
        "\n",
        "for intent in data['intents']:\n",
        "    for pattern in intent['patterns']:\n",
        "        w = nltk.word_tokenize(pattern)\n",
        "        words.extend(w)\n",
        "        docs_x.append(w)\n",
        "        docs_y.append(intent[\"tag\"])\n",
        "\n",
        "    if intent['tag'] not in labels:\n",
        "        labels.append(intent['tag'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vCoi-OFFc3eZ"
      },
      "source": [
        "Stemming words"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E-XmT7phcN2c"
      },
      "outputs": [],
      "source": [
        "words = [stemmer.stem(w.lower()) for w in words if w != \"?\"]\n",
        "words = sorted(list(set(words)))\n",
        "\n",
        "labels = sorted(labels)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "93mcp3Kcc7_Q"
      },
      "outputs": [],
      "source": [
        "training = []\n",
        "output = []\n",
        "\n",
        "out_empty = [0 for _ in range(len(labels))]\n",
        "\n",
        "for x, doc in enumerate(docs_x):\n",
        "    bag = []\n",
        "\n",
        "    wrds = [stemmer.stem(w.lower()) for w in doc]\n",
        "\n",
        "    for w in words:\n",
        "        if w in wrds:\n",
        "            bag.append(1)\n",
        "        else:\n",
        "            bag.append(0)\n",
        "\n",
        "    output_row = out_empty[:]\n",
        "    output_row[labels.index(docs_y[x])] = 1\n",
        "\n",
        "    training.append(bag)\n",
        "    output.append(output_row)\n",
        "\n",
        "training = numpy.array(training)\n",
        "output = numpy.array(output)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lOhRryAAdMiI"
      },
      "source": [
        "Developing model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ztZ-VxHHc-Z8",
        "outputId": "d5a79db0-5d0a-4080-a175-e397155f8828"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.10/dist-packages/tflearn/initializations.py:164: calling TruncatedNormal.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Call initializer instance with the dtype argument instead of passing it to the constructor\n"
          ]
        }
      ],
      "source": [
        "tensorflow.compat.v1.reset_default_graph()\n",
        "\n",
        "net = tflearn.input_data(shape=[None, len(training[0])])\n",
        "net = tflearn.fully_connected(net, 8)\n",
        "net = tflearn.fully_connected(net, 8)\n",
        "net = tflearn.fully_connected(net, len(output[0]), activation=\"softmax\")\n",
        "net = tflearn.regression(net)\n",
        "\n",
        "model = tflearn.DNN(net)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XWRic53sdmLz"
      },
      "source": [
        "Training model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "gWYgn3tZKMVg",
        "outputId": "7d07f35a-be52-4f7c-8fc0-0210345b2d6f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1;30;43mStreaming output truncated to the last 5000 lines.\u001b[0m\n",
            "Training Step: 1290  | total loss: \u001b[1m\u001b[32m0.23469\u001b[0m\u001b[0m | time: 0.026s\n",
            "| Adam | epoch: 258 | loss: 0.23469 - acc: 1.0000 -- iter: 36/36\n",
            "--\n",
            "Training Step: 1291  | total loss: \u001b[1m\u001b[32m0.23469\u001b[0m\u001b[0m | time: 0.004s\n",
            "| Adam | epoch: 259 | loss: 0.23469 - acc: 1.0000 -- iter: 08/36\n",
            "Training Step: 1292  | total loss: \u001b[1m\u001b[32m0.23237\u001b[0m\u001b[0m | time: 0.008s\n",
            "| Adam | epoch: 259 | loss: 0.23237 - acc: 1.0000 -- iter: 16/36\n",
            "Training Step: 1293  | total loss: \u001b[1m\u001b[32m0.23543\u001b[0m\u001b[0m | time: 0.011s\n",
            "| Adam | epoch: 259 | loss: 0.23543 - acc: 1.0000 -- iter: 24/36\n",
            "Training Step: 1294  | total loss: \u001b[1m\u001b[32m0.23518\u001b[0m\u001b[0m | time: 0.016s\n",
            "| Adam | epoch: 259 | loss: 0.23518 - acc: 1.0000 -- iter: 32/36\n",
            "Training Step: 1295  | total loss: \u001b[1m\u001b[32m0.23117\u001b[0m\u001b[0m | time: 0.023s\n",
            "| Adam | epoch: 259 | loss: 0.23117 - acc: 1.0000 -- iter: 36/36\n",
            "--\n",
            "Training Step: 1296  | total loss: \u001b[1m\u001b[32m0.22745\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 260 | loss: 0.22745 - acc: 1.0000 -- iter: 08/36\n",
            "Training Step: 1297  | total loss: \u001b[1m\u001b[32m0.23270\u001b[0m\u001b[0m | time: 0.010s\n",
            "| Adam | epoch: 260 | loss: 0.23270 - acc: 1.0000 -- iter: 16/36\n",
            "Training Step: 1298  | total loss: \u001b[1m\u001b[32m0.22487\u001b[0m\u001b[0m | time: 0.014s\n",
            "| Adam | epoch: 260 | loss: 0.22487 - acc: 1.0000 -- iter: 24/36\n",
            "Training Step: 1299  | total loss: \u001b[1m\u001b[32m0.23112\u001b[0m\u001b[0m | time: 0.019s\n",
            "| Adam | epoch: 260 | loss: 0.23112 - acc: 1.0000 -- iter: 32/36\n",
            "Training Step: 1300  | total loss: \u001b[1m\u001b[32m0.24578\u001b[0m\u001b[0m | time: 0.024s\n",
            "| Adam | epoch: 260 | loss: 0.24578 - acc: 1.0000 -- iter: 36/36\n",
            "--\n",
            "Training Step: 1301  | total loss: \u001b[1m\u001b[32m0.23363\u001b[0m\u001b[0m | time: 0.004s\n",
            "| Adam | epoch: 261 | loss: 0.23363 - acc: 1.0000 -- iter: 08/36\n",
            "Training Step: 1302  | total loss: \u001b[1m\u001b[32m0.22263\u001b[0m\u001b[0m | time: 0.007s\n",
            "| Adam | epoch: 261 | loss: 0.22263 - acc: 1.0000 -- iter: 16/36\n",
            "Training Step: 1303  | total loss: \u001b[1m\u001b[32m0.22444\u001b[0m\u001b[0m | time: 0.014s\n",
            "| Adam | epoch: 261 | loss: 0.22444 - acc: 1.0000 -- iter: 24/36\n",
            "Training Step: 1304  | total loss: \u001b[1m\u001b[32m0.21748\u001b[0m\u001b[0m | time: 0.021s\n",
            "| Adam | epoch: 261 | loss: 0.21748 - acc: 1.0000 -- iter: 32/36\n",
            "Training Step: 1305  | total loss: \u001b[1m\u001b[32m0.21610\u001b[0m\u001b[0m | time: 0.028s\n",
            "| Adam | epoch: 261 | loss: 0.21610 - acc: 1.0000 -- iter: 36/36\n",
            "--\n",
            "Training Step: 1306  | total loss: \u001b[1m\u001b[32m0.22253\u001b[0m\u001b[0m | time: 0.007s\n",
            "| Adam | epoch: 262 | loss: 0.22253 - acc: 1.0000 -- iter: 08/36\n",
            "Training Step: 1307  | total loss: \u001b[1m\u001b[32m0.22160\u001b[0m\u001b[0m | time: 0.011s\n",
            "| Adam | epoch: 262 | loss: 0.22160 - acc: 1.0000 -- iter: 16/36\n",
            "Training Step: 1308  | total loss: \u001b[1m\u001b[32m0.22049\u001b[0m\u001b[0m | time: 0.015s\n",
            "| Adam | epoch: 262 | loss: 0.22049 - acc: 1.0000 -- iter: 24/36\n",
            "Training Step: 1309  | total loss: \u001b[1m\u001b[32m0.22545\u001b[0m\u001b[0m | time: 0.019s\n",
            "| Adam | epoch: 262 | loss: 0.22545 - acc: 1.0000 -- iter: 32/36\n",
            "Training Step: 1310  | total loss: \u001b[1m\u001b[32m0.21933\u001b[0m\u001b[0m | time: 0.023s\n",
            "| Adam | epoch: 262 | loss: 0.21933 - acc: 1.0000 -- iter: 36/36\n",
            "--\n",
            "Training Step: 1311  | total loss: \u001b[1m\u001b[32m0.21847\u001b[0m\u001b[0m | time: 0.004s\n",
            "| Adam | epoch: 263 | loss: 0.21847 - acc: 1.0000 -- iter: 08/36\n",
            "Training Step: 1312  | total loss: \u001b[1m\u001b[32m0.22051\u001b[0m\u001b[0m | time: 0.008s\n",
            "| Adam | epoch: 263 | loss: 0.22051 - acc: 1.0000 -- iter: 16/36\n",
            "Training Step: 1313  | total loss: \u001b[1m\u001b[32m0.21912\u001b[0m\u001b[0m | time: 0.014s\n",
            "| Adam | epoch: 263 | loss: 0.21912 - acc: 1.0000 -- iter: 24/36\n",
            "Training Step: 1314  | total loss: \u001b[1m\u001b[32m0.21912\u001b[0m\u001b[0m | time: 0.017s\n",
            "| Adam | epoch: 263 | loss: 0.21912 - acc: 1.0000 -- iter: 32/36\n",
            "Training Step: 1315  | total loss: \u001b[1m\u001b[32m0.21293\u001b[0m\u001b[0m | time: 0.022s\n",
            "| Adam | epoch: 263 | loss: 0.21293 - acc: 1.0000 -- iter: 36/36\n",
            "--\n",
            "Training Step: 1316  | total loss: \u001b[1m\u001b[32m0.21758\u001b[0m\u001b[0m | time: 0.004s\n",
            "| Adam | epoch: 264 | loss: 0.21758 - acc: 1.0000 -- iter: 08/36\n",
            "Training Step: 1317  | total loss: \u001b[1m\u001b[32m0.21975\u001b[0m\u001b[0m | time: 0.008s\n",
            "| Adam | epoch: 264 | loss: 0.21975 - acc: 1.0000 -- iter: 16/36\n",
            "Training Step: 1318  | total loss: \u001b[1m\u001b[32m0.23400\u001b[0m\u001b[0m | time: 0.012s\n",
            "| Adam | epoch: 264 | loss: 0.23400 - acc: 1.0000 -- iter: 24/36\n",
            "Training Step: 1319  | total loss: \u001b[1m\u001b[32m0.22598\u001b[0m\u001b[0m | time: 0.016s\n",
            "| Adam | epoch: 264 | loss: 0.22598 - acc: 1.0000 -- iter: 32/36\n",
            "Training Step: 1320  | total loss: \u001b[1m\u001b[32m0.21869\u001b[0m\u001b[0m | time: 0.021s\n",
            "| Adam | epoch: 264 | loss: 0.21869 - acc: 1.0000 -- iter: 36/36\n",
            "--\n",
            "Training Step: 1321  | total loss: \u001b[1m\u001b[32m0.21733\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 265 | loss: 0.21733 - acc: 1.0000 -- iter: 08/36\n",
            "Training Step: 1322  | total loss: \u001b[1m\u001b[32m0.21880\u001b[0m\u001b[0m | time: 0.009s\n",
            "| Adam | epoch: 265 | loss: 0.21880 - acc: 1.0000 -- iter: 16/36\n",
            "Training Step: 1323  | total loss: \u001b[1m\u001b[32m0.21880\u001b[0m\u001b[0m | time: 0.013s\n",
            "| Adam | epoch: 265 | loss: 0.21880 - acc: 1.0000 -- iter: 24/36\n",
            "Training Step: 1324  | total loss: \u001b[1m\u001b[32m0.20921\u001b[0m\u001b[0m | time: 0.018s\n",
            "| Adam | epoch: 265 | loss: 0.20921 - acc: 1.0000 -- iter: 32/36\n",
            "Training Step: 1325  | total loss: \u001b[1m\u001b[32m0.21854\u001b[0m\u001b[0m | time: 0.021s\n",
            "| Adam | epoch: 265 | loss: 0.21854 - acc: 1.0000 -- iter: 36/36\n",
            "--\n",
            "Training Step: 1326  | total loss: \u001b[1m\u001b[32m0.22436\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 266 | loss: 0.22436 - acc: 1.0000 -- iter: 08/36\n",
            "Training Step: 1327  | total loss: \u001b[1m\u001b[32m0.21549\u001b[0m\u001b[0m | time: 0.010s\n",
            "| Adam | epoch: 266 | loss: 0.21549 - acc: 1.0000 -- iter: 16/36\n",
            "Training Step: 1328  | total loss: \u001b[1m\u001b[32m0.21558\u001b[0m\u001b[0m | time: 0.013s\n",
            "| Adam | epoch: 266 | loss: 0.21558 - acc: 1.0000 -- iter: 24/36\n",
            "Training Step: 1329  | total loss: \u001b[1m\u001b[32m0.21970\u001b[0m\u001b[0m | time: 0.019s\n",
            "| Adam | epoch: 266 | loss: 0.21970 - acc: 1.0000 -- iter: 32/36\n",
            "Training Step: 1330  | total loss: \u001b[1m\u001b[32m0.21656\u001b[0m\u001b[0m | time: 0.027s\n",
            "| Adam | epoch: 266 | loss: 0.21656 - acc: 1.0000 -- iter: 36/36\n",
            "--\n",
            "Training Step: 1331  | total loss: \u001b[1m\u001b[32m0.21656\u001b[0m\u001b[0m | time: 0.004s\n",
            "| Adam | epoch: 267 | loss: 0.21656 - acc: 1.0000 -- iter: 08/36\n",
            "Training Step: 1332  | total loss: \u001b[1m\u001b[32m0.21773\u001b[0m\u001b[0m | time: 0.011s\n",
            "| Adam | epoch: 267 | loss: 0.21773 - acc: 1.0000 -- iter: 16/36\n",
            "Training Step: 1333  | total loss: \u001b[1m\u001b[32m0.21873\u001b[0m\u001b[0m | time: 0.017s\n",
            "| Adam | epoch: 267 | loss: 0.21873 - acc: 1.0000 -- iter: 24/36\n",
            "Training Step: 1334  | total loss: \u001b[1m\u001b[32m0.21688\u001b[0m\u001b[0m | time: 0.023s\n",
            "| Adam | epoch: 267 | loss: 0.21688 - acc: 1.0000 -- iter: 32/36\n",
            "Training Step: 1335  | total loss: \u001b[1m\u001b[32m0.21963\u001b[0m\u001b[0m | time: 0.026s\n",
            "| Adam | epoch: 267 | loss: 0.21963 - acc: 1.0000 -- iter: 36/36\n",
            "--\n",
            "Training Step: 1336  | total loss: \u001b[1m\u001b[32m0.21963\u001b[0m\u001b[0m | time: 0.007s\n",
            "| Adam | epoch: 268 | loss: 0.21963 - acc: 1.0000 -- iter: 08/36\n",
            "Training Step: 1337  | total loss: \u001b[1m\u001b[32m0.21524\u001b[0m\u001b[0m | time: 0.014s\n",
            "| Adam | epoch: 268 | loss: 0.21524 - acc: 1.0000 -- iter: 16/36\n",
            "Training Step: 1338  | total loss: \u001b[1m\u001b[32m0.21524\u001b[0m\u001b[0m | time: 0.020s\n",
            "| Adam | epoch: 268 | loss: 0.21524 - acc: 1.0000 -- iter: 24/36\n",
            "Training Step: 1339  | total loss: \u001b[1m\u001b[32m0.20881\u001b[0m\u001b[0m | time: 0.027s\n",
            "| Adam | epoch: 268 | loss: 0.20881 - acc: 1.0000 -- iter: 32/36\n",
            "Training Step: 1340  | total loss: \u001b[1m\u001b[32m0.20881\u001b[0m\u001b[0m | time: 0.034s\n",
            "| Adam | epoch: 268 | loss: 0.20881 - acc: 1.0000 -- iter: 36/36\n",
            "--\n",
            "Training Step: 1341  | total loss: \u001b[1m\u001b[32m0.21074\u001b[0m\u001b[0m | time: 0.008s\n",
            "| Adam | epoch: 269 | loss: 0.21074 - acc: 1.0000 -- iter: 08/36\n",
            "Training Step: 1342  | total loss: \u001b[1m\u001b[32m0.21065\u001b[0m\u001b[0m | time: 0.014s\n",
            "| Adam | epoch: 269 | loss: 0.21065 - acc: 1.0000 -- iter: 16/36\n",
            "Training Step: 1343  | total loss: \u001b[1m\u001b[32m0.20946\u001b[0m\u001b[0m | time: 0.020s\n",
            "| Adam | epoch: 269 | loss: 0.20946 - acc: 1.0000 -- iter: 24/36\n",
            "Training Step: 1344  | total loss: \u001b[1m\u001b[32m0.20967\u001b[0m\u001b[0m | time: 0.026s\n",
            "| Adam | epoch: 269 | loss: 0.20967 - acc: 1.0000 -- iter: 32/36\n",
            "Training Step: 1345  | total loss: \u001b[1m\u001b[32m0.20978\u001b[0m\u001b[0m | time: 0.033s\n",
            "| Adam | epoch: 269 | loss: 0.20978 - acc: 1.0000 -- iter: 36/36\n",
            "--\n",
            "Training Step: 1346  | total loss: \u001b[1m\u001b[32m0.21302\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 270 | loss: 0.21302 - acc: 1.0000 -- iter: 08/36\n",
            "Training Step: 1347  | total loss: \u001b[1m\u001b[32m0.21302\u001b[0m\u001b[0m | time: 0.010s\n",
            "| Adam | epoch: 270 | loss: 0.21302 - acc: 1.0000 -- iter: 16/36\n",
            "Training Step: 1348  | total loss: \u001b[1m\u001b[32m1.09334\u001b[0m\u001b[0m | time: 0.013s\n",
            "| Adam | epoch: 270 | loss: 1.09334 - acc: 0.9125 -- iter: 24/36\n",
            "Training Step: 1349  | total loss: \u001b[1m\u001b[32m1.09334\u001b[0m\u001b[0m | time: 0.019s\n",
            "| Adam | epoch: 270 | loss: 1.09334 - acc: 0.9212 -- iter: 32/36\n",
            "Training Step: 1350  | total loss: \u001b[1m\u001b[32m0.92910\u001b[0m\u001b[0m | time: 0.026s\n",
            "| Adam | epoch: 270 | loss: 0.92910 - acc: 0.9362 -- iter: 36/36\n",
            "--\n",
            "Training Step: 1351  | total loss: \u001b[1m\u001b[32m0.92910\u001b[0m\u001b[0m | time: 0.007s\n",
            "| Adam | epoch: 271 | loss: 0.92910 - acc: 0.9362 -- iter: 08/36\n",
            "Training Step: 1352  | total loss: \u001b[1m\u001b[32m0.79075\u001b[0m\u001b[0m | time: 0.013s\n",
            "| Adam | epoch: 271 | loss: 0.79075 - acc: 0.9426 -- iter: 16/36\n",
            "Training Step: 1353  | total loss: \u001b[1m\u001b[32m0.79075\u001b[0m\u001b[0m | time: 0.020s\n",
            "| Adam | epoch: 271 | loss: 0.79075 - acc: 0.9483 -- iter: 24/36\n",
            "Training Step: 1354  | total loss: \u001b[1m\u001b[32m0.68253\u001b[0m\u001b[0m | time: 0.024s\n",
            "| Adam | epoch: 271 | loss: 0.68253 - acc: 0.9535 -- iter: 32/36\n",
            "Training Step: 1355  | total loss: \u001b[1m\u001b[32m0.68253\u001b[0m\u001b[0m | time: 0.030s\n",
            "| Adam | epoch: 271 | loss: 0.68253 - acc: 0.9581 -- iter: 36/36\n",
            "--\n",
            "Training Step: 1356  | total loss: \u001b[1m\u001b[32m0.60059\u001b[0m\u001b[0m | time: 0.006s\n",
            "| Adam | epoch: 272 | loss: 0.60059 - acc: 0.9623 -- iter: 08/36\n",
            "Training Step: 1357  | total loss: \u001b[1m\u001b[32m0.60059\u001b[0m\u001b[0m | time: 0.014s\n",
            "| Adam | epoch: 272 | loss: 0.60059 - acc: 0.9661 -- iter: 16/36\n",
            "Training Step: 1358  | total loss: \u001b[1m\u001b[32m0.55882\u001b[0m\u001b[0m | time: 0.018s\n",
            "| Adam | epoch: 272 | loss: 0.55882 - acc: 0.9695 -- iter: 24/36\n",
            "Training Step: 1359  | total loss: \u001b[1m\u001b[32m0.49221\u001b[0m\u001b[0m | time: 0.026s\n",
            "| Adam | epoch: 272 | loss: 0.49221 - acc: 0.9725 -- iter: 32/36\n",
            "Training Step: 1360  | total loss: \u001b[1m\u001b[32m0.49221\u001b[0m\u001b[0m | time: 0.032s\n",
            "| Adam | epoch: 272 | loss: 0.49221 - acc: 0.9753 -- iter: 36/36\n",
            "--\n",
            "Training Step: 1361  | total loss: \u001b[1m\u001b[32m0.46197\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 273 | loss: 0.46197 - acc: 0.9778 -- iter: 08/36\n",
            "Training Step: 1362  | total loss: \u001b[1m\u001b[32m0.41558\u001b[0m\u001b[0m | time: 0.011s\n",
            "| Adam | epoch: 273 | loss: 0.41558 - acc: 0.9800 -- iter: 16/36\n",
            "Training Step: 1363  | total loss: \u001b[1m\u001b[32m0.41558\u001b[0m\u001b[0m | time: 0.017s\n",
            "| Adam | epoch: 273 | loss: 0.41558 - acc: 0.9820 -- iter: 24/36\n",
            "Training Step: 1364  | total loss: \u001b[1m\u001b[32m0.37002\u001b[0m\u001b[0m | time: 0.022s\n",
            "| Adam | epoch: 273 | loss: 0.37002 - acc: 0.9838 -- iter: 32/36\n",
            "Training Step: 1365  | total loss: \u001b[1m\u001b[32m0.36129\u001b[0m\u001b[0m | time: 0.029s\n",
            "| Adam | epoch: 273 | loss: 0.36129 - acc: 0.9854 -- iter: 36/36\n",
            "--\n",
            "Training Step: 1366  | total loss: \u001b[1m\u001b[32m0.34826\u001b[0m\u001b[0m | time: 0.007s\n",
            "| Adam | epoch: 274 | loss: 0.34826 - acc: 0.9869 -- iter: 08/36\n",
            "Training Step: 1367  | total loss: \u001b[1m\u001b[32m0.33177\u001b[0m\u001b[0m | time: 0.012s\n",
            "| Adam | epoch: 274 | loss: 0.33177 - acc: 0.9882 -- iter: 16/36\n",
            "Training Step: 1368  | total loss: \u001b[1m\u001b[32m0.31682\u001b[0m\u001b[0m | time: 0.020s\n",
            "| Adam | epoch: 274 | loss: 0.31682 - acc: 0.9894 -- iter: 24/36\n",
            "Training Step: 1369  | total loss: \u001b[1m\u001b[32m0.30956\u001b[0m\u001b[0m | time: 0.025s\n",
            "| Adam | epoch: 274 | loss: 0.30956 - acc: 0.9904 -- iter: 32/36\n",
            "Training Step: 1370  | total loss: \u001b[1m\u001b[32m0.29878\u001b[0m\u001b[0m | time: 0.033s\n",
            "| Adam | epoch: 274 | loss: 0.29878 - acc: 0.9914 -- iter: 36/36\n",
            "--\n",
            "Training Step: 1371  | total loss: \u001b[1m\u001b[32m0.28479\u001b[0m\u001b[0m | time: 0.004s\n",
            "| Adam | epoch: 275 | loss: 0.28479 - acc: 0.9922 -- iter: 08/36\n",
            "Training Step: 1372  | total loss: \u001b[1m\u001b[32m0.27616\u001b[0m\u001b[0m | time: 0.010s\n",
            "| Adam | epoch: 275 | loss: 0.27616 - acc: 0.9930 -- iter: 16/36\n",
            "Training Step: 1373  | total loss: \u001b[1m\u001b[32m0.26627\u001b[0m\u001b[0m | time: 0.015s\n",
            "| Adam | epoch: 275 | loss: 0.26627 - acc: 0.9937 -- iter: 24/36\n",
            "Training Step: 1374  | total loss: \u001b[1m\u001b[32m0.25725\u001b[0m\u001b[0m | time: 0.021s\n",
            "| Adam | epoch: 275 | loss: 0.25725 - acc: 0.9943 -- iter: 32/36\n",
            "Training Step: 1375  | total loss: \u001b[1m\u001b[32m0.25446\u001b[0m\u001b[0m | time: 0.026s\n",
            "| Adam | epoch: 275 | loss: 0.25446 - acc: 0.9949 -- iter: 36/36\n",
            "--\n",
            "Training Step: 1376  | total loss: \u001b[1m\u001b[32m0.24518\u001b[0m\u001b[0m | time: 0.004s\n",
            "| Adam | epoch: 276 | loss: 0.24518 - acc: 0.9954 -- iter: 08/36\n",
            "Training Step: 1377  | total loss: \u001b[1m\u001b[32m0.24444\u001b[0m\u001b[0m | time: 0.011s\n",
            "| Adam | epoch: 276 | loss: 0.24444 - acc: 0.9959 -- iter: 16/36\n",
            "Training Step: 1378  | total loss: \u001b[1m\u001b[32m0.23469\u001b[0m\u001b[0m | time: 0.019s\n",
            "| Adam | epoch: 276 | loss: 0.23469 - acc: 0.9963 -- iter: 24/36\n",
            "Training Step: 1379  | total loss: \u001b[1m\u001b[32m0.23380\u001b[0m\u001b[0m | time: 0.024s\n",
            "| Adam | epoch: 276 | loss: 0.23380 - acc: 0.9967 -- iter: 32/36\n",
            "Training Step: 1380  | total loss: \u001b[1m\u001b[32m0.23380\u001b[0m\u001b[0m | time: 0.029s\n",
            "| Adam | epoch: 276 | loss: 0.23380 - acc: 0.9970 -- iter: 36/36\n",
            "--\n",
            "Training Step: 1381  | total loss: \u001b[1m\u001b[32m0.23290\u001b[0m\u001b[0m | time: 0.004s\n",
            "| Adam | epoch: 277 | loss: 0.23290 - acc: 0.9973 -- iter: 08/36\n",
            "Training Step: 1382  | total loss: \u001b[1m\u001b[32m0.22676\u001b[0m\u001b[0m | time: 0.007s\n",
            "| Adam | epoch: 277 | loss: 0.22676 - acc: 0.9976 -- iter: 16/36\n",
            "Training Step: 1383  | total loss: \u001b[1m\u001b[32m0.22391\u001b[0m\u001b[0m | time: 0.012s\n",
            "| Adam | epoch: 277 | loss: 0.22391 - acc: 0.9978 -- iter: 24/36\n",
            "Training Step: 1384  | total loss: \u001b[1m\u001b[32m0.22867\u001b[0m\u001b[0m | time: 0.018s\n",
            "| Adam | epoch: 277 | loss: 0.22867 - acc: 0.9980 -- iter: 32/36\n",
            "Training Step: 1385  | total loss: \u001b[1m\u001b[32m0.23239\u001b[0m\u001b[0m | time: 0.022s\n",
            "| Adam | epoch: 277 | loss: 0.23239 - acc: 0.9982 -- iter: 36/36\n",
            "--\n",
            "Training Step: 1386  | total loss: \u001b[1m\u001b[32m0.23693\u001b[0m\u001b[0m | time: 0.006s\n",
            "| Adam | epoch: 278 | loss: 0.23693 - acc: 0.9984 -- iter: 08/36\n",
            "Training Step: 1387  | total loss: \u001b[1m\u001b[32m0.23288\u001b[0m\u001b[0m | time: 0.012s\n",
            "| Adam | epoch: 278 | loss: 0.23288 - acc: 0.9986 -- iter: 16/36\n",
            "Training Step: 1388  | total loss: \u001b[1m\u001b[32m0.22468\u001b[0m\u001b[0m | time: 0.017s\n",
            "| Adam | epoch: 278 | loss: 0.22468 - acc: 0.9987 -- iter: 24/36\n",
            "Training Step: 1389  | total loss: \u001b[1m\u001b[32m0.22114\u001b[0m\u001b[0m | time: 0.022s\n",
            "| Adam | epoch: 278 | loss: 0.22114 - acc: 0.9988 -- iter: 32/36\n",
            "Training Step: 1390  | total loss: \u001b[1m\u001b[32m0.21627\u001b[0m\u001b[0m | time: 0.028s\n",
            "| Adam | epoch: 278 | loss: 0.21627 - acc: 0.9990 -- iter: 36/36\n",
            "--\n",
            "Training Step: 1391  | total loss: \u001b[1m\u001b[32m0.20867\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 279 | loss: 0.20867 - acc: 0.9991 -- iter: 08/36\n",
            "Training Step: 1392  | total loss: \u001b[1m\u001b[32m0.20174\u001b[0m\u001b[0m | time: 0.010s\n",
            "| Adam | epoch: 279 | loss: 0.20174 - acc: 0.9992 -- iter: 16/36\n",
            "Training Step: 1393  | total loss: \u001b[1m\u001b[32m0.20590\u001b[0m\u001b[0m | time: 0.014s\n",
            "| Adam | epoch: 279 | loss: 0.20590 - acc: 0.9992 -- iter: 24/36\n",
            "Training Step: 1394  | total loss: \u001b[1m\u001b[32m0.19989\u001b[0m\u001b[0m | time: 0.018s\n",
            "| Adam | epoch: 279 | loss: 0.19989 - acc: 0.9993 -- iter: 32/36\n",
            "Training Step: 1395  | total loss: \u001b[1m\u001b[32m0.20419\u001b[0m\u001b[0m | time: 0.023s\n",
            "| Adam | epoch: 279 | loss: 0.20419 - acc: 0.9994 -- iter: 36/36\n",
            "--\n",
            "Training Step: 1396  | total loss: \u001b[1m\u001b[32m0.20692\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 280 | loss: 0.20692 - acc: 0.9994 -- iter: 08/36\n",
            "Training Step: 1397  | total loss: \u001b[1m\u001b[32m0.20223\u001b[0m\u001b[0m | time: 0.009s\n",
            "| Adam | epoch: 280 | loss: 0.20223 - acc: 0.9995 -- iter: 16/36\n",
            "Training Step: 1398  | total loss: \u001b[1m\u001b[32m0.19787\u001b[0m\u001b[0m | time: 0.013s\n",
            "| Adam | epoch: 280 | loss: 0.19787 - acc: 0.9995 -- iter: 24/36\n",
            "Training Step: 1399  | total loss: \u001b[1m\u001b[32m0.19833\u001b[0m\u001b[0m | time: 0.018s\n",
            "| Adam | epoch: 280 | loss: 0.19833 - acc: 0.9996 -- iter: 32/36\n",
            "Training Step: 1400  | total loss: \u001b[1m\u001b[32m0.19766\u001b[0m\u001b[0m | time: 0.022s\n",
            "| Adam | epoch: 280 | loss: 0.19766 - acc: 0.9996 -- iter: 36/36\n",
            "--\n",
            "Training Step: 1401  | total loss: \u001b[1m\u001b[32m0.19369\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 281 | loss: 0.19369 - acc: 0.9997 -- iter: 08/36\n",
            "Training Step: 1402  | total loss: \u001b[1m\u001b[32m0.19601\u001b[0m\u001b[0m | time: 0.010s\n",
            "| Adam | epoch: 281 | loss: 0.19601 - acc: 0.9997 -- iter: 16/36\n",
            "Training Step: 1403  | total loss: \u001b[1m\u001b[32m0.19806\u001b[0m\u001b[0m | time: 0.014s\n",
            "| Adam | epoch: 281 | loss: 0.19806 - acc: 0.9997 -- iter: 24/36\n",
            "Training Step: 1404  | total loss: \u001b[1m\u001b[32m0.19982\u001b[0m\u001b[0m | time: 0.019s\n",
            "| Adam | epoch: 281 | loss: 0.19982 - acc: 0.9998 -- iter: 32/36\n",
            "Training Step: 1405  | total loss: \u001b[1m\u001b[32m0.19640\u001b[0m\u001b[0m | time: 0.023s\n",
            "| Adam | epoch: 281 | loss: 0.19640 - acc: 0.9998 -- iter: 36/36\n",
            "--\n",
            "Training Step: 1406  | total loss: \u001b[1m\u001b[32m0.19527\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 282 | loss: 0.19527 - acc: 0.9998 -- iter: 08/36\n",
            "Training Step: 1407  | total loss: \u001b[1m\u001b[32m0.19329\u001b[0m\u001b[0m | time: 0.009s\n",
            "| Adam | epoch: 282 | loss: 0.19329 - acc: 0.9998 -- iter: 16/36\n",
            "Training Step: 1408  | total loss: \u001b[1m\u001b[32m0.19865\u001b[0m\u001b[0m | time: 0.015s\n",
            "| Adam | epoch: 282 | loss: 0.19865 - acc: 0.9998 -- iter: 24/36\n",
            "Training Step: 1409  | total loss: \u001b[1m\u001b[32m0.20031\u001b[0m\u001b[0m | time: 0.019s\n",
            "| Adam | epoch: 282 | loss: 0.20031 - acc: 0.9999 -- iter: 32/36\n",
            "Training Step: 1410  | total loss: \u001b[1m\u001b[32m0.20168\u001b[0m\u001b[0m | time: 0.024s\n",
            "| Adam | epoch: 282 | loss: 0.20168 - acc: 0.9999 -- iter: 36/36\n",
            "--\n",
            "Training Step: 1411  | total loss: \u001b[1m\u001b[32m0.20260\u001b[0m\u001b[0m | time: 0.004s\n",
            "| Adam | epoch: 283 | loss: 0.20260 - acc: 0.9999 -- iter: 08/36\n",
            "Training Step: 1412  | total loss: \u001b[1m\u001b[32m0.19586\u001b[0m\u001b[0m | time: 0.009s\n",
            "| Adam | epoch: 283 | loss: 0.19586 - acc: 0.9999 -- iter: 16/36\n",
            "Training Step: 1413  | total loss: \u001b[1m\u001b[32m0.19045\u001b[0m\u001b[0m | time: 0.014s\n",
            "| Adam | epoch: 283 | loss: 0.19045 - acc: 0.9999 -- iter: 24/36\n",
            "Training Step: 1414  | total loss: \u001b[1m\u001b[32m0.19409\u001b[0m\u001b[0m | time: 0.018s\n",
            "| Adam | epoch: 283 | loss: 0.19409 - acc: 0.9999 -- iter: 32/36\n",
            "Training Step: 1415  | total loss: \u001b[1m\u001b[32m0.20075\u001b[0m\u001b[0m | time: 0.024s\n",
            "| Adam | epoch: 283 | loss: 0.20075 - acc: 0.9999 -- iter: 36/36\n",
            "--\n",
            "Training Step: 1416  | total loss: \u001b[1m\u001b[32m0.20664\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 284 | loss: 0.20664 - acc: 0.9999 -- iter: 08/36\n",
            "Training Step: 1417  | total loss: \u001b[1m\u001b[32m0.19617\u001b[0m\u001b[0m | time: 0.011s\n",
            "| Adam | epoch: 284 | loss: 0.19617 - acc: 0.9999 -- iter: 16/36\n",
            "Training Step: 1418  | total loss: \u001b[1m\u001b[32m0.19546\u001b[0m\u001b[0m | time: 0.015s\n",
            "| Adam | epoch: 284 | loss: 0.19546 - acc: 1.0000 -- iter: 24/36\n",
            "Training Step: 1419  | total loss: \u001b[1m\u001b[32m1.12560\u001b[0m\u001b[0m | time: 0.021s\n",
            "| Adam | epoch: 284 | loss: 1.12560 - acc: 0.9250 -- iter: 32/36\n",
            "Training Step: 1420  | total loss: \u001b[1m\u001b[32m1.03151\u001b[0m\u001b[0m | time: 0.025s\n",
            "| Adam | epoch: 284 | loss: 1.03151 - acc: 0.9325 -- iter: 36/36\n",
            "--\n",
            "Training Step: 1421  | total loss: \u001b[1m\u001b[32m0.95525\u001b[0m\u001b[0m | time: 0.004s\n",
            "| Adam | epoch: 285 | loss: 0.95525 - acc: 0.9392 -- iter: 08/36\n",
            "Training Step: 1422  | total loss: \u001b[1m\u001b[32m0.88658\u001b[0m\u001b[0m | time: 0.009s\n",
            "| Adam | epoch: 285 | loss: 0.88658 - acc: 0.9453 -- iter: 16/36\n",
            "Training Step: 1423  | total loss: \u001b[1m\u001b[32m0.81108\u001b[0m\u001b[0m | time: 0.013s\n",
            "| Adam | epoch: 285 | loss: 0.81108 - acc: 0.9508 -- iter: 24/36\n",
            "Training Step: 1424  | total loss: \u001b[1m\u001b[32m0.74831\u001b[0m\u001b[0m | time: 0.016s\n",
            "| Adam | epoch: 285 | loss: 0.74831 - acc: 0.9557 -- iter: 32/36\n",
            "Training Step: 1425  | total loss: \u001b[1m\u001b[32m0.69239\u001b[0m\u001b[0m | time: 0.020s\n",
            "| Adam | epoch: 285 | loss: 0.69239 - acc: 0.9601 -- iter: 36/36\n",
            "--\n",
            "Training Step: 1426  | total loss: \u001b[1m\u001b[32m0.64011\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 286 | loss: 0.64011 - acc: 0.9641 -- iter: 08/36\n",
            "Training Step: 1427  | total loss: \u001b[1m\u001b[32m0.59611\u001b[0m\u001b[0m | time: 0.009s\n",
            "| Adam | epoch: 286 | loss: 0.59611 - acc: 0.9677 -- iter: 16/36\n",
            "Training Step: 1428  | total loss: \u001b[1m\u001b[32m0.55647\u001b[0m\u001b[0m | time: 0.013s\n",
            "| Adam | epoch: 286 | loss: 0.55647 - acc: 0.9677 -- iter: 24/36\n",
            "Training Step: 1429  | total loss: \u001b[1m\u001b[32m0.51780\u001b[0m\u001b[0m | time: 0.018s\n",
            "| Adam | epoch: 286 | loss: 0.51780 - acc: 0.9709 -- iter: 32/36\n",
            "Training Step: 1430  | total loss: \u001b[1m\u001b[32m0.48919\u001b[0m\u001b[0m | time: 0.022s\n",
            "| Adam | epoch: 286 | loss: 0.48919 - acc: 0.9738 -- iter: 36/36\n",
            "--\n",
            "Training Step: 1431  | total loss: \u001b[1m\u001b[32m0.45601\u001b[0m\u001b[0m | time: 0.003s\n",
            "| Adam | epoch: 287 | loss: 0.45601 - acc: 0.9765 -- iter: 08/36\n",
            "Training Step: 1432  | total loss: \u001b[1m\u001b[32m0.43092\u001b[0m\u001b[0m | time: 0.007s\n",
            "| Adam | epoch: 287 | loss: 0.43092 - acc: 0.9788 -- iter: 16/36\n",
            "Training Step: 1433  | total loss: \u001b[1m\u001b[32m0.40462\u001b[0m\u001b[0m | time: 0.012s\n",
            "| Adam | epoch: 287 | loss: 0.40462 - acc: 0.9809 -- iter: 24/36\n",
            "Training Step: 1434  | total loss: \u001b[1m\u001b[32m0.38075\u001b[0m\u001b[0m | time: 0.015s\n",
            "| Adam | epoch: 287 | loss: 0.38075 - acc: 0.9828 -- iter: 32/36\n",
            "Training Step: 1435  | total loss: \u001b[1m\u001b[32m0.36757\u001b[0m\u001b[0m | time: 0.020s\n",
            "| Adam | epoch: 287 | loss: 0.36757 - acc: 0.9845 -- iter: 36/36\n",
            "--\n",
            "Training Step: 1436  | total loss: \u001b[1m\u001b[32m0.34145\u001b[0m\u001b[0m | time: 0.006s\n",
            "| Adam | epoch: 288 | loss: 0.34145 - acc: 0.9861 -- iter: 08/36\n",
            "Training Step: 1437  | total loss: \u001b[1m\u001b[32m0.32522\u001b[0m\u001b[0m | time: 0.010s\n",
            "| Adam | epoch: 288 | loss: 0.32522 - acc: 0.9875 -- iter: 16/36\n",
            "Training Step: 1438  | total loss: \u001b[1m\u001b[32m0.30620\u001b[0m\u001b[0m | time: 0.013s\n",
            "| Adam | epoch: 288 | loss: 0.30620 - acc: 0.9887 -- iter: 24/36\n",
            "Training Step: 1439  | total loss: \u001b[1m\u001b[32m0.28912\u001b[0m\u001b[0m | time: 0.017s\n",
            "| Adam | epoch: 288 | loss: 0.28912 - acc: 0.9899 -- iter: 32/36\n",
            "Training Step: 1440  | total loss: \u001b[1m\u001b[32m0.27362\u001b[0m\u001b[0m | time: 0.021s\n",
            "| Adam | epoch: 288 | loss: 0.27362 - acc: 0.9909 -- iter: 36/36\n",
            "--\n",
            "Training Step: 1441  | total loss: \u001b[1m\u001b[32m0.26841\u001b[0m\u001b[0m | time: 0.006s\n",
            "| Adam | epoch: 289 | loss: 0.26841 - acc: 0.9918 -- iter: 08/36\n",
            "Training Step: 1442  | total loss: \u001b[1m\u001b[32m0.25739\u001b[0m\u001b[0m | time: 0.009s\n",
            "| Adam | epoch: 289 | loss: 0.25739 - acc: 0.9926 -- iter: 16/36\n",
            "Training Step: 1443  | total loss: \u001b[1m\u001b[32m0.25444\u001b[0m\u001b[0m | time: 0.013s\n",
            "| Adam | epoch: 289 | loss: 0.25444 - acc: 0.9933 -- iter: 24/36\n",
            "Training Step: 1444  | total loss: \u001b[1m\u001b[32m0.24426\u001b[0m\u001b[0m | time: 0.018s\n",
            "| Adam | epoch: 289 | loss: 0.24426 - acc: 0.9940 -- iter: 32/36\n",
            "Training Step: 1445  | total loss: \u001b[1m\u001b[32m0.24103\u001b[0m\u001b[0m | time: 0.022s\n",
            "| Adam | epoch: 289 | loss: 0.24103 - acc: 0.9946 -- iter: 36/36\n",
            "--\n",
            "Training Step: 1446  | total loss: \u001b[1m\u001b[32m0.23804\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 290 | loss: 0.23804 - acc: 0.9952 -- iter: 08/36\n",
            "Training Step: 1447  | total loss: \u001b[1m\u001b[32m0.23790\u001b[0m\u001b[0m | time: 0.010s\n",
            "| Adam | epoch: 290 | loss: 0.23790 - acc: 0.9961 -- iter: 16/36\n",
            "Training Step: 1448  | total loss: \u001b[1m\u001b[32m0.22861\u001b[0m\u001b[0m | time: 0.014s\n",
            "| Adam | epoch: 290 | loss: 0.22861 - acc: 0.9965 -- iter: 24/36\n",
            "Training Step: 1449  | total loss: \u001b[1m\u001b[32m0.22172\u001b[0m\u001b[0m | time: 0.019s\n",
            "| Adam | epoch: 290 | loss: 0.22172 - acc: 0.9968 -- iter: 32/36\n",
            "Training Step: 1450  | total loss: \u001b[1m\u001b[32m0.21218\u001b[0m\u001b[0m | time: 0.023s\n",
            "| Adam | epoch: 290 | loss: 0.21218 - acc: 0.9971 -- iter: 36/36\n",
            "--\n",
            "Training Step: 1451  | total loss: \u001b[1m\u001b[32m0.20323\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 291 | loss: 0.20323 - acc: 0.9971 -- iter: 08/36\n",
            "Training Step: 1452  | total loss: \u001b[1m\u001b[32m0.19509\u001b[0m\u001b[0m | time: 0.009s\n",
            "| Adam | epoch: 291 | loss: 0.19509 - acc: 0.9974 -- iter: 16/36\n",
            "Training Step: 1453  | total loss: \u001b[1m\u001b[32m0.19171\u001b[0m\u001b[0m | time: 0.014s\n",
            "| Adam | epoch: 291 | loss: 0.19171 - acc: 0.9977 -- iter: 24/36\n",
            "Training Step: 1454  | total loss: \u001b[1m\u001b[32m0.19197\u001b[0m\u001b[0m | time: 0.018s\n",
            "| Adam | epoch: 291 | loss: 0.19197 - acc: 0.9979 -- iter: 32/36\n",
            "Training Step: 1455  | total loss: \u001b[1m\u001b[32m0.19727\u001b[0m\u001b[0m | time: 0.023s\n",
            "| Adam | epoch: 291 | loss: 0.19727 - acc: 0.9983 -- iter: 36/36\n",
            "--\n",
            "Training Step: 1456  | total loss: \u001b[1m\u001b[32m0.19727\u001b[0m\u001b[0m | time: 0.004s\n",
            "| Adam | epoch: 292 | loss: 0.19727 - acc: 0.9983 -- iter: 08/36\n",
            "Training Step: 1457  | total loss: \u001b[1m\u001b[32m0.19098\u001b[0m\u001b[0m | time: 0.009s\n",
            "| Adam | epoch: 292 | loss: 0.19098 - acc: 0.9985 -- iter: 16/36\n",
            "Training Step: 1458  | total loss: \u001b[1m\u001b[32m0.19046\u001b[0m\u001b[0m | time: 0.013s\n",
            "| Adam | epoch: 292 | loss: 0.19046 - acc: 0.9986 -- iter: 24/36\n",
            "Training Step: 1459  | total loss: \u001b[1m\u001b[32m0.19212\u001b[0m\u001b[0m | time: 0.018s\n",
            "| Adam | epoch: 292 | loss: 0.19212 - acc: 0.9988 -- iter: 32/36\n",
            "Training Step: 1460  | total loss: \u001b[1m\u001b[32m0.19212\u001b[0m\u001b[0m | time: 0.023s\n",
            "| Adam | epoch: 292 | loss: 0.19212 - acc: 0.9989 -- iter: 36/36\n",
            "--\n",
            "Training Step: 1461  | total loss: \u001b[1m\u001b[32m0.18815\u001b[0m\u001b[0m | time: 0.003s\n",
            "| Adam | epoch: 293 | loss: 0.18815 - acc: 0.9991 -- iter: 08/36\n",
            "Training Step: 1462  | total loss: \u001b[1m\u001b[32m0.18785\u001b[0m\u001b[0m | time: 0.006s\n",
            "| Adam | epoch: 293 | loss: 0.18785 - acc: 0.9992 -- iter: 16/36\n",
            "Training Step: 1463  | total loss: \u001b[1m\u001b[32m0.17501\u001b[0m\u001b[0m | time: 0.009s\n",
            "| Adam | epoch: 293 | loss: 0.17501 - acc: 0.9993 -- iter: 24/36\n",
            "Training Step: 1464  | total loss: \u001b[1m\u001b[32m0.16344\u001b[0m\u001b[0m | time: 0.012s\n",
            "| Adam | epoch: 293 | loss: 0.16344 - acc: 0.9993 -- iter: 32/36\n",
            "Training Step: 1465  | total loss: \u001b[1m\u001b[32m0.16017\u001b[0m\u001b[0m | time: 0.015s\n",
            "| Adam | epoch: 293 | loss: 0.16017 - acc: 0.9994 -- iter: 36/36\n",
            "--\n",
            "Training Step: 1466  | total loss: \u001b[1m\u001b[32m0.16502\u001b[0m\u001b[0m | time: 0.003s\n",
            "| Adam | epoch: 294 | loss: 0.16502 - acc: 0.9995 -- iter: 08/36\n",
            "Training Step: 1467  | total loss: \u001b[1m\u001b[32m0.16502\u001b[0m\u001b[0m | time: 0.006s\n",
            "| Adam | epoch: 294 | loss: 0.16502 - acc: 0.9995 -- iter: 16/36\n",
            "Training Step: 1468  | total loss: \u001b[1m\u001b[32m0.16955\u001b[0m\u001b[0m | time: 0.011s\n",
            "| Adam | epoch: 294 | loss: 0.16955 - acc: 0.9995 -- iter: 24/36\n",
            "Training Step: 1469  | total loss: \u001b[1m\u001b[32m0.15732\u001b[0m\u001b[0m | time: 0.020s\n",
            "| Adam | epoch: 294 | loss: 0.15732 - acc: 0.9996 -- iter: 32/36\n",
            "Training Step: 1470  | total loss: \u001b[1m\u001b[32m0.14968\u001b[0m\u001b[0m | time: 0.023s\n",
            "| Adam | epoch: 294 | loss: 0.14968 - acc: 0.9997 -- iter: 36/36\n",
            "--\n",
            "Training Step: 1471  | total loss: \u001b[1m\u001b[32m0.15198\u001b[0m\u001b[0m | time: 0.007s\n",
            "| Adam | epoch: 295 | loss: 0.15198 - acc: 0.9997 -- iter: 08/36\n",
            "Training Step: 1472  | total loss: \u001b[1m\u001b[32m0.16283\u001b[0m\u001b[0m | time: 0.014s\n",
            "| Adam | epoch: 295 | loss: 0.16283 - acc: 0.9997 -- iter: 16/36\n",
            "Training Step: 1473  | total loss: \u001b[1m\u001b[32m0.16159\u001b[0m\u001b[0m | time: 0.019s\n",
            "| Adam | epoch: 295 | loss: 0.16159 - acc: 0.9997 -- iter: 24/36\n",
            "Training Step: 1474  | total loss: \u001b[1m\u001b[32m0.15690\u001b[0m\u001b[0m | time: 0.023s\n",
            "| Adam | epoch: 295 | loss: 0.15690 - acc: 0.9997 -- iter: 32/36\n",
            "Training Step: 1475  | total loss: \u001b[1m\u001b[32m0.15690\u001b[0m\u001b[0m | time: 0.028s\n",
            "| Adam | epoch: 295 | loss: 0.15690 - acc: 0.9998 -- iter: 36/36\n",
            "--\n",
            "Training Step: 1476  | total loss: \u001b[1m\u001b[32m0.15351\u001b[0m\u001b[0m | time: 0.007s\n",
            "| Adam | epoch: 296 | loss: 0.15351 - acc: 0.9998 -- iter: 08/36\n",
            "Training Step: 1477  | total loss: \u001b[1m\u001b[32m0.15038\u001b[0m\u001b[0m | time: 0.015s\n",
            "| Adam | epoch: 296 | loss: 0.15038 - acc: 0.9998 -- iter: 16/36\n",
            "Training Step: 1478  | total loss: \u001b[1m\u001b[32m0.15400\u001b[0m\u001b[0m | time: 0.021s\n",
            "| Adam | epoch: 296 | loss: 0.15400 - acc: 0.9998 -- iter: 24/36\n",
            "Training Step: 1479  | total loss: \u001b[1m\u001b[32m0.15400\u001b[0m\u001b[0m | time: 0.029s\n",
            "| Adam | epoch: 296 | loss: 0.15400 - acc: 0.9999 -- iter: 32/36\n",
            "Training Step: 1480  | total loss: \u001b[1m\u001b[32m0.16468\u001b[0m\u001b[0m | time: 0.036s\n",
            "| Adam | epoch: 296 | loss: 0.16468 - acc: 0.9999 -- iter: 36/36\n",
            "--\n",
            "Training Step: 1481  | total loss: \u001b[1m\u001b[32m0.16468\u001b[0m\u001b[0m | time: 0.007s\n",
            "| Adam | epoch: 297 | loss: 0.16468 - acc: 0.9999 -- iter: 08/36\n",
            "Training Step: 1482  | total loss: \u001b[1m\u001b[32m0.17573\u001b[0m\u001b[0m | time: 0.015s\n",
            "| Adam | epoch: 297 | loss: 0.17573 - acc: 0.9999 -- iter: 16/36\n",
            "Training Step: 1483  | total loss: \u001b[1m\u001b[32m0.17573\u001b[0m\u001b[0m | time: 0.025s\n",
            "| Adam | epoch: 297 | loss: 0.17573 - acc: 0.9999 -- iter: 24/36\n",
            "Training Step: 1484  | total loss: \u001b[1m\u001b[32m0.17395\u001b[0m\u001b[0m | time: 0.032s\n",
            "| Adam | epoch: 297 | loss: 0.17395 - acc: 0.9999 -- iter: 32/36\n",
            "Training Step: 1485  | total loss: \u001b[1m\u001b[32m0.16713\u001b[0m\u001b[0m | time: 0.039s\n",
            "| Adam | epoch: 297 | loss: 0.16713 - acc: 0.9999 -- iter: 36/36\n",
            "--\n",
            "Training Step: 1486  | total loss: \u001b[1m\u001b[32m0.16738\u001b[0m\u001b[0m | time: 0.003s\n",
            "| Adam | epoch: 298 | loss: 0.16738 - acc: 0.9999 -- iter: 08/36\n",
            "Training Step: 1487  | total loss: \u001b[1m\u001b[32m0.17354\u001b[0m\u001b[0m | time: 0.012s\n",
            "| Adam | epoch: 298 | loss: 0.17354 - acc: 0.9999 -- iter: 16/36\n",
            "Training Step: 1488  | total loss: \u001b[1m\u001b[32m0.17897\u001b[0m\u001b[0m | time: 0.020s\n",
            "| Adam | epoch: 298 | loss: 0.17897 - acc: 0.9999 -- iter: 24/36\n",
            "Training Step: 1489  | total loss: \u001b[1m\u001b[32m0.17897\u001b[0m\u001b[0m | time: 0.027s\n",
            "| Adam | epoch: 298 | loss: 0.17897 - acc: 0.9999 -- iter: 32/36\n",
            "Training Step: 1490  | total loss: \u001b[1m\u001b[32m0.17293\u001b[0m\u001b[0m | time: 0.031s\n",
            "| Adam | epoch: 298 | loss: 0.17293 - acc: 1.0000 -- iter: 36/36\n",
            "--\n",
            "Training Step: 1491  | total loss: \u001b[1m\u001b[32m0.17038\u001b[0m\u001b[0m | time: 0.003s\n",
            "| Adam | epoch: 299 | loss: 0.17038 - acc: 1.0000 -- iter: 08/36\n",
            "Training Step: 1492  | total loss: \u001b[1m\u001b[32m0.17313\u001b[0m\u001b[0m | time: 0.011s\n",
            "| Adam | epoch: 299 | loss: 0.17313 - acc: 1.0000 -- iter: 16/36\n",
            "Training Step: 1493  | total loss: \u001b[1m\u001b[32m0.17616\u001b[0m\u001b[0m | time: 0.014s\n",
            "| Adam | epoch: 299 | loss: 0.17616 - acc: 1.0000 -- iter: 24/36\n",
            "Training Step: 1494  | total loss: \u001b[1m\u001b[32m0.17876\u001b[0m\u001b[0m | time: 0.023s\n",
            "| Adam | epoch: 299 | loss: 0.17876 - acc: 1.0000 -- iter: 32/36\n",
            "Training Step: 1495  | total loss: \u001b[1m\u001b[32m0.17172\u001b[0m\u001b[0m | time: 0.029s\n",
            "| Adam | epoch: 299 | loss: 0.17172 - acc: 1.0000 -- iter: 36/36\n",
            "--\n",
            "Training Step: 1496  | total loss: \u001b[1m\u001b[32m0.17172\u001b[0m\u001b[0m | time: 0.006s\n",
            "| Adam | epoch: 300 | loss: 0.17172 - acc: 1.0000 -- iter: 08/36\n",
            "Training Step: 1497  | total loss: \u001b[1m\u001b[32m0.16814\u001b[0m\u001b[0m | time: 0.008s\n",
            "| Adam | epoch: 300 | loss: 0.16814 - acc: 1.0000 -- iter: 16/36\n",
            "Training Step: 1498  | total loss: \u001b[1m\u001b[32m0.16807\u001b[0m\u001b[0m | time: 0.013s\n",
            "| Adam | epoch: 300 | loss: 0.16807 - acc: 1.0000 -- iter: 24/36\n",
            "Training Step: 1499  | total loss: \u001b[1m\u001b[32m0.16029\u001b[0m\u001b[0m | time: 0.021s\n",
            "| Adam | epoch: 300 | loss: 0.16029 - acc: 1.0000 -- iter: 32/36\n",
            "Training Step: 1500  | total loss: \u001b[1m\u001b[32m0.15327\u001b[0m\u001b[0m | time: 0.028s\n",
            "| Adam | epoch: 300 | loss: 0.15327 - acc: 1.0000 -- iter: 36/36\n",
            "--\n",
            "Training Step: 1501  | total loss: \u001b[1m\u001b[32m0.15626\u001b[0m\u001b[0m | time: 0.006s\n",
            "| Adam | epoch: 301 | loss: 0.15626 - acc: 1.0000 -- iter: 08/36\n",
            "Training Step: 1502  | total loss: \u001b[1m\u001b[32m0.15146\u001b[0m\u001b[0m | time: 0.012s\n",
            "| Adam | epoch: 301 | loss: 0.15146 - acc: 1.0000 -- iter: 16/36\n",
            "Training Step: 1503  | total loss: \u001b[1m\u001b[32m0.15146\u001b[0m\u001b[0m | time: 0.020s\n",
            "| Adam | epoch: 301 | loss: 0.15146 - acc: 1.0000 -- iter: 24/36\n",
            "Training Step: 1504  | total loss: \u001b[1m\u001b[32m0.15613\u001b[0m\u001b[0m | time: 0.026s\n",
            "| Adam | epoch: 301 | loss: 0.15613 - acc: 1.0000 -- iter: 32/36\n",
            "Training Step: 1505  | total loss: \u001b[1m\u001b[32m0.15695\u001b[0m\u001b[0m | time: 0.029s\n",
            "| Adam | epoch: 301 | loss: 0.15695 - acc: 1.0000 -- iter: 36/36\n",
            "--\n",
            "Training Step: 1506  | total loss: \u001b[1m\u001b[32m0.16148\u001b[0m\u001b[0m | time: 0.003s\n",
            "| Adam | epoch: 302 | loss: 0.16148 - acc: 1.0000 -- iter: 08/36\n",
            "Training Step: 1507  | total loss: \u001b[1m\u001b[32m0.16415\u001b[0m\u001b[0m | time: 0.007s\n",
            "| Adam | epoch: 302 | loss: 0.16415 - acc: 1.0000 -- iter: 16/36\n",
            "Training Step: 1508  | total loss: \u001b[1m\u001b[32m0.16200\u001b[0m\u001b[0m | time: 0.011s\n",
            "| Adam | epoch: 302 | loss: 0.16200 - acc: 1.0000 -- iter: 24/36\n",
            "Training Step: 1509  | total loss: \u001b[1m\u001b[32m0.16046\u001b[0m\u001b[0m | time: 0.016s\n",
            "| Adam | epoch: 302 | loss: 0.16046 - acc: 1.0000 -- iter: 32/36\n",
            "Training Step: 1510  | total loss: \u001b[1m\u001b[32m0.15665\u001b[0m\u001b[0m | time: 0.021s\n",
            "| Adam | epoch: 302 | loss: 0.15665 - acc: 1.0000 -- iter: 36/36\n",
            "--\n",
            "Training Step: 1511  | total loss: \u001b[1m\u001b[32m0.15665\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 303 | loss: 0.15665 - acc: 1.0000 -- iter: 08/36\n",
            "Training Step: 1512  | total loss: \u001b[1m\u001b[32m0.15911\u001b[0m\u001b[0m | time: 0.011s\n",
            "| Adam | epoch: 303 | loss: 0.15911 - acc: 1.0000 -- iter: 16/36\n",
            "Training Step: 1513  | total loss: \u001b[1m\u001b[32m0.16481\u001b[0m\u001b[0m | time: 0.016s\n",
            "| Adam | epoch: 303 | loss: 0.16481 - acc: 1.0000 -- iter: 24/36\n",
            "Training Step: 1514  | total loss: \u001b[1m\u001b[32m0.16338\u001b[0m\u001b[0m | time: 0.019s\n",
            "| Adam | epoch: 303 | loss: 0.16338 - acc: 1.0000 -- iter: 32/36\n",
            "Training Step: 1515  | total loss: \u001b[1m\u001b[32m0.15932\u001b[0m\u001b[0m | time: 0.024s\n",
            "| Adam | epoch: 303 | loss: 0.15932 - acc: 1.0000 -- iter: 36/36\n",
            "--\n",
            "Training Step: 1516  | total loss: \u001b[1m\u001b[32m0.15932\u001b[0m\u001b[0m | time: 0.004s\n",
            "| Adam | epoch: 304 | loss: 0.15932 - acc: 1.0000 -- iter: 08/36\n",
            "Training Step: 1517  | total loss: \u001b[1m\u001b[32m0.16091\u001b[0m\u001b[0m | time: 0.009s\n",
            "| Adam | epoch: 304 | loss: 0.16091 - acc: 1.0000 -- iter: 16/36\n",
            "Training Step: 1518  | total loss: \u001b[1m\u001b[32m0.16581\u001b[0m\u001b[0m | time: 0.012s\n",
            "| Adam | epoch: 304 | loss: 0.16581 - acc: 1.0000 -- iter: 24/36\n",
            "Training Step: 1519  | total loss: \u001b[1m\u001b[32m0.16891\u001b[0m\u001b[0m | time: 0.016s\n",
            "| Adam | epoch: 304 | loss: 0.16891 - acc: 1.0000 -- iter: 32/36\n",
            "Training Step: 1520  | total loss: \u001b[1m\u001b[32m0.16496\u001b[0m\u001b[0m | time: 0.020s\n",
            "| Adam | epoch: 304 | loss: 0.16496 - acc: 1.0000 -- iter: 36/36\n",
            "--\n",
            "Training Step: 1521  | total loss: \u001b[1m\u001b[32m0.16076\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 305 | loss: 0.16076 - acc: 1.0000 -- iter: 08/36\n",
            "Training Step: 1522  | total loss: \u001b[1m\u001b[32m0.15611\u001b[0m\u001b[0m | time: 0.009s\n",
            "| Adam | epoch: 305 | loss: 0.15611 - acc: 1.0000 -- iter: 16/36\n",
            "Training Step: 1523  | total loss: \u001b[1m\u001b[32m0.16198\u001b[0m\u001b[0m | time: 0.013s\n",
            "| Adam | epoch: 305 | loss: 0.16198 - acc: 1.0000 -- iter: 24/36\n",
            "Training Step: 1524  | total loss: \u001b[1m\u001b[32m0.16712\u001b[0m\u001b[0m | time: 0.017s\n",
            "| Adam | epoch: 305 | loss: 0.16712 - acc: 1.0000 -- iter: 32/36\n",
            "Training Step: 1525  | total loss: \u001b[1m\u001b[32m0.17186\u001b[0m\u001b[0m | time: 0.021s\n",
            "| Adam | epoch: 305 | loss: 0.17186 - acc: 1.0000 -- iter: 36/36\n",
            "--\n",
            "Training Step: 1526  | total loss: \u001b[1m\u001b[32m0.16774\u001b[0m\u001b[0m | time: 0.003s\n",
            "| Adam | epoch: 306 | loss: 0.16774 - acc: 1.0000 -- iter: 08/36\n",
            "Training Step: 1527  | total loss: \u001b[1m\u001b[32m0.16068\u001b[0m\u001b[0m | time: 0.006s\n",
            "| Adam | epoch: 306 | loss: 0.16068 - acc: 1.0000 -- iter: 16/36\n",
            "Training Step: 1528  | total loss: \u001b[1m\u001b[32m0.15894\u001b[0m\u001b[0m | time: 0.010s\n",
            "| Adam | epoch: 306 | loss: 0.15894 - acc: 1.0000 -- iter: 24/36\n",
            "Training Step: 1529  | total loss: \u001b[1m\u001b[32m0.15335\u001b[0m\u001b[0m | time: 0.014s\n",
            "| Adam | epoch: 306 | loss: 0.15335 - acc: 1.0000 -- iter: 32/36\n",
            "Training Step: 1530  | total loss: \u001b[1m\u001b[32m0.14823\u001b[0m\u001b[0m | time: 0.018s\n",
            "| Adam | epoch: 306 | loss: 0.14823 - acc: 1.0000 -- iter: 36/36\n",
            "--\n",
            "Training Step: 1531  | total loss: \u001b[1m\u001b[32m0.14759\u001b[0m\u001b[0m | time: 0.004s\n",
            "| Adam | epoch: 307 | loss: 0.14759 - acc: 1.0000 -- iter: 08/36\n",
            "Training Step: 1532  | total loss: \u001b[1m\u001b[32m0.15263\u001b[0m\u001b[0m | time: 0.007s\n",
            "| Adam | epoch: 307 | loss: 0.15263 - acc: 1.0000 -- iter: 16/36\n",
            "Training Step: 1533  | total loss: \u001b[1m\u001b[32m0.14961\u001b[0m\u001b[0m | time: 0.010s\n",
            "| Adam | epoch: 307 | loss: 0.14961 - acc: 1.0000 -- iter: 24/36\n",
            "Training Step: 1534  | total loss: \u001b[1m\u001b[32m0.14958\u001b[0m\u001b[0m | time: 0.014s\n",
            "| Adam | epoch: 307 | loss: 0.14958 - acc: 1.0000 -- iter: 32/36\n",
            "Training Step: 1535  | total loss: \u001b[1m\u001b[32m0.14997\u001b[0m\u001b[0m | time: 0.021s\n",
            "| Adam | epoch: 307 | loss: 0.14997 - acc: 1.0000 -- iter: 36/36\n",
            "--\n",
            "Training Step: 1536  | total loss: \u001b[1m\u001b[32m0.15023\u001b[0m\u001b[0m | time: 0.003s\n",
            "| Adam | epoch: 308 | loss: 0.15023 - acc: 1.0000 -- iter: 08/36\n",
            "Training Step: 1537  | total loss: \u001b[1m\u001b[32m0.14957\u001b[0m\u001b[0m | time: 0.007s\n",
            "| Adam | epoch: 308 | loss: 0.14957 - acc: 1.0000 -- iter: 16/36\n",
            "Training Step: 1538  | total loss: \u001b[1m\u001b[32m0.15280\u001b[0m\u001b[0m | time: 0.010s\n",
            "| Adam | epoch: 308 | loss: 0.15280 - acc: 1.0000 -- iter: 24/36\n",
            "Training Step: 1539  | total loss: \u001b[1m\u001b[32m0.14731\u001b[0m\u001b[0m | time: 0.015s\n",
            "| Adam | epoch: 308 | loss: 0.14731 - acc: 1.0000 -- iter: 32/36\n",
            "Training Step: 1540  | total loss: \u001b[1m\u001b[32m0.14332\u001b[0m\u001b[0m | time: 0.019s\n",
            "| Adam | epoch: 308 | loss: 0.14332 - acc: 1.0000 -- iter: 36/36\n",
            "--\n",
            "Training Step: 1541  | total loss: \u001b[1m\u001b[32m0.14466\u001b[0m\u001b[0m | time: 0.004s\n",
            "| Adam | epoch: 309 | loss: 0.14466 - acc: 1.0000 -- iter: 08/36\n",
            "Training Step: 1542  | total loss: \u001b[1m\u001b[32m0.14572\u001b[0m\u001b[0m | time: 0.007s\n",
            "| Adam | epoch: 309 | loss: 0.14572 - acc: 1.0000 -- iter: 16/36\n",
            "Training Step: 1543  | total loss: \u001b[1m\u001b[32m0.15004\u001b[0m\u001b[0m | time: 0.011s\n",
            "| Adam | epoch: 309 | loss: 0.15004 - acc: 1.0000 -- iter: 24/36\n",
            "Training Step: 1544  | total loss: \u001b[1m\u001b[32m0.14642\u001b[0m\u001b[0m | time: 0.014s\n",
            "| Adam | epoch: 309 | loss: 0.14642 - acc: 1.0000 -- iter: 32/36\n",
            "Training Step: 1545  | total loss: \u001b[1m\u001b[32m0.14663\u001b[0m\u001b[0m | time: 0.018s\n",
            "| Adam | epoch: 309 | loss: 0.14663 - acc: 1.0000 -- iter: 36/36\n",
            "--\n",
            "Training Step: 1546  | total loss: \u001b[1m\u001b[32m0.14321\u001b[0m\u001b[0m | time: 0.004s\n",
            "| Adam | epoch: 310 | loss: 0.14321 - acc: 1.0000 -- iter: 08/36\n",
            "Training Step: 1547  | total loss: \u001b[1m\u001b[32m0.13934\u001b[0m\u001b[0m | time: 0.008s\n",
            "| Adam | epoch: 310 | loss: 0.13934 - acc: 1.0000 -- iter: 16/36\n",
            "Training Step: 1548  | total loss: \u001b[1m\u001b[32m0.13564\u001b[0m\u001b[0m | time: 0.013s\n",
            "| Adam | epoch: 310 | loss: 0.13564 - acc: 1.0000 -- iter: 24/36\n",
            "Training Step: 1549  | total loss: \u001b[1m\u001b[32m0.12975\u001b[0m\u001b[0m | time: 0.017s\n",
            "| Adam | epoch: 310 | loss: 0.12975 - acc: 1.0000 -- iter: 32/36\n",
            "Training Step: 1550  | total loss: \u001b[1m\u001b[32m0.13439\u001b[0m\u001b[0m | time: 0.022s\n",
            "| Adam | epoch: 310 | loss: 0.13439 - acc: 1.0000 -- iter: 36/36\n",
            "--\n",
            "Training Step: 1551  | total loss: \u001b[1m\u001b[32m0.14239\u001b[0m\u001b[0m | time: 0.003s\n",
            "| Adam | epoch: 311 | loss: 0.14239 - acc: 1.0000 -- iter: 08/36\n",
            "Training Step: 1552  | total loss: \u001b[1m\u001b[32m0.13412\u001b[0m\u001b[0m | time: 0.007s\n",
            "| Adam | epoch: 311 | loss: 0.13412 - acc: 1.0000 -- iter: 16/36\n",
            "Training Step: 1553  | total loss: \u001b[1m\u001b[32m0.12977\u001b[0m\u001b[0m | time: 0.012s\n",
            "| Adam | epoch: 311 | loss: 0.12977 - acc: 1.0000 -- iter: 24/36\n",
            "Training Step: 1554  | total loss: \u001b[1m\u001b[32m0.12579\u001b[0m\u001b[0m | time: 0.015s\n",
            "| Adam | epoch: 311 | loss: 0.12579 - acc: 1.0000 -- iter: 32/36\n",
            "Training Step: 1555  | total loss: \u001b[1m\u001b[32m0.13006\u001b[0m\u001b[0m | time: 0.020s\n",
            "| Adam | epoch: 311 | loss: 0.13006 - acc: 1.0000 -- iter: 36/36\n",
            "--\n",
            "Training Step: 1556  | total loss: \u001b[1m\u001b[32m0.13006\u001b[0m\u001b[0m | time: 0.004s\n",
            "| Adam | epoch: 312 | loss: 0.13006 - acc: 1.0000 -- iter: 08/36\n",
            "Training Step: 1557  | total loss: \u001b[1m\u001b[32m0.13763\u001b[0m\u001b[0m | time: 0.008s\n",
            "| Adam | epoch: 312 | loss: 0.13763 - acc: 1.0000 -- iter: 16/36\n",
            "Training Step: 1558  | total loss: \u001b[1m\u001b[32m0.13767\u001b[0m\u001b[0m | time: 0.013s\n",
            "| Adam | epoch: 312 | loss: 0.13767 - acc: 1.0000 -- iter: 24/36\n",
            "Training Step: 1559  | total loss: \u001b[1m\u001b[32m0.14294\u001b[0m\u001b[0m | time: 0.017s\n",
            "| Adam | epoch: 312 | loss: 0.14294 - acc: 1.0000 -- iter: 32/36\n",
            "Training Step: 1560  | total loss: \u001b[1m\u001b[32m0.14171\u001b[0m\u001b[0m | time: 0.021s\n",
            "| Adam | epoch: 312 | loss: 0.14171 - acc: 1.0000 -- iter: 36/36\n",
            "--\n",
            "Training Step: 1561  | total loss: \u001b[1m\u001b[32m0.13872\u001b[0m\u001b[0m | time: 0.004s\n",
            "| Adam | epoch: 313 | loss: 0.13872 - acc: 1.0000 -- iter: 08/36\n",
            "Training Step: 1562  | total loss: \u001b[1m\u001b[32m0.13358\u001b[0m\u001b[0m | time: 0.008s\n",
            "| Adam | epoch: 313 | loss: 0.13358 - acc: 1.0000 -- iter: 16/36\n",
            "Training Step: 1563  | total loss: \u001b[1m\u001b[32m0.13456\u001b[0m\u001b[0m | time: 0.014s\n",
            "| Adam | epoch: 313 | loss: 0.13456 - acc: 1.0000 -- iter: 24/36\n",
            "Training Step: 1564  | total loss: \u001b[1m\u001b[32m0.13004\u001b[0m\u001b[0m | time: 0.018s\n",
            "| Adam | epoch: 313 | loss: 0.13004 - acc: 1.0000 -- iter: 32/36\n",
            "Training Step: 1565  | total loss: \u001b[1m\u001b[32m0.13290\u001b[0m\u001b[0m | time: 0.022s\n",
            "| Adam | epoch: 313 | loss: 0.13290 - acc: 1.0000 -- iter: 36/36\n",
            "--\n",
            "Training Step: 1566  | total loss: \u001b[1m\u001b[32m0.13541\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 314 | loss: 0.13541 - acc: 1.0000 -- iter: 08/36\n",
            "Training Step: 1567  | total loss: \u001b[1m\u001b[32m0.13426\u001b[0m\u001b[0m | time: 0.009s\n",
            "| Adam | epoch: 314 | loss: 0.13426 - acc: 1.0000 -- iter: 16/36\n",
            "Training Step: 1568  | total loss: \u001b[1m\u001b[32m0.13444\u001b[0m\u001b[0m | time: 0.013s\n",
            "| Adam | epoch: 314 | loss: 0.13444 - acc: 1.0000 -- iter: 24/36\n",
            "Training Step: 1569  | total loss: \u001b[1m\u001b[32m0.13808\u001b[0m\u001b[0m | time: 0.017s\n",
            "| Adam | epoch: 314 | loss: 0.13808 - acc: 1.0000 -- iter: 32/36\n",
            "Training Step: 1570  | total loss: \u001b[1m\u001b[32m0.13907\u001b[0m\u001b[0m | time: 0.021s\n",
            "| Adam | epoch: 314 | loss: 0.13907 - acc: 1.0000 -- iter: 36/36\n",
            "--\n",
            "Training Step: 1571  | total loss: \u001b[1m\u001b[32m0.13938\u001b[0m\u001b[0m | time: 0.004s\n",
            "| Adam | epoch: 315 | loss: 0.13938 - acc: 1.0000 -- iter: 08/36\n",
            "Training Step: 1572  | total loss: \u001b[1m\u001b[32m0.13961\u001b[0m\u001b[0m | time: 0.009s\n",
            "| Adam | epoch: 315 | loss: 0.13961 - acc: 1.0000 -- iter: 16/36\n",
            "Training Step: 1573  | total loss: \u001b[1m\u001b[32m0.13937\u001b[0m\u001b[0m | time: 0.017s\n",
            "| Adam | epoch: 315 | loss: 0.13937 - acc: 1.0000 -- iter: 24/36\n",
            "Training Step: 1574  | total loss: \u001b[1m\u001b[32m0.13752\u001b[0m\u001b[0m | time: 0.027s\n",
            "| Adam | epoch: 315 | loss: 0.13752 - acc: 1.0000 -- iter: 32/36\n",
            "Training Step: 1575  | total loss: \u001b[1m\u001b[32m0.13527\u001b[0m\u001b[0m | time: 0.035s\n",
            "| Adam | epoch: 315 | loss: 0.13527 - acc: 1.0000 -- iter: 36/36\n",
            "--\n",
            "Training Step: 1576  | total loss: \u001b[1m\u001b[32m0.13527\u001b[0m\u001b[0m | time: 0.007s\n",
            "| Adam | epoch: 316 | loss: 0.13527 - acc: 1.0000 -- iter: 08/36\n",
            "Training Step: 1577  | total loss: \u001b[1m\u001b[32m0.13613\u001b[0m\u001b[0m | time: 0.017s\n",
            "| Adam | epoch: 316 | loss: 0.13613 - acc: 1.0000 -- iter: 16/36\n",
            "Training Step: 1578  | total loss: \u001b[1m\u001b[32m0.13515\u001b[0m\u001b[0m | time: 0.023s\n",
            "| Adam | epoch: 316 | loss: 0.13515 - acc: 1.0000 -- iter: 24/36\n",
            "Training Step: 1579  | total loss: \u001b[1m\u001b[32m0.13467\u001b[0m\u001b[0m | time: 0.032s\n",
            "| Adam | epoch: 316 | loss: 0.13467 - acc: 1.0000 -- iter: 32/36\n",
            "Training Step: 1580  | total loss: \u001b[1m\u001b[32m0.13467\u001b[0m\u001b[0m | time: 0.037s\n",
            "| Adam | epoch: 316 | loss: 0.13467 - acc: 1.0000 -- iter: 36/36\n",
            "--\n",
            "Training Step: 1581  | total loss: \u001b[1m\u001b[32m0.13206\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 317 | loss: 0.13206 - acc: 1.0000 -- iter: 08/36\n",
            "Training Step: 1582  | total loss: \u001b[1m\u001b[32m0.12954\u001b[0m\u001b[0m | time: 0.008s\n",
            "| Adam | epoch: 317 | loss: 0.12954 - acc: 1.0000 -- iter: 16/36\n",
            "Training Step: 1583  | total loss: \u001b[1m\u001b[32m0.12741\u001b[0m\u001b[0m | time: 0.012s\n",
            "| Adam | epoch: 317 | loss: 0.12741 - acc: 1.0000 -- iter: 24/36\n",
            "Training Step: 1584  | total loss: \u001b[1m\u001b[32m0.12541\u001b[0m\u001b[0m | time: 0.020s\n",
            "| Adam | epoch: 317 | loss: 0.12541 - acc: 1.0000 -- iter: 32/36\n",
            "Training Step: 1585  | total loss: \u001b[1m\u001b[32m0.12580\u001b[0m\u001b[0m | time: 0.025s\n",
            "| Adam | epoch: 317 | loss: 0.12580 - acc: 1.0000 -- iter: 36/36\n",
            "--\n",
            "Training Step: 1586  | total loss: \u001b[1m\u001b[32m0.12911\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 318 | loss: 0.12911 - acc: 1.0000 -- iter: 08/36\n",
            "Training Step: 1587  | total loss: \u001b[1m\u001b[32m0.12884\u001b[0m\u001b[0m | time: 0.011s\n",
            "| Adam | epoch: 318 | loss: 0.12884 - acc: 1.0000 -- iter: 16/36\n",
            "Training Step: 1588  | total loss: \u001b[1m\u001b[32m0.13454\u001b[0m\u001b[0m | time: 0.015s\n",
            "| Adam | epoch: 318 | loss: 0.13454 - acc: 1.0000 -- iter: 24/36\n",
            "Training Step: 1589  | total loss: \u001b[1m\u001b[32m0.13454\u001b[0m\u001b[0m | time: 0.020s\n",
            "| Adam | epoch: 318 | loss: 0.13454 - acc: 1.0000 -- iter: 32/36\n",
            "Training Step: 1590  | total loss: \u001b[1m\u001b[32m0.12821\u001b[0m\u001b[0m | time: 0.024s\n",
            "| Adam | epoch: 318 | loss: 0.12821 - acc: 1.0000 -- iter: 36/36\n",
            "--\n",
            "Training Step: 1591  | total loss: \u001b[1m\u001b[32m0.12281\u001b[0m\u001b[0m | time: 0.003s\n",
            "| Adam | epoch: 319 | loss: 0.12281 - acc: 1.0000 -- iter: 08/36\n",
            "Training Step: 1592  | total loss: \u001b[1m\u001b[32m0.12281\u001b[0m\u001b[0m | time: 0.008s\n",
            "| Adam | epoch: 319 | loss: 0.12281 - acc: 1.0000 -- iter: 16/36\n",
            "Training Step: 1593  | total loss: \u001b[1m\u001b[32m0.12058\u001b[0m\u001b[0m | time: 0.013s\n",
            "| Adam | epoch: 319 | loss: 0.12058 - acc: 1.0000 -- iter: 24/36\n",
            "Training Step: 1594  | total loss: \u001b[1m\u001b[32m0.12093\u001b[0m\u001b[0m | time: 0.020s\n",
            "| Adam | epoch: 319 | loss: 0.12093 - acc: 1.0000 -- iter: 32/36\n",
            "Training Step: 1595  | total loss: \u001b[1m\u001b[32m0.12359\u001b[0m\u001b[0m | time: 0.025s\n",
            "| Adam | epoch: 319 | loss: 0.12359 - acc: 1.0000 -- iter: 36/36\n",
            "--\n",
            "Training Step: 1596  | total loss: \u001b[1m\u001b[32m0.12591\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 320 | loss: 0.12591 - acc: 1.0000 -- iter: 08/36\n",
            "Training Step: 1597  | total loss: \u001b[1m\u001b[32m0.12743\u001b[0m\u001b[0m | time: 0.009s\n",
            "| Adam | epoch: 320 | loss: 0.12743 - acc: 1.0000 -- iter: 16/36\n",
            "Training Step: 1598  | total loss: \u001b[1m\u001b[32m0.12588\u001b[0m\u001b[0m | time: 0.014s\n",
            "| Adam | epoch: 320 | loss: 0.12588 - acc: 1.0000 -- iter: 24/36\n",
            "Training Step: 1599  | total loss: \u001b[1m\u001b[32m0.12588\u001b[0m\u001b[0m | time: 0.020s\n",
            "| Adam | epoch: 320 | loss: 0.12588 - acc: 1.0000 -- iter: 32/36\n",
            "Training Step: 1600  | total loss: \u001b[1m\u001b[32m0.12426\u001b[0m\u001b[0m | time: 0.028s\n",
            "| Adam | epoch: 320 | loss: 0.12426 - acc: 1.0000 -- iter: 36/36\n",
            "--\n",
            "Training Step: 1601  | total loss: \u001b[1m\u001b[32m0.12875\u001b[0m\u001b[0m | time: 0.006s\n",
            "| Adam | epoch: 321 | loss: 0.12875 - acc: 1.0000 -- iter: 08/36\n",
            "Training Step: 1602  | total loss: \u001b[1m\u001b[32m0.13255\u001b[0m\u001b[0m | time: 0.013s\n",
            "| Adam | epoch: 321 | loss: 0.13255 - acc: 1.0000 -- iter: 16/36\n",
            "Training Step: 1603  | total loss: \u001b[1m\u001b[32m0.12854\u001b[0m\u001b[0m | time: 0.019s\n",
            "| Adam | epoch: 321 | loss: 0.12854 - acc: 1.0000 -- iter: 24/36\n",
            "Training Step: 1604  | total loss: \u001b[1m\u001b[32m0.12717\u001b[0m\u001b[0m | time: 0.021s\n",
            "| Adam | epoch: 321 | loss: 0.12717 - acc: 1.0000 -- iter: 32/36\n",
            "Training Step: 1605  | total loss: \u001b[1m\u001b[32m0.12684\u001b[0m\u001b[0m | time: 0.026s\n",
            "| Adam | epoch: 321 | loss: 0.12684 - acc: 1.0000 -- iter: 36/36\n",
            "--\n",
            "Training Step: 1606  | total loss: \u001b[1m\u001b[32m0.12442\u001b[0m\u001b[0m | time: 0.004s\n",
            "| Adam | epoch: 322 | loss: 0.12442 - acc: 1.0000 -- iter: 08/36\n",
            "Training Step: 1607  | total loss: \u001b[1m\u001b[32m0.12542\u001b[0m\u001b[0m | time: 0.010s\n",
            "| Adam | epoch: 322 | loss: 0.12542 - acc: 1.0000 -- iter: 16/36\n",
            "Training Step: 1608  | total loss: \u001b[1m\u001b[32m0.12627\u001b[0m\u001b[0m | time: 0.015s\n",
            "| Adam | epoch: 322 | loss: 0.12627 - acc: 1.0000 -- iter: 24/36\n",
            "Training Step: 1609  | total loss: \u001b[1m\u001b[32m0.12561\u001b[0m\u001b[0m | time: 0.020s\n",
            "| Adam | epoch: 322 | loss: 0.12561 - acc: 1.0000 -- iter: 32/36\n",
            "Training Step: 1610  | total loss: \u001b[1m\u001b[32m0.12539\u001b[0m\u001b[0m | time: 0.023s\n",
            "| Adam | epoch: 322 | loss: 0.12539 - acc: 1.0000 -- iter: 36/36\n",
            "--\n",
            "Training Step: 1611  | total loss: \u001b[1m\u001b[32m1.35778\u001b[0m\u001b[0m | time: 0.003s\n",
            "| Adam | epoch: 323 | loss: 1.35778 - acc: 1.0000 -- iter: 08/36\n",
            "Training Step: 1612  | total loss: \u001b[1m\u001b[32m1.23220\u001b[0m\u001b[0m | time: 0.009s\n",
            "| Adam | epoch: 323 | loss: 1.23220 - acc: 0.9250 -- iter: 16/36\n",
            "Training Step: 1613  | total loss: \u001b[1m\u001b[32m1.12348\u001b[0m\u001b[0m | time: 0.013s\n",
            "| Adam | epoch: 323 | loss: 1.12348 - acc: 0.9325 -- iter: 24/36\n",
            "Training Step: 1614  | total loss: \u001b[1m\u001b[32m1.02563\u001b[0m\u001b[0m | time: 0.016s\n",
            "| Adam | epoch: 323 | loss: 1.02563 - acc: 0.9392 -- iter: 32/36\n",
            "Training Step: 1615  | total loss: \u001b[1m\u001b[32m0.93203\u001b[0m\u001b[0m | time: 0.020s\n",
            "| Adam | epoch: 323 | loss: 0.93203 - acc: 0.9453 -- iter: 36/36\n",
            "--\n",
            "Training Step: 1616  | total loss: \u001b[1m\u001b[32m0.85645\u001b[0m\u001b[0m | time: 0.004s\n",
            "| Adam | epoch: 324 | loss: 0.85645 - acc: 0.9508 -- iter: 08/36\n",
            "Training Step: 1617  | total loss: \u001b[1m\u001b[32m0.78077\u001b[0m\u001b[0m | time: 0.008s\n",
            "| Adam | epoch: 324 | loss: 0.78077 - acc: 0.9557 -- iter: 16/36\n",
            "Training Step: 1618  | total loss: \u001b[1m\u001b[32m0.71118\u001b[0m\u001b[0m | time: 0.011s\n",
            "| Adam | epoch: 324 | loss: 0.71118 - acc: 0.9601 -- iter: 24/36\n",
            "Training Step: 1619  | total loss: \u001b[1m\u001b[32m0.65180\u001b[0m\u001b[0m | time: 0.016s\n",
            "| Adam | epoch: 324 | loss: 0.65180 - acc: 0.9641 -- iter: 32/36\n",
            "Training Step: 1663  | total loss: \u001b[1m\u001b[32m0.10125\u001b[0m\u001b[0m | time: 0.015s\n",
            "| Adam | epoch: 333 | loss: 0.10125 - acc: 0.9997 -- iter: 24/36\n",
            "Training Step: 1664  | total loss: \u001b[1m\u001b[32m0.10414\u001b[0m\u001b[0m | time: 0.018s\n",
            "| Adam | epoch: 333 | loss: 0.10414 - acc: 0.9997 -- iter: 32/36\n",
            "Training Step: 1665  | total loss: \u001b[1m\u001b[32m0.60110\u001b[0m\u001b[0m | time: 0.021s\n",
            "| Adam | epoch: 333 | loss: 0.60110 - acc: 0.9997 -- iter: 36/36\n",
            "--\n",
            "Training Step: 1666  | total loss: \u001b[1m\u001b[32m0.55241\u001b[0m\u001b[0m | time: 0.003s\n",
            "| Adam | epoch: 334 | loss: 0.55241 - acc: 0.9372 -- iter: 08/36\n",
            "Training Step: 1667  | total loss: \u001b[1m\u001b[32m0.51217\u001b[0m\u001b[0m | time: 0.007s\n",
            "| Adam | epoch: 334 | loss: 0.51217 - acc: 0.9435 -- iter: 16/36\n",
            "Training Step: 1668  | total loss: \u001b[1m\u001b[32m0.51217\u001b[0m\u001b[0m | time: 0.012s\n",
            "| Adam | epoch: 334 | loss: 0.51217 - acc: 0.9492 -- iter: 24/36\n",
            "Training Step: 1669  | total loss: \u001b[1m\u001b[32m0.43927\u001b[0m\u001b[0m | time: 0.017s\n",
            "| Adam | epoch: 334 | loss: 0.43927 - acc: 0.9543 -- iter: 32/36\n",
            "Training Step: 1670  | total loss: \u001b[1m\u001b[32m0.40658\u001b[0m\u001b[0m | time: 0.022s\n",
            "| Adam | epoch: 334 | loss: 0.40658 - acc: 0.9588 -- iter: 36/36\n",
            "--\n",
            "Training Step: 1671  | total loss: \u001b[1m\u001b[32m1.55525\u001b[0m\u001b[0m | time: 0.004s\n",
            "| Adam | epoch: 335 | loss: 1.55525 - acc: 0.9629 -- iter: 08/36\n",
            "Training Step: 1672  | total loss: \u001b[1m\u001b[32m1.40912\u001b[0m\u001b[0m | time: 0.008s\n",
            "| Adam | epoch: 335 | loss: 1.40912 - acc: 0.8667 -- iter: 16/36\n",
            "Training Step: 1673  | total loss: \u001b[1m\u001b[32m1.28088\u001b[0m\u001b[0m | time: 0.012s\n",
            "| Adam | epoch: 335 | loss: 1.28088 - acc: 0.8800 -- iter: 24/36\n",
            "Training Step: 1674  | total loss: \u001b[1m\u001b[32m1.16549\u001b[0m\u001b[0m | time: 0.018s\n",
            "| Adam | epoch: 335 | loss: 1.16549 - acc: 0.8920 -- iter: 32/36\n",
            "Training Step: 1675  | total loss: \u001b[1m\u001b[32m1.05665\u001b[0m\u001b[0m | time: 0.024s\n",
            "| Adam | epoch: 335 | loss: 1.05665 - acc: 0.9028 -- iter: 36/36\n",
            "--\n",
            "Training Step: 1676  | total loss: \u001b[1m\u001b[32m0.96294\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 336 | loss: 0.96294 - acc: 0.9125 -- iter: 08/36\n",
            "Training Step: 1677  | total loss: \u001b[1m\u001b[32m0.88050\u001b[0m\u001b[0m | time: 0.010s\n",
            "| Adam | epoch: 336 | loss: 0.88050 - acc: 0.9213 -- iter: 16/36\n",
            "Training Step: 1678  | total loss: \u001b[1m\u001b[32m0.80142\u001b[0m\u001b[0m | time: 0.014s\n",
            "| Adam | epoch: 336 | loss: 0.80142 - acc: 0.9291 -- iter: 24/36\n",
            "Training Step: 1679  | total loss: \u001b[1m\u001b[32m0.73035\u001b[0m\u001b[0m | time: 0.018s\n",
            "| Adam | epoch: 336 | loss: 0.73035 - acc: 0.9362 -- iter: 32/36\n",
            "Training Step: 1680  | total loss: \u001b[1m\u001b[32m0.66632\u001b[0m\u001b[0m | time: 0.026s\n",
            "| Adam | epoch: 336 | loss: 0.66632 - acc: 0.9426 -- iter: 36/36\n",
            "--\n",
            "Training Step: 1681  | total loss: \u001b[1m\u001b[32m0.61215\u001b[0m\u001b[0m | time: 0.003s\n",
            "| Adam | epoch: 337 | loss: 0.61215 - acc: 0.9535 -- iter: 08/36\n",
            "Training Step: 1682  | total loss: \u001b[1m\u001b[32m0.56134\u001b[0m\u001b[0m | time: 0.009s\n",
            "| Adam | epoch: 337 | loss: 0.56134 - acc: 0.9582 -- iter: 16/36\n",
            "Training Step: 1683  | total loss: \u001b[1m\u001b[32m0.51828\u001b[0m\u001b[0m | time: 0.013s\n",
            "| Adam | epoch: 337 | loss: 0.51828 - acc: 0.9623 -- iter: 24/36\n",
            "Training Step: 1684  | total loss: \u001b[1m\u001b[32m0.47649\u001b[0m\u001b[0m | time: 0.016s\n",
            "| Adam | epoch: 337 | loss: 0.47649 - acc: 0.9661 -- iter: 32/36\n",
            "Training Step: 1685  | total loss: \u001b[1m\u001b[32m0.44356\u001b[0m\u001b[0m | time: 0.021s\n",
            "| Adam | epoch: 337 | loss: 0.44356 - acc: 0.9695 -- iter: 36/36\n",
            "--\n",
            "Training Step: 1686  | total loss: \u001b[1m\u001b[32m0.41387\u001b[0m\u001b[0m | time: 0.004s\n",
            "| Adam | epoch: 338 | loss: 0.41387 - acc: 0.9725 -- iter: 08/36\n",
            "Training Step: 1687  | total loss: \u001b[1m\u001b[32m0.38169\u001b[0m\u001b[0m | time: 0.009s\n",
            "| Adam | epoch: 338 | loss: 0.38169 - acc: 0.9725 -- iter: 16/36\n",
            "Training Step: 1688  | total loss: \u001b[1m\u001b[32m0.35321\u001b[0m\u001b[0m | time: 0.014s\n",
            "| Adam | epoch: 338 | loss: 0.35321 - acc: 0.9753 -- iter: 24/36\n",
            "Training Step: 1689  | total loss: \u001b[1m\u001b[32m0.33065\u001b[0m\u001b[0m | time: 0.019s\n",
            "| Adam | epoch: 338 | loss: 0.33065 - acc: 0.9778 -- iter: 32/36\n",
            "Training Step: 1690  | total loss: \u001b[1m\u001b[32m0.31043\u001b[0m\u001b[0m | time: 0.023s\n",
            "| Adam | epoch: 338 | loss: 0.31043 - acc: 0.9800 -- iter: 36/36\n",
            "--\n",
            "Training Step: 1691  | total loss: \u001b[1m\u001b[32m0.29071\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 339 | loss: 0.29071 - acc: 0.9820 -- iter: 08/36\n",
            "Training Step: 1692  | total loss: \u001b[1m\u001b[32m0.27287\u001b[0m\u001b[0m | time: 0.010s\n",
            "| Adam | epoch: 339 | loss: 0.27287 - acc: 0.9838 -- iter: 16/36\n",
            "Training Step: 1693  | total loss: \u001b[1m\u001b[32m0.25552\u001b[0m\u001b[0m | time: 0.015s\n",
            "| Adam | epoch: 339 | loss: 0.25552 - acc: 0.9854 -- iter: 24/36\n",
            "Training Step: 1694  | total loss: \u001b[1m\u001b[32m0.24233\u001b[0m\u001b[0m | time: 0.019s\n",
            "| Adam | epoch: 339 | loss: 0.24233 - acc: 0.9869 -- iter: 32/36\n",
            "Training Step: 1695  | total loss: \u001b[1m\u001b[32m0.22590\u001b[0m\u001b[0m | time: 0.024s\n",
            "| Adam | epoch: 339 | loss: 0.22590 - acc: 0.9882 -- iter: 36/36\n",
            "--\n",
            "Training Step: 1696  | total loss: \u001b[1m\u001b[32m0.21264\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 340 | loss: 0.21264 - acc: 0.9894 -- iter: 08/36\n",
            "Training Step: 1697  | total loss: \u001b[1m\u001b[32m0.20552\u001b[0m\u001b[0m | time: 0.009s\n",
            "| Adam | epoch: 340 | loss: 0.20552 - acc: 0.9904 -- iter: 16/36\n",
            "Training Step: 1698  | total loss: \u001b[1m\u001b[32m0.19905\u001b[0m\u001b[0m | time: 0.013s\n",
            "| Adam | epoch: 340 | loss: 0.19905 - acc: 0.9922 -- iter: 24/36\n",
            "Training Step: 1699  | total loss: \u001b[1m\u001b[32m0.18842\u001b[0m\u001b[0m | time: 0.017s\n",
            "| Adam | epoch: 340 | loss: 0.18842 - acc: 0.9930 -- iter: 32/36\n",
            "Training Step: 1700  | total loss: \u001b[1m\u001b[32m0.18401\u001b[0m\u001b[0m | time: 0.021s\n",
            "| Adam | epoch: 340 | loss: 0.18401 - acc: 0.9937 -- iter: 36/36\n",
            "--\n",
            "Training Step: 1701  | total loss: \u001b[1m\u001b[32m0.17342\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 341 | loss: 0.17342 - acc: 0.9943 -- iter: 08/36\n",
            "Training Step: 1702  | total loss: \u001b[1m\u001b[32m0.16840\u001b[0m\u001b[0m | time: 0.008s\n",
            "| Adam | epoch: 341 | loss: 0.16840 - acc: 0.9949 -- iter: 16/36\n",
            "Training Step: 1703  | total loss: \u001b[1m\u001b[32m0.16209\u001b[0m\u001b[0m | time: 0.013s\n",
            "| Adam | epoch: 341 | loss: 0.16209 - acc: 0.9954 -- iter: 24/36\n",
            "Training Step: 1704  | total loss: \u001b[1m\u001b[32m0.15631\u001b[0m\u001b[0m | time: 0.017s\n",
            "| Adam | epoch: 341 | loss: 0.15631 - acc: 0.9959 -- iter: 32/36\n",
            "Training Step: 1705  | total loss: \u001b[1m\u001b[32m0.14816\u001b[0m\u001b[0m | time: 0.021s\n",
            "| Adam | epoch: 341 | loss: 0.14816 - acc: 0.9963 -- iter: 36/36\n",
            "--\n",
            "Training Step: 1706  | total loss: \u001b[1m\u001b[32m0.14631\u001b[0m\u001b[0m | time: 0.006s\n",
            "| Adam | epoch: 342 | loss: 0.14631 - acc: 0.9967 -- iter: 08/36\n",
            "Training Step: 1707  | total loss: \u001b[1m\u001b[32m0.14102\u001b[0m\u001b[0m | time: 0.010s\n",
            "| Adam | epoch: 342 | loss: 0.14102 - acc: 0.9970 -- iter: 16/36\n",
            "Training Step: 1708  | total loss: \u001b[1m\u001b[32m0.14102\u001b[0m\u001b[0m | time: 0.014s\n",
            "| Adam | epoch: 342 | loss: 0.14102 - acc: 0.9973 -- iter: 24/36\n",
            "Training Step: 1709  | total loss: \u001b[1m\u001b[32m0.13779\u001b[0m\u001b[0m | time: 0.018s\n",
            "| Adam | epoch: 342 | loss: 0.13779 - acc: 0.9976 -- iter: 32/36\n",
            "Training Step: 1710  | total loss: \u001b[1m\u001b[32m0.13720\u001b[0m\u001b[0m | time: 0.024s\n",
            "| Adam | epoch: 342 | loss: 0.13720 - acc: 0.9978 -- iter: 36/36\n",
            "--\n",
            "Training Step: 1711  | total loss: \u001b[1m\u001b[32m0.13660\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 343 | loss: 0.13660 - acc: 0.9980 -- iter: 08/36\n",
            "Training Step: 1712  | total loss: \u001b[1m\u001b[32m0.13871\u001b[0m\u001b[0m | time: 0.010s\n",
            "| Adam | epoch: 343 | loss: 0.13871 - acc: 0.9982 -- iter: 16/36\n",
            "Training Step: 1713  | total loss: \u001b[1m\u001b[32m0.12984\u001b[0m\u001b[0m | time: 0.015s\n",
            "| Adam | epoch: 343 | loss: 0.12984 - acc: 0.9984 -- iter: 24/36\n",
            "Training Step: 1714  | total loss: \u001b[1m\u001b[32m0.12521\u001b[0m\u001b[0m | time: 0.022s\n",
            "| Adam | epoch: 343 | loss: 0.12521 - acc: 0.9986 -- iter: 32/36\n",
            "Training Step: 1715  | total loss: \u001b[1m\u001b[32m0.12274\u001b[0m\u001b[0m | time: 0.028s\n",
            "| Adam | epoch: 343 | loss: 0.12274 - acc: 0.9986 -- iter: 36/36\n",
            "--\n",
            "Training Step: 1716  | total loss: \u001b[1m\u001b[32m0.12151\u001b[0m\u001b[0m | time: 0.006s\n",
            "| Adam | epoch: 344 | loss: 0.12151 - acc: 0.9987 -- iter: 08/36\n",
            "Training Step: 1717  | total loss: \u001b[1m\u001b[32m0.12035\u001b[0m\u001b[0m | time: 0.013s\n",
            "| Adam | epoch: 344 | loss: 0.12035 - acc: 0.9988 -- iter: 16/36\n",
            "Training Step: 1718  | total loss: \u001b[1m\u001b[32m0.11776\u001b[0m\u001b[0m | time: 0.020s\n",
            "| Adam | epoch: 344 | loss: 0.11776 - acc: 0.9990 -- iter: 24/36\n",
            "Training Step: 1719  | total loss: \u001b[1m\u001b[32m0.11654\u001b[0m\u001b[0m | time: 0.025s\n",
            "| Adam | epoch: 344 | loss: 0.11654 - acc: 0.9991 -- iter: 32/36\n",
            "Training Step: 1720  | total loss: \u001b[1m\u001b[32m0.11505\u001b[0m\u001b[0m | time: 0.034s\n",
            "| Adam | epoch: 344 | loss: 0.11505 - acc: 0.9992 -- iter: 36/36\n",
            "--\n",
            "Training Step: 1721  | total loss: \u001b[1m\u001b[32m0.11624\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 345 | loss: 0.11624 - acc: 0.9992 -- iter: 08/36\n",
            "Training Step: 1722  | total loss: \u001b[1m\u001b[32m0.11351\u001b[0m\u001b[0m | time: 0.010s\n",
            "| Adam | epoch: 345 | loss: 0.11351 - acc: 0.9993 -- iter: 16/36\n",
            "Training Step: 1723  | total loss: \u001b[1m\u001b[32m0.11100\u001b[0m\u001b[0m | time: 0.016s\n",
            "| Adam | epoch: 345 | loss: 0.11100 - acc: 0.9994 -- iter: 24/36\n",
            "Training Step: 1724  | total loss: \u001b[1m\u001b[32m0.11134\u001b[0m\u001b[0m | time: 0.023s\n",
            "| Adam | epoch: 345 | loss: 0.11134 - acc: 0.9994 -- iter: 32/36\n",
            "Training Step: 1725  | total loss: \u001b[1m\u001b[32m0.10883\u001b[0m\u001b[0m | time: 0.029s\n",
            "| Adam | epoch: 345 | loss: 0.10883 - acc: 0.9995 -- iter: 36/36\n",
            "--\n",
            "Training Step: 1726  | total loss: \u001b[1m\u001b[32m0.10577\u001b[0m\u001b[0m | time: 0.003s\n",
            "| Adam | epoch: 346 | loss: 0.10577 - acc: 0.9996 -- iter: 08/36\n",
            "Training Step: 1727  | total loss: \u001b[1m\u001b[32m0.10442\u001b[0m\u001b[0m | time: 0.008s\n",
            "| Adam | epoch: 346 | loss: 0.10442 - acc: 0.9996 -- iter: 16/36\n",
            "Training Step: 1728  | total loss: \u001b[1m\u001b[32m0.10316\u001b[0m\u001b[0m | time: 0.012s\n",
            "| Adam | epoch: 346 | loss: 0.10316 - acc: 0.9997 -- iter: 24/36\n",
            "Training Step: 1729  | total loss: \u001b[1m\u001b[32m0.10316\u001b[0m\u001b[0m | time: 0.017s\n",
            "| Adam | epoch: 346 | loss: 0.10316 - acc: 0.9997 -- iter: 32/36\n",
            "Training Step: 1730  | total loss: \u001b[1m\u001b[32m0.10011\u001b[0m\u001b[0m | time: 0.020s\n",
            "| Adam | epoch: 346 | loss: 0.10011 - acc: 0.9997 -- iter: 36/36\n",
            "--\n",
            "Training Step: 1731  | total loss: \u001b[1m\u001b[32m0.10202\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 347 | loss: 0.10202 - acc: 0.9998 -- iter: 08/36\n",
            "Training Step: 1732  | total loss: \u001b[1m\u001b[32m0.10350\u001b[0m\u001b[0m | time: 0.010s\n",
            "| Adam | epoch: 347 | loss: 0.10350 - acc: 0.9998 -- iter: 16/36\n",
            "Training Step: 1733  | total loss: \u001b[1m\u001b[32m0.10350\u001b[0m\u001b[0m | time: 0.016s\n",
            "| Adam | epoch: 347 | loss: 0.10350 - acc: 0.9998 -- iter: 24/36\n",
            "Training Step: 1734  | total loss: \u001b[1m\u001b[32m0.09780\u001b[0m\u001b[0m | time: 0.021s\n",
            "| Adam | epoch: 347 | loss: 0.09780 - acc: 0.9998 -- iter: 32/36\n",
            "Training Step: 1735  | total loss: \u001b[1m\u001b[32m0.09780\u001b[0m\u001b[0m | time: 0.026s\n",
            "| Adam | epoch: 347 | loss: 0.09780 - acc: 0.9998 -- iter: 36/36\n",
            "--\n",
            "Training Step: 1736  | total loss: \u001b[1m\u001b[32m0.09925\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 348 | loss: 0.09925 - acc: 0.9998 -- iter: 08/36\n",
            "Training Step: 1737  | total loss: \u001b[1m\u001b[32m0.09925\u001b[0m\u001b[0m | time: 0.010s\n",
            "| Adam | epoch: 348 | loss: 0.09925 - acc: 0.9999 -- iter: 16/36\n",
            "Training Step: 1738  | total loss: \u001b[1m\u001b[32m0.09469\u001b[0m\u001b[0m | time: 0.014s\n",
            "| Adam | epoch: 348 | loss: 0.09469 - acc: 0.9999 -- iter: 24/36\n",
            "Training Step: 1739  | total loss: \u001b[1m\u001b[32m0.09343\u001b[0m\u001b[0m | time: 0.020s\n",
            "| Adam | epoch: 348 | loss: 0.09343 - acc: 0.9999 -- iter: 32/36\n",
            "Training Step: 1740  | total loss: \u001b[1m\u001b[32m0.09223\u001b[0m\u001b[0m | time: 0.025s\n",
            "| Adam | epoch: 348 | loss: 0.09223 - acc: 0.9999 -- iter: 36/36\n",
            "--\n",
            "Training Step: 1741  | total loss: \u001b[1m\u001b[32m0.09223\u001b[0m\u001b[0m | time: 0.006s\n",
            "| Adam | epoch: 349 | loss: 0.09223 - acc: 0.9999 -- iter: 08/36\n",
            "Training Step: 1742  | total loss: \u001b[1m\u001b[32m0.09261\u001b[0m\u001b[0m | time: 0.012s\n",
            "| Adam | epoch: 349 | loss: 0.09261 - acc: 0.9999 -- iter: 16/36\n",
            "Training Step: 1743  | total loss: \u001b[1m\u001b[32m0.09261\u001b[0m\u001b[0m | time: 0.018s\n",
            "| Adam | epoch: 349 | loss: 0.09261 - acc: 0.9999 -- iter: 24/36\n",
            "Training Step: 1744  | total loss: \u001b[1m\u001b[32m0.09597\u001b[0m\u001b[0m | time: 0.024s\n",
            "| Adam | epoch: 349 | loss: 0.09597 - acc: 0.9999 -- iter: 32/36\n",
            "Training Step: 1745  | total loss: \u001b[1m\u001b[32m0.09513\u001b[0m\u001b[0m | time: 0.030s\n",
            "| Adam | epoch: 349 | loss: 0.09513 - acc: 0.9999 -- iter: 36/36\n",
            "--\n",
            "Training Step: 1746  | total loss: \u001b[1m\u001b[32m0.09513\u001b[0m\u001b[0m | time: 0.006s\n",
            "| Adam | epoch: 350 | loss: 0.09513 - acc: 0.9999 -- iter: 08/36\n",
            "Training Step: 1747  | total loss: \u001b[1m\u001b[32m0.08843\u001b[0m\u001b[0m | time: 0.011s\n",
            "| Adam | epoch: 350 | loss: 0.08843 - acc: 1.0000 -- iter: 16/36\n",
            "Training Step: 1748  | total loss: \u001b[1m\u001b[32m0.08843\u001b[0m\u001b[0m | time: 0.018s\n",
            "| Adam | epoch: 350 | loss: 0.08843 - acc: 1.0000 -- iter: 24/36\n",
            "Training Step: 1749  | total loss: \u001b[1m\u001b[32m0.08955\u001b[0m\u001b[0m | time: 0.023s\n",
            "| Adam | epoch: 350 | loss: 0.08955 - acc: 1.0000 -- iter: 32/36\n",
            "Training Step: 1750  | total loss: \u001b[1m\u001b[32m0.09029\u001b[0m\u001b[0m | time: 0.028s\n",
            "| Adam | epoch: 350 | loss: 0.09029 - acc: 1.0000 -- iter: 36/36\n",
            "--\n",
            "Training Step: 1751  | total loss: \u001b[1m\u001b[32m0.09177\u001b[0m\u001b[0m | time: 0.004s\n",
            "| Adam | epoch: 351 | loss: 0.09177 - acc: 1.0000 -- iter: 08/36\n",
            "Training Step: 1752  | total loss: \u001b[1m\u001b[32m0.09304\u001b[0m\u001b[0m | time: 0.009s\n",
            "| Adam | epoch: 351 | loss: 0.09304 - acc: 1.0000 -- iter: 16/36\n",
            "Training Step: 1753  | total loss: \u001b[1m\u001b[32m0.08761\u001b[0m\u001b[0m | time: 0.013s\n",
            "| Adam | epoch: 351 | loss: 0.08761 - acc: 1.0000 -- iter: 24/36\n",
            "Training Step: 1754  | total loss: \u001b[1m\u001b[32m0.08761\u001b[0m\u001b[0m | time: 0.020s\n",
            "| Adam | epoch: 351 | loss: 0.08761 - acc: 1.0000 -- iter: 32/36\n",
            "Training Step: 1755  | total loss: \u001b[1m\u001b[32m0.75714\u001b[0m\u001b[0m | time: 0.025s\n",
            "| Adam | epoch: 351 | loss: 0.75714 - acc: 1.0000 -- iter: 36/36\n",
            "--\n",
            "Training Step: 1756  | total loss: \u001b[1m\u001b[32m0.75714\u001b[0m\u001b[0m | time: 0.006s\n",
            "| Adam | epoch: 352 | loss: 0.75714 - acc: 0.9125 -- iter: 08/36\n",
            "Training Step: 1757  | total loss: \u001b[1m\u001b[32m0.62876\u001b[0m\u001b[0m | time: 0.010s\n",
            "| Adam | epoch: 352 | loss: 0.62876 - acc: 0.9212 -- iter: 16/36\n",
            "Training Step: 1758  | total loss: \u001b[1m\u001b[32m0.57854\u001b[0m\u001b[0m | time: 0.016s\n",
            "| Adam | epoch: 352 | loss: 0.57854 - acc: 0.9291 -- iter: 24/36\n",
            "Training Step: 1759  | total loss: \u001b[1m\u001b[32m0.52857\u001b[0m\u001b[0m | time: 0.021s\n",
            "| Adam | epoch: 352 | loss: 0.52857 - acc: 0.9362 -- iter: 32/36\n",
            "Training Step: 1760  | total loss: \u001b[1m\u001b[32m0.49098\u001b[0m\u001b[0m | time: 0.028s\n",
            "| Adam | epoch: 352 | loss: 0.49098 - acc: 0.9426 -- iter: 36/36\n",
            "--\n",
            "Training Step: 1761  | total loss: \u001b[1m\u001b[32m0.97884\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 353 | loss: 0.97884 - acc: 0.9483 -- iter: 08/36\n",
            "Training Step: 1762  | total loss: \u001b[1m\u001b[32m0.89702\u001b[0m\u001b[0m | time: 0.014s\n",
            "| Adam | epoch: 353 | loss: 0.89702 - acc: 0.8910 -- iter: 16/36\n",
            "Training Step: 1763  | total loss: \u001b[1m\u001b[32m0.81419\u001b[0m\u001b[0m | time: 0.021s\n",
            "| Adam | epoch: 353 | loss: 0.81419 - acc: 0.9019 -- iter: 24/36\n",
            "Training Step: 1764  | total loss: \u001b[1m\u001b[32m0.81419\u001b[0m\u001b[0m | time: 0.028s\n",
            "| Adam | epoch: 353 | loss: 0.81419 - acc: 0.9117 -- iter: 32/36\n",
            "Training Step: 1765  | total loss: \u001b[1m\u001b[32m0.73962\u001b[0m\u001b[0m | time: 0.035s\n",
            "| Adam | epoch: 353 | loss: 0.73962 - acc: 0.9205 -- iter: 36/36\n",
            "--\n",
            "Training Step: 1766  | total loss: \u001b[1m\u001b[32m0.61666\u001b[0m\u001b[0m | time: 0.007s\n",
            "| Adam | epoch: 354 | loss: 0.61666 - acc: 0.9356 -- iter: 08/36\n",
            "Training Step: 1767  | total loss: \u001b[1m\u001b[32m0.61666\u001b[0m\u001b[0m | time: 0.018s\n",
            "| Adam | epoch: 354 | loss: 0.61666 - acc: 0.9356 -- iter: 16/36\n",
            "Training Step: 1768  | total loss: \u001b[1m\u001b[32m0.51616\u001b[0m\u001b[0m | time: 0.022s\n",
            "| Adam | epoch: 354 | loss: 0.51616 - acc: 0.9421 -- iter: 24/36\n",
            "Training Step: 1769  | total loss: \u001b[1m\u001b[32m0.51616\u001b[0m\u001b[0m | time: 0.030s\n",
            "| Adam | epoch: 354 | loss: 0.51616 - acc: 0.9479 -- iter: 32/36\n",
            "Training Step: 1770  | total loss: \u001b[1m\u001b[32m0.47940\u001b[0m\u001b[0m | time: 0.039s\n",
            "| Adam | epoch: 354 | loss: 0.47940 - acc: 0.9531 -- iter: 36/36\n",
            "--\n",
            "Training Step: 1771  | total loss: \u001b[1m\u001b[32m0.44632\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 355 | loss: 0.44632 - acc: 0.9578 -- iter: 08/36\n",
            "Training Step: 1772  | total loss: \u001b[1m\u001b[32m0.41083\u001b[0m\u001b[0m | time: 0.011s\n",
            "| Adam | epoch: 355 | loss: 0.41083 - acc: 0.9620 -- iter: 16/36\n",
            "Training Step: 1773  | total loss: \u001b[1m\u001b[32m0.37806\u001b[0m\u001b[0m | time: 0.018s\n",
            "| Adam | epoch: 355 | loss: 0.37806 - acc: 0.9658 -- iter: 24/36\n",
            "Training Step: 1774  | total loss: \u001b[1m\u001b[32m1.27300\u001b[0m\u001b[0m | time: 0.026s\n",
            "| Adam | epoch: 355 | loss: 1.27300 - acc: 0.9067 -- iter: 32/36\n",
            "Training Step: 1775  | total loss: \u001b[1m\u001b[32m1.15840\u001b[0m\u001b[0m | time: 0.031s\n",
            "| Adam | epoch: 355 | loss: 1.15840 - acc: 0.9160 -- iter: 36/36\n",
            "--\n",
            "Training Step: 1776  | total loss: \u001b[1m\u001b[32m1.05297\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 356 | loss: 1.05297 - acc: 0.9244 -- iter: 08/36\n",
            "Training Step: 1777  | total loss: \u001b[1m\u001b[32m0.95814\u001b[0m\u001b[0m | time: 0.011s\n",
            "| Adam | epoch: 356 | loss: 0.95814 - acc: 0.9320 -- iter: 16/36\n",
            "Training Step: 1778  | total loss: \u001b[1m\u001b[32m0.87043\u001b[0m\u001b[0m | time: 0.015s\n",
            "| Adam | epoch: 356 | loss: 0.87043 - acc: 0.9388 -- iter: 24/36\n",
            "Training Step: 1779  | total loss: \u001b[1m\u001b[32m2.25619\u001b[0m\u001b[0m | time: 0.022s\n",
            "| Adam | epoch: 356 | loss: 2.25619 - acc: 0.9449 -- iter: 32/36\n",
            "Training Step: 1780  | total loss: \u001b[1m\u001b[32m2.25619\u001b[0m\u001b[0m | time: 0.029s\n",
            "| Adam | epoch: 356 | loss: 2.25619 - acc: 0.8754 -- iter: 36/36\n",
            "--\n",
            "Training Step: 1781  | total loss: \u001b[1m\u001b[32m2.03917\u001b[0m\u001b[0m | time: 0.004s\n",
            "| Adam | epoch: 357 | loss: 2.03917 - acc: 0.8879 -- iter: 08/36\n",
            "Training Step: 1782  | total loss: \u001b[1m\u001b[32m1.67030\u001b[0m\u001b[0m | time: 0.011s\n",
            "| Adam | epoch: 357 | loss: 1.67030 - acc: 0.8991 -- iter: 16/36\n",
            "Training Step: 1783  | total loss: \u001b[1m\u001b[32m1.67030\u001b[0m\u001b[0m | time: 0.019s\n",
            "| Adam | epoch: 357 | loss: 1.67030 - acc: 0.9092 -- iter: 24/36\n",
            "Training Step: 1784  | total loss: \u001b[1m\u001b[32m1.51349\u001b[0m\u001b[0m | time: 0.027s\n",
            "| Adam | epoch: 357 | loss: 1.51349 - acc: 0.9183 -- iter: 32/36\n",
            "Training Step: 1785  | total loss: \u001b[1m\u001b[32m2.69282\u001b[0m\u001b[0m | time: 0.032s\n",
            "| Adam | epoch: 357 | loss: 2.69282 - acc: 0.9264 -- iter: 36/36\n",
            "--\n",
            "Training Step: 1786  | total loss: \u001b[1m\u001b[32m2.69282\u001b[0m\u001b[0m | time: 0.007s\n",
            "| Adam | epoch: 358 | loss: 2.69282 - acc: 0.8338 -- iter: 08/36\n",
            "Training Step: 1787  | total loss: \u001b[1m\u001b[32m2.43590\u001b[0m\u001b[0m | time: 0.014s\n",
            "| Adam | epoch: 358 | loss: 2.43590 - acc: 0.8504 -- iter: 16/36\n",
            "Training Step: 1788  | total loss: \u001b[1m\u001b[32m1.98380\u001b[0m\u001b[0m | time: 0.021s\n",
            "| Adam | epoch: 358 | loss: 1.98380 - acc: 0.8654 -- iter: 24/36\n",
            "Training Step: 1789  | total loss: \u001b[1m\u001b[32m1.98380\u001b[0m\u001b[0m | time: 0.028s\n",
            "| Adam | epoch: 358 | loss: 1.98380 - acc: 0.8788 -- iter: 32/36\n",
            "Training Step: 1790  | total loss: \u001b[1m\u001b[32m1.79967\u001b[0m\u001b[0m | time: 0.034s\n",
            "| Adam | epoch: 358 | loss: 1.79967 - acc: 0.8910 -- iter: 36/36\n",
            "--\n",
            "Training Step: 1791  | total loss: \u001b[1m\u001b[32m1.62448\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 359 | loss: 1.62448 - acc: 0.9019 -- iter: 08/36\n",
            "Training Step: 1792  | total loss: \u001b[1m\u001b[32m1.47258\u001b[0m\u001b[0m | time: 0.014s\n",
            "| Adam | epoch: 359 | loss: 1.47258 - acc: 0.9117 -- iter: 16/36\n",
            "Training Step: 1793  | total loss: \u001b[1m\u001b[32m1.33189\u001b[0m\u001b[0m | time: 0.020s\n",
            "| Adam | epoch: 359 | loss: 1.33189 - acc: 0.9205 -- iter: 24/36\n",
            "Training Step: 1794  | total loss: \u001b[1m\u001b[32m1.21386\u001b[0m\u001b[0m | time: 0.027s\n",
            "| Adam | epoch: 359 | loss: 1.21386 - acc: 0.9285 -- iter: 32/36\n",
            "Training Step: 1795  | total loss: \u001b[1m\u001b[32m1.10774\u001b[0m\u001b[0m | time: 0.035s\n",
            "| Adam | epoch: 359 | loss: 1.10774 - acc: 0.9356 -- iter: 36/36\n",
            "--\n",
            "Training Step: 1796  | total loss: \u001b[1m\u001b[32m0.91920\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 360 | loss: 0.91920 - acc: 0.9420 -- iter: 08/36\n",
            "Training Step: 1797  | total loss: \u001b[1m\u001b[32m0.91920\u001b[0m\u001b[0m | time: 0.012s\n",
            "| Adam | epoch: 360 | loss: 0.91920 - acc: 0.9478 -- iter: 16/36\n",
            "Training Step: 1798  | total loss: \u001b[1m\u001b[32m1.63862\u001b[0m\u001b[0m | time: 0.017s\n",
            "| Adam | epoch: 360 | loss: 1.63862 - acc: 0.8656 -- iter: 24/36\n",
            "Training Step: 1799  | total loss: \u001b[1m\u001b[32m1.63862\u001b[0m\u001b[0m | time: 0.022s\n",
            "| Adam | epoch: 360 | loss: 1.63862 - acc: 0.8790 -- iter: 32/36\n",
            "Training Step: 1800  | total loss: \u001b[1m\u001b[32m1.34899\u001b[0m\u001b[0m | time: 0.028s\n",
            "| Adam | epoch: 360 | loss: 1.34899 - acc: 0.8911 -- iter: 36/36\n",
            "--\n",
            "Training Step: 1801  | total loss: \u001b[1m\u001b[32m1.22190\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 361 | loss: 1.22190 - acc: 0.9020 -- iter: 08/36\n",
            "Training Step: 1802  | total loss: \u001b[1m\u001b[32m1.10930\u001b[0m\u001b[0m | time: 0.014s\n",
            "| Adam | epoch: 361 | loss: 1.10930 - acc: 0.9118 -- iter: 16/36\n",
            "Training Step: 1803  | total loss: \u001b[1m\u001b[32m2.23492\u001b[0m\u001b[0m | time: 0.019s\n",
            "| Adam | epoch: 361 | loss: 2.23492 - acc: 0.9206 -- iter: 24/36\n",
            "Training Step: 1804  | total loss: \u001b[1m\u001b[32m2.02067\u001b[0m\u001b[0m | time: 0.024s\n",
            "| Adam | epoch: 361 | loss: 2.02067 - acc: 0.8411 -- iter: 32/36\n",
            "Training Step: 1805  | total loss: \u001b[1m\u001b[32m1.82762\u001b[0m\u001b[0m | time: 0.029s\n",
            "| Adam | epoch: 361 | loss: 1.82762 - acc: 0.8569 -- iter: 36/36\n",
            "--\n",
            "Training Step: 1806  | total loss: \u001b[1m\u001b[32m1.82762\u001b[0m\u001b[0m | time: 0.008s\n",
            "| Adam | epoch: 362 | loss: 1.82762 - acc: 0.8713 -- iter: 08/36\n",
            "Training Step: 1807  | total loss: \u001b[1m\u001b[32m1.65391\u001b[0m\u001b[0m | time: 0.014s\n",
            "| Adam | epoch: 362 | loss: 1.65391 - acc: 0.8841 -- iter: 16/36\n",
            "Training Step: 1808  | total loss: \u001b[1m\u001b[32m1.50100\u001b[0m\u001b[0m | time: 0.020s\n",
            "| Adam | epoch: 362 | loss: 1.50100 - acc: 0.8957 -- iter: 24/36\n",
            "Training Step: 1809  | total loss: \u001b[1m\u001b[32m2.50702\u001b[0m\u001b[0m | time: 0.025s\n",
            "| Adam | epoch: 362 | loss: 2.50702 - acc: 0.8155 -- iter: 32/36\n",
            "Training Step: 1810  | total loss: \u001b[1m\u001b[32m2.50702\u001b[0m\u001b[0m | time: 0.032s\n",
            "| Adam | epoch: 362 | loss: 2.50702 - acc: 0.8155 -- iter: 36/36\n",
            "--\n",
            "Training Step: 1811  | total loss: \u001b[1m\u001b[32m2.05586\u001b[0m\u001b[0m | time: 0.004s\n",
            "| Adam | epoch: 363 | loss: 2.05586 - acc: 0.8340 -- iter: 08/36\n",
            "Training Step: 1812  | total loss: \u001b[1m\u001b[32m2.05586\u001b[0m\u001b[0m | time: 0.010s\n",
            "| Adam | epoch: 363 | loss: 2.05586 - acc: 0.8506 -- iter: 16/36\n",
            "Training Step: 1813  | total loss: \u001b[1m\u001b[32m1.68260\u001b[0m\u001b[0m | time: 0.013s\n",
            "| Adam | epoch: 363 | loss: 1.68260 - acc: 0.8655 -- iter: 24/36\n",
            "Training Step: 1814  | total loss: \u001b[1m\u001b[32m1.68260\u001b[0m\u001b[0m | time: 0.018s\n",
            "| Adam | epoch: 363 | loss: 1.68260 - acc: 0.8790 -- iter: 32/36\n",
            "Training Step: 1815  | total loss: \u001b[1m\u001b[32m2.78931\u001b[0m\u001b[0m | time: 0.024s\n",
            "| Adam | epoch: 363 | loss: 2.78931 - acc: 0.8911 -- iter: 36/36\n",
            "--\n",
            "Training Step: 1816  | total loss: \u001b[1m\u001b[32m2.78931\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 364 | loss: 2.78931 - acc: 0.8020 -- iter: 08/36\n",
            "Training Step: 1817  | total loss: \u001b[1m\u001b[32m2.28678\u001b[0m\u001b[0m | time: 0.009s\n",
            "| Adam | epoch: 364 | loss: 2.28678 - acc: 0.8218 -- iter: 16/36\n",
            "Training Step: 1818  | total loss: \u001b[1m\u001b[32m2.28678\u001b[0m\u001b[0m | time: 0.016s\n",
            "| Adam | epoch: 364 | loss: 2.28678 - acc: 0.8396 -- iter: 24/36\n",
            "Training Step: 1819  | total loss: \u001b[1m\u001b[32m1.87788\u001b[0m\u001b[0m | time: 0.021s\n",
            "| Adam | epoch: 364 | loss: 1.87788 - acc: 0.8556 -- iter: 32/36\n",
            "Training Step: 1820  | total loss: \u001b[1m\u001b[32m1.87788\u001b[0m\u001b[0m | time: 0.025s\n",
            "| Adam | epoch: 364 | loss: 1.87788 - acc: 0.8701 -- iter: 36/36\n",
            "--\n",
            "Training Step: 1821  | total loss: \u001b[1m\u001b[32m1.69931\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 365 | loss: 1.69931 - acc: 0.8831 -- iter: 08/36\n",
            "Training Step: 1822  | total loss: \u001b[1m\u001b[32m2.32442\u001b[0m\u001b[0m | time: 0.011s\n",
            "| Adam | epoch: 365 | loss: 2.32442 - acc: 0.8073 -- iter: 16/36\n",
            "Training Step: 1823  | total loss: \u001b[1m\u001b[32m1.90940\u001b[0m\u001b[0m | time: 0.015s\n",
            "| Adam | epoch: 365 | loss: 1.90940 - acc: 0.8265 -- iter: 24/36\n",
            "Training Step: 1824  | total loss: \u001b[1m\u001b[32m1.90940\u001b[0m\u001b[0m | time: 0.020s\n",
            "| Adam | epoch: 365 | loss: 1.90940 - acc: 0.8439 -- iter: 32/36\n",
            "Training Step: 1825  | total loss: \u001b[1m\u001b[32m1.73149\u001b[0m\u001b[0m | time: 0.026s\n",
            "| Adam | epoch: 365 | loss: 1.73149 - acc: 0.8595 -- iter: 36/36\n",
            "--\n",
            "Training Step: 1826  | total loss: \u001b[1m\u001b[32m1.56557\u001b[0m\u001b[0m | time: 0.006s\n",
            "| Adam | epoch: 366 | loss: 1.56557 - acc: 0.8735 -- iter: 08/36\n",
            "Training Step: 1827  | total loss: \u001b[1m\u001b[32m1.42080\u001b[0m\u001b[0m | time: 0.013s\n",
            "| Adam | epoch: 366 | loss: 1.42080 - acc: 0.8862 -- iter: 16/36\n",
            "Training Step: 1828  | total loss: \u001b[1m\u001b[32m1.95243\u001b[0m\u001b[0m | time: 0.020s\n",
            "| Adam | epoch: 366 | loss: 1.95243 - acc: 0.8226 -- iter: 24/36\n",
            "Training Step: 1829  | total loss: \u001b[1m\u001b[32m1.95243\u001b[0m\u001b[0m | time: 0.027s\n",
            "| Adam | epoch: 366 | loss: 1.95243 - acc: 0.8403 -- iter: 32/36\n",
            "Training Step: 1830  | total loss: \u001b[1m\u001b[32m1.76894\u001b[0m\u001b[0m | time: 0.035s\n",
            "| Adam | epoch: 366 | loss: 1.76894 - acc: 0.8563 -- iter: 36/36\n",
            "--\n",
            "Training Step: 1831  | total loss: \u001b[1m\u001b[32m1.60378\u001b[0m\u001b[0m | time: 0.004s\n",
            "| Adam | epoch: 367 | loss: 1.60378 - acc: 0.8707 -- iter: 08/36\n",
            "Training Step: 1832  | total loss: \u001b[1m\u001b[32m1.45360\u001b[0m\u001b[0m | time: 0.011s\n",
            "| Adam | epoch: 367 | loss: 1.45360 - acc: 0.8836 -- iter: 16/36\n",
            "Training Step: 1833  | total loss: \u001b[1m\u001b[32m1.32257\u001b[0m\u001b[0m | time: 0.017s\n",
            "| Adam | epoch: 367 | loss: 1.32257 - acc: 0.8952 -- iter: 24/36\n",
            "Training Step: 1834  | total loss: \u001b[1m\u001b[32m2.22645\u001b[0m\u001b[0m | time: 0.024s\n",
            "| Adam | epoch: 367 | loss: 2.22645 - acc: 0.8307 -- iter: 32/36\n",
            "Training Step: 1835  | total loss: \u001b[1m\u001b[32m2.01486\u001b[0m\u001b[0m | time: 0.031s\n",
            "| Adam | epoch: 367 | loss: 2.01486 - acc: 0.8476 -- iter: 36/36\n",
            "--\n",
            "Training Step: 1836  | total loss: \u001b[1m\u001b[32m1.82365\u001b[0m\u001b[0m | time: 0.006s\n",
            "| Adam | epoch: 368 | loss: 1.82365 - acc: 0.8629 -- iter: 08/36\n",
            "Training Step: 1837  | total loss: \u001b[1m\u001b[32m1.65154\u001b[0m\u001b[0m | time: 0.010s\n",
            "| Adam | epoch: 368 | loss: 1.65154 - acc: 0.8766 -- iter: 16/36\n",
            "Training Step: 1838  | total loss: \u001b[1m\u001b[32m1.49912\u001b[0m\u001b[0m | time: 0.017s\n",
            "| Adam | epoch: 368 | loss: 1.49912 - acc: 0.8889 -- iter: 24/36\n",
            "Training Step: 1839  | total loss: \u001b[1m\u001b[32m1.36143\u001b[0m\u001b[0m | time: 0.023s\n",
            "| Adam | epoch: 368 | loss: 1.36143 - acc: 0.9000 -- iter: 32/36\n",
            "Training Step: 1840  | total loss: \u001b[1m\u001b[32m2.41824\u001b[0m\u001b[0m | time: 0.029s\n",
            "| Adam | epoch: 368 | loss: 2.41824 - acc: 0.8100 -- iter: 36/36\n",
            "--\n",
            "Training Step: 1841  | total loss: \u001b[1m\u001b[32m1.98655\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 369 | loss: 1.98655 - acc: 0.8290 -- iter: 08/36\n",
            "Training Step: 1842  | total loss: \u001b[1m\u001b[32m1.98655\u001b[0m\u001b[0m | time: 0.011s\n",
            "| Adam | epoch: 369 | loss: 1.98655 - acc: 0.8461 -- iter: 16/36\n",
            "Training Step: 1843  | total loss: \u001b[1m\u001b[32m1.63627\u001b[0m\u001b[0m | time: 0.016s\n",
            "| Adam | epoch: 369 | loss: 1.63627 - acc: 0.8615 -- iter: 24/36\n",
            "Training Step: 1844  | total loss: \u001b[1m\u001b[32m1.63627\u001b[0m\u001b[0m | time: 0.021s\n",
            "| Adam | epoch: 369 | loss: 1.63627 - acc: 0.8754 -- iter: 32/36\n",
            "Training Step: 1845  | total loss: \u001b[1m\u001b[32m2.60518\u001b[0m\u001b[0m | time: 0.026s\n",
            "| Adam | epoch: 369 | loss: 2.60518 - acc: 0.8878 -- iter: 36/36\n",
            "--\n",
            "Training Step: 1846  | total loss: \u001b[1m\u001b[32m2.35855\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 370 | loss: 2.35855 - acc: 0.7990 -- iter: 08/36\n",
            "Training Step: 1847  | total loss: \u001b[1m\u001b[32m2.13308\u001b[0m\u001b[0m | time: 0.008s\n",
            "| Adam | epoch: 370 | loss: 2.13308 - acc: 0.8191 -- iter: 16/36\n",
            "Training Step: 1848  | total loss: \u001b[1m\u001b[32m2.13308\u001b[0m\u001b[0m | time: 0.014s\n",
            "| Adam | epoch: 370 | loss: 2.13308 - acc: 0.8372 -- iter: 24/36\n",
            "Training Step: 1849  | total loss: \u001b[1m\u001b[32m1.74856\u001b[0m\u001b[0m | time: 0.020s\n",
            "| Adam | epoch: 370 | loss: 1.74856 - acc: 0.8535 -- iter: 32/36\n",
            "Training Step: 1850  | total loss: \u001b[1m\u001b[32m1.74856\u001b[0m\u001b[0m | time: 0.026s\n",
            "| Adam | epoch: 370 | loss: 1.74856 - acc: 0.8682 -- iter: 36/36\n",
            "--\n",
            "Training Step: 1851  | total loss: \u001b[1m\u001b[32m1.58575\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 371 | loss: 1.58575 - acc: 0.8813 -- iter: 08/36\n",
            "Training Step: 1852  | total loss: \u001b[1m\u001b[32m1.44117\u001b[0m\u001b[0m | time: 0.012s\n",
            "| Adam | epoch: 371 | loss: 1.44117 - acc: 0.8932 -- iter: 16/36\n",
            "Training Step: 1853  | total loss: \u001b[1m\u001b[32m1.31143\u001b[0m\u001b[0m | time: 0.018s\n",
            "| Adam | epoch: 371 | loss: 1.31143 - acc: 0.9039 -- iter: 24/36\n",
            "Training Step: 1854  | total loss: \u001b[1m\u001b[32m1.09895\u001b[0m\u001b[0m | time: 0.021s\n",
            "| Adam | epoch: 371 | loss: 1.09895 - acc: 0.9135 -- iter: 32/36\n",
            "Training Step: 1855  | total loss: \u001b[1m\u001b[32m1.00018\u001b[0m\u001b[0m | time: 0.025s\n",
            "| Adam | epoch: 371 | loss: 1.00018 - acc: 0.9221 -- iter: 36/36\n",
            "--\n",
            "Training Step: 1856  | total loss: \u001b[1m\u001b[32m0.91162\u001b[0m\u001b[0m | time: 0.004s\n",
            "| Adam | epoch: 372 | loss: 0.91162 - acc: 0.9299 -- iter: 08/36\n",
            "Training Step: 1857  | total loss: \u001b[1m\u001b[32m2.04271\u001b[0m\u001b[0m | time: 0.010s\n",
            "| Adam | epoch: 372 | loss: 2.04271 - acc: 0.9369 -- iter: 16/36\n",
            "Training Step: 1858  | total loss: \u001b[1m\u001b[32m1.84970\u001b[0m\u001b[0m | time: 0.017s\n",
            "| Adam | epoch: 372 | loss: 1.84970 - acc: 0.8682 -- iter: 24/36\n",
            "Training Step: 1859  | total loss: \u001b[1m\u001b[32m1.84970\u001b[0m\u001b[0m | time: 0.023s\n",
            "| Adam | epoch: 372 | loss: 1.84970 - acc: 0.8814 -- iter: 32/36\n",
            "Training Step: 1860  | total loss: \u001b[1m\u001b[32m1.52990\u001b[0m\u001b[0m | time: 0.029s\n",
            "| Adam | epoch: 372 | loss: 1.52990 - acc: 0.9039 -- iter: 36/36\n",
            "--\n",
            "Training Step: 1861  | total loss: \u001b[1m\u001b[32m1.52990\u001b[0m\u001b[0m | time: 0.004s\n",
            "| Adam | epoch: 373 | loss: 1.52990 - acc: 0.9039 -- iter: 08/36\n",
            "Training Step: 1862  | total loss: \u001b[1m\u001b[32m1.39293\u001b[0m\u001b[0m | time: 0.011s\n",
            "| Adam | epoch: 373 | loss: 1.39293 - acc: 0.9136 -- iter: 16/36\n",
            "Training Step: 1863  | total loss: \u001b[1m\u001b[32m1.26238\u001b[0m\u001b[0m | time: 0.015s\n",
            "| Adam | epoch: 373 | loss: 1.26238 - acc: 0.9222 -- iter: 24/36\n",
            "Training Step: 1864  | total loss: \u001b[1m\u001b[32m2.07214\u001b[0m\u001b[0m | time: 0.020s\n",
            "| Adam | epoch: 373 | loss: 2.07214 - acc: 0.8425 -- iter: 32/36\n",
            "Training Step: 1865  | total loss: \u001b[1m\u001b[32m1.87624\u001b[0m\u001b[0m | time: 0.026s\n",
            "| Adam | epoch: 373 | loss: 1.87624 - acc: 0.8582 -- iter: 36/36\n",
            "--\n",
            "Training Step: 1866  | total loss: \u001b[1m\u001b[32m1.70268\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 374 | loss: 1.70268 - acc: 0.8724 -- iter: 08/36\n",
            "Training Step: 1867  | total loss: \u001b[1m\u001b[32m1.54659\u001b[0m\u001b[0m | time: 0.011s\n",
            "| Adam | epoch: 374 | loss: 1.54659 - acc: 0.8852 -- iter: 16/36\n",
            "Training Step: 1868  | total loss: \u001b[1m\u001b[32m1.40322\u001b[0m\u001b[0m | time: 0.017s\n",
            "| Adam | epoch: 374 | loss: 1.40322 - acc: 0.8967 -- iter: 24/36\n",
            "Training Step: 1869  | total loss: \u001b[1m\u001b[32m1.27504\u001b[0m\u001b[0m | time: 0.024s\n",
            "| Adam | epoch: 374 | loss: 1.27504 - acc: 0.9070 -- iter: 32/36\n",
            "Training Step: 1870  | total loss: \u001b[1m\u001b[32m1.05877\u001b[0m\u001b[0m | time: 0.028s\n",
            "| Adam | epoch: 374 | loss: 1.05877 - acc: 0.9247 -- iter: 36/36\n",
            "--\n",
            "Training Step: 1871  | total loss: \u001b[1m\u001b[32m1.05877\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 375 | loss: 1.05877 - acc: 0.9247 -- iter: 08/36\n",
            "Training Step: 1872  | total loss: \u001b[1m\u001b[32m0.96784\u001b[0m\u001b[0m | time: 0.012s\n",
            "| Adam | epoch: 375 | loss: 0.96784 - acc: 0.9322 -- iter: 16/36\n",
            "Training Step: 1873  | total loss: \u001b[1m\u001b[32m0.88599\u001b[0m\u001b[0m | time: 0.017s\n",
            "| Adam | epoch: 375 | loss: 0.88599 - acc: 0.9390 -- iter: 24/36\n",
            "Training Step: 1874  | total loss: \u001b[1m\u001b[32m0.81303\u001b[0m\u001b[0m | time: 0.022s\n",
            "| Adam | epoch: 375 | loss: 0.81303 - acc: 0.9451 -- iter: 32/36\n",
            "Training Step: 1875  | total loss: \u001b[1m\u001b[32m1.50205\u001b[0m\u001b[0m | time: 0.026s\n",
            "| Adam | epoch: 375 | loss: 1.50205 - acc: 0.8805 -- iter: 36/36\n",
            "--\n",
            "Training Step: 1876  | total loss: \u001b[1m\u001b[32m1.50205\u001b[0m\u001b[0m | time: 0.004s\n",
            "| Adam | epoch: 376 | loss: 1.50205 - acc: 0.8805 -- iter: 08/36\n",
            "Training Step: 1877  | total loss: \u001b[1m\u001b[32m1.24424\u001b[0m\u001b[0m | time: 0.008s\n",
            "| Adam | epoch: 376 | loss: 1.24424 - acc: 0.8925 -- iter: 16/36\n",
            "Training Step: 1878  | total loss: \u001b[1m\u001b[32m1.24424\u001b[0m\u001b[0m | time: 0.014s\n",
            "| Adam | epoch: 376 | loss: 1.24424 - acc: 0.9032 -- iter: 24/36\n",
            "Training Step: 1879  | total loss: \u001b[1m\u001b[32m1.13230\u001b[0m\u001b[0m | time: 0.017s\n",
            "| Adam | epoch: 376 | loss: 1.13230 - acc: 0.9129 -- iter: 32/36\n",
            "Training Step: 1880  | total loss: \u001b[1m\u001b[32m0.94027\u001b[0m\u001b[0m | time: 0.019s\n",
            "| Adam | epoch: 376 | loss: 0.94027 - acc: 0.9216 -- iter: 36/36\n",
            "--\n",
            "Training Step: 1881  | total loss: \u001b[1m\u001b[32m1.68128\u001b[0m\u001b[0m | time: 0.002s\n",
            "| Adam | epoch: 377 | loss: 1.68128 - acc: 0.9294 -- iter: 08/36\n",
            "Training Step: 1882  | total loss: \u001b[1m\u001b[32m1.52079\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 377 | loss: 1.52079 - acc: 0.8490 -- iter: 16/36\n",
            "Training Step: 1883  | total loss: \u001b[1m\u001b[32m1.38224\u001b[0m\u001b[0m | time: 0.009s\n",
            "| Adam | epoch: 377 | loss: 1.38224 - acc: 0.8641 -- iter: 24/36\n",
            "Training Step: 1884  | total loss: \u001b[1m\u001b[32m1.25752\u001b[0m\u001b[0m | time: 0.014s\n",
            "| Adam | epoch: 377 | loss: 1.25752 - acc: 0.8777 -- iter: 32/36\n",
            "Training Step: 1885  | total loss: \u001b[1m\u001b[32m1.14460\u001b[0m\u001b[0m | time: 0.019s\n",
            "| Adam | epoch: 377 | loss: 1.14460 - acc: 0.8899 -- iter: 36/36\n",
            "--\n",
            "Training Step: 1886  | total loss: \u001b[1m\u001b[32m1.04615\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 378 | loss: 1.04615 - acc: 0.9009 -- iter: 08/36\n",
            "Training Step: 1887  | total loss: \u001b[1m\u001b[32m1.04615\u001b[0m\u001b[0m | time: 0.009s\n",
            "| Adam | epoch: 378 | loss: 1.04615 - acc: 0.9108 -- iter: 16/36\n",
            "Training Step: 1888  | total loss: \u001b[1m\u001b[32m1.89955\u001b[0m\u001b[0m | time: 0.015s\n",
            "| Adam | epoch: 378 | loss: 1.89955 - acc: 0.8198 -- iter: 24/36\n",
            "Training Step: 1889  | total loss: \u001b[1m\u001b[32m1.71859\u001b[0m\u001b[0m | time: 0.020s\n",
            "| Adam | epoch: 378 | loss: 1.71859 - acc: 0.8378 -- iter: 32/36\n",
            "Training Step: 1890  | total loss: \u001b[1m\u001b[32m1.55569\u001b[0m\u001b[0m | time: 0.025s\n",
            "| Adam | epoch: 378 | loss: 1.55569 - acc: 0.8540 -- iter: 36/36\n",
            "--\n",
            "Training Step: 1891  | total loss: \u001b[1m\u001b[32m1.41468\u001b[0m\u001b[0m | time: 0.004s\n",
            "| Adam | epoch: 379 | loss: 1.41468 - acc: 0.8686 -- iter: 08/36\n",
            "Training Step: 1892  | total loss: \u001b[1m\u001b[32m1.28859\u001b[0m\u001b[0m | time: 0.009s\n",
            "| Adam | epoch: 379 | loss: 1.28859 - acc: 0.8817 -- iter: 16/36\n",
            "Training Step: 1893  | total loss: \u001b[1m\u001b[32m1.92546\u001b[0m\u001b[0m | time: 0.012s\n",
            "| Adam | epoch: 379 | loss: 1.92546 - acc: 0.8936 -- iter: 24/36\n",
            "Training Step: 1894  | total loss: \u001b[1m\u001b[32m1.74664\u001b[0m\u001b[0m | time: 0.017s\n",
            "| Adam | epoch: 379 | loss: 1.74664 - acc: 0.8292 -- iter: 32/36\n",
            "Training Step: 1895  | total loss: \u001b[1m\u001b[32m1.58447\u001b[0m\u001b[0m | time: 0.022s\n",
            "| Adam | epoch: 379 | loss: 1.58447 - acc: 0.8463 -- iter: 36/36\n",
            "--\n",
            "Training Step: 1896  | total loss: \u001b[1m\u001b[32m1.43854\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 380 | loss: 1.43854 - acc: 0.8617 -- iter: 08/36\n",
            "Training Step: 1897  | total loss: \u001b[1m\u001b[32m1.30761\u001b[0m\u001b[0m | time: 0.010s\n",
            "| Adam | epoch: 380 | loss: 1.30761 - acc: 0.8755 -- iter: 16/36\n",
            "Training Step: 1898  | total loss: \u001b[1m\u001b[32m1.18975\u001b[0m\u001b[0m | time: 0.016s\n",
            "| Adam | epoch: 380 | loss: 1.18975 - acc: 0.8879 -- iter: 24/36\n",
            "Training Step: 1899  | total loss: \u001b[1m\u001b[32m1.65378\u001b[0m\u001b[0m | time: 0.020s\n",
            "| Adam | epoch: 380 | loss: 1.65378 - acc: 0.8991 -- iter: 32/36\n",
            "Training Step: 1900  | total loss: \u001b[1m\u001b[32m1.50615\u001b[0m\u001b[0m | time: 0.024s\n",
            "| Adam | epoch: 380 | loss: 1.50615 - acc: 0.8342 -- iter: 36/36\n",
            "--\n",
            "Training Step: 1901  | total loss: \u001b[1m\u001b[32m1.36209\u001b[0m\u001b[0m | time: 0.007s\n",
            "| Adam | epoch: 381 | loss: 1.36209 - acc: 0.8508 -- iter: 08/36\n",
            "Training Step: 1902  | total loss: \u001b[1m\u001b[32m1.23239\u001b[0m\u001b[0m | time: 0.011s\n",
            "| Adam | epoch: 381 | loss: 1.23239 - acc: 0.8657 -- iter: 16/36\n",
            "Training Step: 1903  | total loss: \u001b[1m\u001b[32m1.12561\u001b[0m\u001b[0m | time: 0.016s\n",
            "| Adam | epoch: 381 | loss: 1.12561 - acc: 0.8792 -- iter: 24/36\n",
            "Training Step: 1904  | total loss: \u001b[1m\u001b[32m1.02959\u001b[0m\u001b[0m | time: 0.022s\n",
            "| Adam | epoch: 381 | loss: 1.02959 - acc: 0.8912 -- iter: 32/36\n",
            "Training Step: 1905  | total loss: \u001b[1m\u001b[32m2.15335\u001b[0m\u001b[0m | time: 0.026s\n",
            "| Adam | epoch: 381 | loss: 2.15335 - acc: 0.9021 -- iter: 36/36\n",
            "--\n",
            "Training Step: 1906  | total loss: \u001b[1m\u001b[32m1.94785\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 382 | loss: 1.94785 - acc: 0.8119 -- iter: 08/36\n",
            "Training Step: 1907  | total loss: \u001b[1m\u001b[32m1.76648\u001b[0m\u001b[0m | time: 0.008s\n",
            "| Adam | epoch: 382 | loss: 1.76648 - acc: 0.8307 -- iter: 16/36\n",
            "Training Step: 1908  | total loss: \u001b[1m\u001b[32m1.60326\u001b[0m\u001b[0m | time: 0.013s\n",
            "| Adam | epoch: 382 | loss: 1.60326 - acc: 0.8629 -- iter: 24/36\n",
            "Training Step: 1909  | total loss: \u001b[1m\u001b[32m1.60326\u001b[0m\u001b[0m | time: 0.017s\n",
            "| Adam | epoch: 382 | loss: 1.60326 - acc: 0.8766 -- iter: 32/36\n",
            "Training Step: 1910  | total loss: \u001b[1m\u001b[32m1.46189\u001b[0m\u001b[0m | time: 0.023s\n",
            "| Adam | epoch: 382 | loss: 1.46189 - acc: 0.8889 -- iter: 36/36\n",
            "--\n",
            "Training Step: 1911  | total loss: \u001b[1m\u001b[32m1.33290\u001b[0m\u001b[0m | time: 0.004s\n",
            "| Adam | epoch: 383 | loss: 1.33290 - acc: 0.8000 -- iter: 08/36\n",
            "Training Step: 1912  | total loss: \u001b[1m\u001b[32m2.07048\u001b[0m\u001b[0m | time: 0.008s\n",
            "| Adam | epoch: 383 | loss: 2.07048 - acc: 0.8200 -- iter: 16/36\n",
            "Training Step: 1913  | total loss: \u001b[1m\u001b[32m1.87459\u001b[0m\u001b[0m | time: 0.012s\n",
            "| Adam | epoch: 383 | loss: 1.87459 - acc: 0.8380 -- iter: 24/36\n",
            "Training Step: 1914  | total loss: \u001b[1m\u001b[32m1.70030\u001b[0m\u001b[0m | time: 0.017s\n",
            "| Adam | epoch: 383 | loss: 1.70030 - acc: 0.8542 -- iter: 32/36\n",
            "Training Step: 1915  | total loss: \u001b[1m\u001b[32m1.54347\u001b[0m\u001b[0m | time: 0.021s\n",
            "| Adam | epoch: 383 | loss: 1.54347 - acc: 0.8688 -- iter: 36/36\n",
            "--\n",
            "Training Step: 1916  | total loss: \u001b[1m\u001b[32m1.40320\u001b[0m\u001b[0m | time: 0.006s\n",
            "| Adam | epoch: 384 | loss: 1.40320 - acc: 0.8819 -- iter: 08/36\n",
            "Training Step: 1917  | total loss: \u001b[1m\u001b[32m1.27679\u001b[0m\u001b[0m | time: 0.010s\n",
            "| Adam | epoch: 384 | loss: 1.27679 - acc: 0.8937 -- iter: 16/36\n",
            "Training Step: 1918  | total loss: \u001b[1m\u001b[32m1.16939\u001b[0m\u001b[0m | time: 0.013s\n",
            "| Adam | epoch: 384 | loss: 1.16939 - acc: 0.9044 -- iter: 24/36\n",
            "Training Step: 1919  | total loss: \u001b[1m\u001b[32m1.06848\u001b[0m\u001b[0m | time: 0.017s\n",
            "| Adam | epoch: 384 | loss: 1.06848 - acc: 0.9044 -- iter: 32/36\n",
            "Training Step: 1920  | total loss: \u001b[1m\u001b[32m0.97907\u001b[0m\u001b[0m | time: 0.020s\n",
            "| Adam | epoch: 384 | loss: 0.97907 - acc: 0.9139 -- iter: 36/36\n",
            "--\n",
            "Training Step: 1921  | total loss: \u001b[1m\u001b[32m0.89809\u001b[0m\u001b[0m | time: 0.003s\n",
            "| Adam | epoch: 385 | loss: 0.89809 - acc: 0.9225 -- iter: 08/36\n",
            "Training Step: 1922  | total loss: \u001b[1m\u001b[32m0.81884\u001b[0m\u001b[0m | time: 0.008s\n",
            "| Adam | epoch: 385 | loss: 0.81884 - acc: 0.9303 -- iter: 16/36\n",
            "Training Step: 1923  | total loss: \u001b[1m\u001b[32m0.75136\u001b[0m\u001b[0m | time: 0.011s\n",
            "| Adam | epoch: 385 | loss: 0.75136 - acc: 0.9373 -- iter: 24/36\n",
            "Training Step: 1924  | total loss: \u001b[1m\u001b[32m1.34704\u001b[0m\u001b[0m | time: 0.015s\n",
            "| Adam | epoch: 385 | loss: 1.34704 - acc: 0.8560 -- iter: 32/36\n",
            "Training Step: 1925  | total loss: \u001b[1m\u001b[32m1.22835\u001b[0m\u001b[0m | time: 0.022s\n",
            "| Adam | epoch: 385 | loss: 1.22835 - acc: 0.8704 -- iter: 36/36\n",
            "--\n",
            "Training Step: 1926  | total loss: \u001b[1m\u001b[32m1.12079\u001b[0m\u001b[0m | time: 0.004s\n",
            "| Adam | epoch: 386 | loss: 1.12079 - acc: 0.8834 -- iter: 08/36\n",
            "Training Step: 1927  | total loss: \u001b[1m\u001b[32m1.02392\u001b[0m\u001b[0m | time: 0.008s\n",
            "| Adam | epoch: 386 | loss: 1.02392 - acc: 0.8950 -- iter: 16/36\n",
            "Training Step: 1928  | total loss: \u001b[1m\u001b[32m0.93780\u001b[0m\u001b[0m | time: 0.011s\n",
            "| Adam | epoch: 386 | loss: 0.93780 - acc: 0.9055 -- iter: 24/36\n",
            "Training Step: 1929  | total loss: \u001b[1m\u001b[32m0.85375\u001b[0m\u001b[0m | time: 0.017s\n",
            "| Adam | epoch: 386 | loss: 0.85375 - acc: 0.9150 -- iter: 32/36\n",
            "Training Step: 1930  | total loss: \u001b[1m\u001b[32m1.17893\u001b[0m\u001b[0m | time: 0.022s\n",
            "| Adam | epoch: 386 | loss: 1.17893 - acc: 0.8485 -- iter: 36/36\n",
            "--\n",
            "Training Step: 1931  | total loss: \u001b[1m\u001b[32m1.07549\u001b[0m\u001b[0m | time: 0.007s\n",
            "| Adam | epoch: 387 | loss: 1.07549 - acc: 0.8636 -- iter: 08/36\n",
            "Training Step: 1932  | total loss: \u001b[1m\u001b[32m0.98442\u001b[0m\u001b[0m | time: 0.010s\n",
            "| Adam | epoch: 387 | loss: 0.98442 - acc: 0.8773 -- iter: 16/36\n",
            "Training Step: 1933  | total loss: \u001b[1m\u001b[32m0.90247\u001b[0m\u001b[0m | time: 0.014s\n",
            "| Adam | epoch: 387 | loss: 0.90247 - acc: 0.8895 -- iter: 24/36\n",
            "Training Step: 1934  | total loss: \u001b[1m\u001b[32m0.82621\u001b[0m\u001b[0m | time: 0.018s\n",
            "| Adam | epoch: 387 | loss: 0.82621 - acc: 0.9006 -- iter: 32/36\n",
            "Training Step: 1935  | total loss: \u001b[1m\u001b[32m0.75925\u001b[0m\u001b[0m | time: 0.021s\n",
            "| Adam | epoch: 387 | loss: 0.75925 - acc: 0.9105 -- iter: 36/36\n",
            "--\n",
            "Training Step: 1936  | total loss: \u001b[1m\u001b[32m1.65966\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 388 | loss: 1.65966 - acc: 0.8195 -- iter: 08/36\n",
            "Training Step: 1937  | total loss: \u001b[1m\u001b[32m1.50896\u001b[0m\u001b[0m | time: 0.009s\n",
            "| Adam | epoch: 388 | loss: 1.50896 - acc: 0.8375 -- iter: 16/36\n",
            "Training Step: 1938  | total loss: \u001b[1m\u001b[32m1.37588\u001b[0m\u001b[0m | time: 0.016s\n",
            "| Adam | epoch: 388 | loss: 1.37588 - acc: 0.8538 -- iter: 24/36\n",
            "Training Step: 1939  | total loss: \u001b[1m\u001b[32m1.25599\u001b[0m\u001b[0m | time: 0.019s\n",
            "| Adam | epoch: 388 | loss: 1.25599 - acc: 0.8684 -- iter: 32/36\n",
            "Training Step: 1940  | total loss: \u001b[1m\u001b[32m1.14603\u001b[0m\u001b[0m | time: 0.024s\n",
            "| Adam | epoch: 388 | loss: 1.14603 - acc: 0.8816 -- iter: 36/36\n",
            "--\n",
            "Training Step: 2098  | total loss: \u001b[1m\u001b[32m0.70249\u001b[0m\u001b[0m | time: 0.018s\n",
            "| Adam | epoch: 420 | loss: 0.70249 - acc: 0.9395 -- iter: 24/36\n",
            "Training Step: 2099  | total loss: \u001b[1m\u001b[32m0.65214\u001b[0m\u001b[0m | time: 0.025s\n",
            "| Adam | epoch: 420 | loss: 0.65214 - acc: 0.9455 -- iter: 32/36\n",
            "Training Step: 2100  | total loss: \u001b[1m\u001b[32m0.60300\u001b[0m\u001b[0m | time: 0.032s\n",
            "| Adam | epoch: 420 | loss: 0.60300 - acc: 0.9510 -- iter: 36/36\n",
            "--\n",
            "Training Step: 2101  | total loss: \u001b[1m\u001b[32m0.52658\u001b[0m\u001b[0m | time: 0.007s\n",
            "| Adam | epoch: 421 | loss: 0.52658 - acc: 0.9603 -- iter: 08/36\n",
            "Training Step: 2102  | total loss: \u001b[1m\u001b[32m0.49562\u001b[0m\u001b[0m | time: 0.014s\n",
            "| Adam | epoch: 421 | loss: 0.49562 - acc: 0.9603 -- iter: 16/36\n",
            "Training Step: 2103  | total loss: \u001b[1m\u001b[32m0.49562\u001b[0m\u001b[0m | time: 0.025s\n",
            "| Adam | epoch: 421 | loss: 0.49562 - acc: 0.9643 -- iter: 24/36\n",
            "Training Step: 2104  | total loss: \u001b[1m\u001b[32m0.43945\u001b[0m\u001b[0m | time: 0.029s\n",
            "| Adam | epoch: 421 | loss: 0.43945 - acc: 0.9678 -- iter: 32/36\n",
            "Training Step: 2105  | total loss: \u001b[1m\u001b[32m0.43945\u001b[0m\u001b[0m | time: 0.035s\n",
            "| Adam | epoch: 421 | loss: 0.43945 - acc: 0.9710 -- iter: 36/36\n",
            "--\n",
            "Training Step: 2106  | total loss: \u001b[1m\u001b[32m0.39020\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 422 | loss: 0.39020 - acc: 0.9766 -- iter: 08/36\n",
            "Training Step: 2107  | total loss: \u001b[1m\u001b[32m0.39020\u001b[0m\u001b[0m | time: 0.012s\n",
            "| Adam | epoch: 422 | loss: 0.39020 - acc: 0.9766 -- iter: 16/36\n",
            "Training Step: 2108  | total loss: \u001b[1m\u001b[32m0.35243\u001b[0m\u001b[0m | time: 0.018s\n",
            "| Adam | epoch: 422 | loss: 0.35243 - acc: 0.9810 -- iter: 24/36\n",
            "Training Step: 2109  | total loss: \u001b[1m\u001b[32m0.35243\u001b[0m\u001b[0m | time: 0.025s\n",
            "| Adam | epoch: 422 | loss: 0.35243 - acc: 0.9810 -- iter: 32/36\n",
            "Training Step: 2110  | total loss: \u001b[1m\u001b[32m0.32182\u001b[0m\u001b[0m | time: 0.031s\n",
            "| Adam | epoch: 422 | loss: 0.32182 - acc: 0.9829 -- iter: 36/36\n",
            "--\n",
            "Training Step: 2111  | total loss: \u001b[1m\u001b[32m0.31195\u001b[0m\u001b[0m | time: 0.004s\n",
            "| Adam | epoch: 423 | loss: 0.31195 - acc: 0.9846 -- iter: 08/36\n",
            "Training Step: 2112  | total loss: \u001b[1m\u001b[32m0.30292\u001b[0m\u001b[0m | time: 0.009s\n",
            "| Adam | epoch: 423 | loss: 0.30292 - acc: 0.9862 -- iter: 16/36\n",
            "Training Step: 2113  | total loss: \u001b[1m\u001b[32m0.29242\u001b[0m\u001b[0m | time: 0.013s\n",
            "| Adam | epoch: 423 | loss: 0.29242 - acc: 0.9875 -- iter: 24/36\n",
            "Training Step: 2114  | total loss: \u001b[1m\u001b[32m0.28154\u001b[0m\u001b[0m | time: 0.019s\n",
            "| Adam | epoch: 423 | loss: 0.28154 - acc: 0.9888 -- iter: 32/36\n",
            "Training Step: 2115  | total loss: \u001b[1m\u001b[32m0.27704\u001b[0m\u001b[0m | time: 0.024s\n",
            "| Adam | epoch: 423 | loss: 0.27704 - acc: 0.9899 -- iter: 36/36\n",
            "--\n",
            "Training Step: 2116  | total loss: \u001b[1m\u001b[32m0.27139\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 424 | loss: 0.27139 - acc: 0.9909 -- iter: 08/36\n",
            "Training Step: 2117  | total loss: \u001b[1m\u001b[32m0.26096\u001b[0m\u001b[0m | time: 0.010s\n",
            "| Adam | epoch: 424 | loss: 0.26096 - acc: 0.9918 -- iter: 16/36\n",
            "Training Step: 2118  | total loss: \u001b[1m\u001b[32m0.25152\u001b[0m\u001b[0m | time: 0.015s\n",
            "| Adam | epoch: 424 | loss: 0.25152 - acc: 0.9926 -- iter: 24/36\n",
            "Training Step: 2119  | total loss: \u001b[1m\u001b[32m0.24912\u001b[0m\u001b[0m | time: 0.020s\n",
            "| Adam | epoch: 424 | loss: 0.24912 - acc: 0.9934 -- iter: 32/36\n",
            "Training Step: 2120  | total loss: \u001b[1m\u001b[32m0.24226\u001b[0m\u001b[0m | time: 0.025s\n",
            "| Adam | epoch: 424 | loss: 0.24226 - acc: 0.9940 -- iter: 36/36\n",
            "--\n",
            "Training Step: 2121  | total loss: \u001b[1m\u001b[32m0.24226\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 425 | loss: 0.24226 - acc: 0.9946 -- iter: 08/36\n",
            "Training Step: 2122  | total loss: \u001b[1m\u001b[32m0.23488\u001b[0m\u001b[0m | time: 0.011s\n",
            "| Adam | epoch: 425 | loss: 0.23488 - acc: 0.9952 -- iter: 16/36\n",
            "Training Step: 2123  | total loss: \u001b[1m\u001b[32m0.22599\u001b[0m\u001b[0m | time: 0.015s\n",
            "| Adam | epoch: 425 | loss: 0.22599 - acc: 0.9957 -- iter: 24/36\n",
            "Training Step: 2124  | total loss: \u001b[1m\u001b[32m0.22246\u001b[0m\u001b[0m | time: 0.020s\n",
            "| Adam | epoch: 425 | loss: 0.22246 - acc: 0.9961 -- iter: 32/36\n",
            "Training Step: 2125  | total loss: \u001b[1m\u001b[32m0.21875\u001b[0m\u001b[0m | time: 0.024s\n",
            "| Adam | epoch: 425 | loss: 0.21875 - acc: 0.9965 -- iter: 36/36\n",
            "--\n",
            "Training Step: 2126  | total loss: \u001b[1m\u001b[32m0.22105\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 426 | loss: 0.22105 - acc: 0.9968 -- iter: 08/36\n",
            "Training Step: 2127  | total loss: \u001b[1m\u001b[32m0.21514\u001b[0m\u001b[0m | time: 0.009s\n",
            "| Adam | epoch: 426 | loss: 0.21514 - acc: 0.9971 -- iter: 16/36\n",
            "Training Step: 2128  | total loss: \u001b[1m\u001b[32m0.20912\u001b[0m\u001b[0m | time: 0.014s\n",
            "| Adam | epoch: 426 | loss: 0.20912 - acc: 0.9974 -- iter: 24/36\n",
            "Training Step: 2129  | total loss: \u001b[1m\u001b[32m0.20248\u001b[0m\u001b[0m | time: 0.018s\n",
            "| Adam | epoch: 426 | loss: 0.20248 - acc: 0.9977 -- iter: 32/36\n",
            "Training Step: 2130  | total loss: \u001b[1m\u001b[32m0.19644\u001b[0m\u001b[0m | time: 0.022s\n",
            "| Adam | epoch: 426 | loss: 0.19644 - acc: 0.9979 -- iter: 36/36\n",
            "--\n",
            "Training Step: 2131  | total loss: \u001b[1m\u001b[32m0.19822\u001b[0m\u001b[0m | time: 0.004s\n",
            "| Adam | epoch: 427 | loss: 0.19822 - acc: 0.9981 -- iter: 08/36\n",
            "Training Step: 2132  | total loss: \u001b[1m\u001b[32m0.19852\u001b[0m\u001b[0m | time: 0.009s\n",
            "| Adam | epoch: 427 | loss: 0.19852 - acc: 0.9983 -- iter: 16/36\n",
            "Training Step: 2133  | total loss: \u001b[1m\u001b[32m0.20003\u001b[0m\u001b[0m | time: 0.014s\n",
            "| Adam | epoch: 427 | loss: 0.20003 - acc: 0.9985 -- iter: 24/36\n",
            "Training Step: 2134  | total loss: \u001b[1m\u001b[32m0.19441\u001b[0m\u001b[0m | time: 0.018s\n",
            "| Adam | epoch: 427 | loss: 0.19441 - acc: 0.9986 -- iter: 32/36\n",
            "Training Step: 2135  | total loss: \u001b[1m\u001b[32m0.19892\u001b[0m\u001b[0m | time: 0.024s\n",
            "| Adam | epoch: 427 | loss: 0.19892 - acc: 0.9988 -- iter: 36/36\n",
            "--\n",
            "Training Step: 2136  | total loss: \u001b[1m\u001b[32m0.20284\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 428 | loss: 0.20284 - acc: 0.9989 -- iter: 08/36\n",
            "Training Step: 2137  | total loss: \u001b[1m\u001b[32m0.19879\u001b[0m\u001b[0m | time: 0.010s\n",
            "| Adam | epoch: 428 | loss: 0.19879 - acc: 0.9990 -- iter: 16/36\n",
            "Training Step: 2138  | total loss: \u001b[1m\u001b[32m0.19846\u001b[0m\u001b[0m | time: 0.013s\n",
            "| Adam | epoch: 428 | loss: 0.19846 - acc: 0.9991 -- iter: 24/36\n",
            "Training Step: 2139  | total loss: \u001b[1m\u001b[32m0.20090\u001b[0m\u001b[0m | time: 0.019s\n",
            "| Adam | epoch: 428 | loss: 0.20090 - acc: 0.9992 -- iter: 32/36\n",
            "Training Step: 2140  | total loss: \u001b[1m\u001b[32m0.19555\u001b[0m\u001b[0m | time: 0.022s\n",
            "| Adam | epoch: 428 | loss: 0.19555 - acc: 0.9993 -- iter: 36/36\n",
            "--\n",
            "Training Step: 2141  | total loss: \u001b[1m\u001b[32m0.18723\u001b[0m\u001b[0m | time: 0.004s\n",
            "| Adam | epoch: 429 | loss: 0.18723 - acc: 0.9993 -- iter: 08/36\n",
            "Training Step: 2142  | total loss: \u001b[1m\u001b[32m0.17968\u001b[0m\u001b[0m | time: 0.008s\n",
            "| Adam | epoch: 429 | loss: 0.17968 - acc: 0.9994 -- iter: 16/36\n",
            "Training Step: 2143  | total loss: \u001b[1m\u001b[32m0.18131\u001b[0m\u001b[0m | time: 0.012s\n",
            "| Adam | epoch: 429 | loss: 0.18131 - acc: 0.9995 -- iter: 24/36\n",
            "Training Step: 2144  | total loss: \u001b[1m\u001b[32m0.18461\u001b[0m\u001b[0m | time: 0.017s\n",
            "| Adam | epoch: 429 | loss: 0.18461 - acc: 0.9995 -- iter: 32/36\n",
            "Training Step: 2145  | total loss: \u001b[1m\u001b[32m0.76897\u001b[0m\u001b[0m | time: 0.023s\n",
            "| Adam | epoch: 429 | loss: 0.76897 - acc: 0.9996 -- iter: 36/36\n",
            "--\n",
            "Training Step: 2146  | total loss: \u001b[1m\u001b[32m0.71173\u001b[0m\u001b[0m | time: 0.004s\n",
            "| Adam | epoch: 430 | loss: 0.71173 - acc: 0.8996 -- iter: 08/36\n",
            "Training Step: 2147  | total loss: \u001b[1m\u001b[32m0.71173\u001b[0m\u001b[0m | time: 0.010s\n",
            "| Adam | epoch: 430 | loss: 0.71173 - acc: 0.9097 -- iter: 16/36\n",
            "Training Step: 2148  | total loss: \u001b[1m\u001b[32m0.66124\u001b[0m\u001b[0m | time: 0.016s\n",
            "| Adam | epoch: 430 | loss: 0.66124 - acc: 0.9187 -- iter: 24/36\n",
            "Training Step: 2149  | total loss: \u001b[1m\u001b[32m0.57380\u001b[0m\u001b[0m | time: 0.022s\n",
            "| Adam | epoch: 430 | loss: 0.57380 - acc: 0.9268 -- iter: 32/36\n",
            "Training Step: 2150  | total loss: \u001b[1m\u001b[32m0.57380\u001b[0m\u001b[0m | time: 0.028s\n",
            "| Adam | epoch: 430 | loss: 0.57380 - acc: 0.9341 -- iter: 36/36\n",
            "--\n",
            "Training Step: 2151  | total loss: \u001b[1m\u001b[32m0.53133\u001b[0m\u001b[0m | time: 0.008s\n",
            "| Adam | epoch: 431 | loss: 0.53133 - acc: 0.9407 -- iter: 08/36\n",
            "Training Step: 2152  | total loss: \u001b[1m\u001b[32m1.26907\u001b[0m\u001b[0m | time: 0.013s\n",
            "| Adam | epoch: 431 | loss: 1.26907 - acc: 0.8592 -- iter: 16/36\n",
            "Training Step: 2153  | total loss: \u001b[1m\u001b[32m1.07135\u001b[0m\u001b[0m | time: 0.018s\n",
            "| Adam | epoch: 431 | loss: 1.07135 - acc: 0.8732 -- iter: 24/36\n",
            "Training Step: 2154  | total loss: \u001b[1m\u001b[32m1.07135\u001b[0m\u001b[0m | time: 0.028s\n",
            "| Adam | epoch: 431 | loss: 1.07135 - acc: 0.8859 -- iter: 32/36\n",
            "Training Step: 2155  | total loss: \u001b[1m\u001b[32m0.98550\u001b[0m\u001b[0m | time: 0.034s\n",
            "| Adam | epoch: 431 | loss: 0.98550 - acc: 0.8973 -- iter: 36/36\n",
            "--\n",
            "Training Step: 2156  | total loss: \u001b[1m\u001b[32m0.90675\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 432 | loss: 0.90675 - acc: 0.9076 -- iter: 08/36\n",
            "Training Step: 2157  | total loss: \u001b[1m\u001b[32m0.82815\u001b[0m\u001b[0m | time: 0.009s\n",
            "| Adam | epoch: 432 | loss: 0.82815 - acc: 0.9168 -- iter: 16/36\n",
            "Training Step: 2158  | total loss: \u001b[1m\u001b[32m0.76210\u001b[0m\u001b[0m | time: 0.016s\n",
            "| Adam | epoch: 432 | loss: 0.76210 - acc: 0.9251 -- iter: 24/36\n",
            "Training Step: 2159  | total loss: \u001b[1m\u001b[32m0.70040\u001b[0m\u001b[0m | time: 0.021s\n",
            "| Adam | epoch: 432 | loss: 0.70040 - acc: 0.9326 -- iter: 32/36\n",
            "Training Step: 2160  | total loss: \u001b[1m\u001b[32m0.61278\u001b[0m\u001b[0m | time: 0.026s\n",
            "| Adam | epoch: 432 | loss: 0.61278 - acc: 0.9454 -- iter: 36/36\n",
            "--\n",
            "Training Step: 2161  | total loss: \u001b[1m\u001b[32m0.61278\u001b[0m\u001b[0m | time: 0.008s\n",
            "| Adam | epoch: 433 | loss: 0.61278 - acc: 0.9454 -- iter: 08/36\n",
            "Training Step: 2162  | total loss: \u001b[1m\u001b[32m0.57325\u001b[0m\u001b[0m | time: 0.013s\n",
            "| Adam | epoch: 433 | loss: 0.57325 - acc: 0.9509 -- iter: 16/36\n",
            "Training Step: 2163  | total loss: \u001b[1m\u001b[32m0.49909\u001b[0m\u001b[0m | time: 0.018s\n",
            "| Adam | epoch: 433 | loss: 0.49909 - acc: 0.9558 -- iter: 24/36\n",
            "Training Step: 2164  | total loss: \u001b[1m\u001b[32m0.49909\u001b[0m\u001b[0m | time: 0.024s\n",
            "| Adam | epoch: 433 | loss: 0.49909 - acc: 0.9602 -- iter: 32/36\n",
            "Training Step: 2165  | total loss: \u001b[1m\u001b[32m0.46120\u001b[0m\u001b[0m | time: 0.032s\n",
            "| Adam | epoch: 433 | loss: 0.46120 - acc: 0.9642 -- iter: 36/36\n",
            "--\n",
            "Training Step: 2166  | total loss: \u001b[1m\u001b[32m0.40738\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 434 | loss: 0.40738 - acc: 0.9678 -- iter: 08/36\n",
            "Training Step: 2167  | total loss: \u001b[1m\u001b[32m0.40738\u001b[0m\u001b[0m | time: 0.012s\n",
            "| Adam | epoch: 434 | loss: 0.40738 - acc: 0.9710 -- iter: 16/36\n",
            "Training Step: 2168  | total loss: \u001b[1m\u001b[32m0.38019\u001b[0m\u001b[0m | time: 0.018s\n",
            "| Adam | epoch: 434 | loss: 0.38019 - acc: 0.9739 -- iter: 24/36\n",
            "Training Step: 2169  | total loss: \u001b[1m\u001b[32m0.35493\u001b[0m\u001b[0m | time: 0.023s\n",
            "| Adam | epoch: 434 | loss: 0.35493 - acc: 0.9765 -- iter: 32/36\n",
            "Training Step: 2170  | total loss: \u001b[1m\u001b[32m0.35493\u001b[0m\u001b[0m | time: 0.032s\n",
            "| Adam | epoch: 434 | loss: 0.35493 - acc: 0.9789 -- iter: 36/36\n",
            "--\n",
            "Training Step: 2171  | total loss: \u001b[1m\u001b[32m0.33570\u001b[0m\u001b[0m | time: 0.007s\n",
            "| Adam | epoch: 435 | loss: 0.33570 - acc: 0.9810 -- iter: 08/36\n",
            "Training Step: 2172  | total loss: \u001b[1m\u001b[32m0.31834\u001b[0m\u001b[0m | time: 0.013s\n",
            "| Adam | epoch: 435 | loss: 0.31834 - acc: 0.9829 -- iter: 16/36\n",
            "Training Step: 2173  | total loss: \u001b[1m\u001b[32m0.31834\u001b[0m\u001b[0m | time: 0.021s\n",
            "| Adam | epoch: 435 | loss: 0.31834 - acc: 0.9846 -- iter: 24/36\n",
            "Training Step: 2174  | total loss: \u001b[1m\u001b[32m0.29976\u001b[0m\u001b[0m | time: 0.028s\n",
            "| Adam | epoch: 435 | loss: 0.29976 - acc: 0.9861 -- iter: 32/36\n",
            "Training Step: 2175  | total loss: \u001b[1m\u001b[32m0.28221\u001b[0m\u001b[0m | time: 0.034s\n",
            "| Adam | epoch: 435 | loss: 0.28221 - acc: 0.9875 -- iter: 36/36\n",
            "--\n",
            "Training Step: 2176  | total loss: \u001b[1m\u001b[32m0.28221\u001b[0m\u001b[0m | time: 0.011s\n",
            "| Adam | epoch: 436 | loss: 0.28221 - acc: 0.9888 -- iter: 08/36\n",
            "Training Step: 2177  | total loss: \u001b[1m\u001b[32m0.25709\u001b[0m\u001b[0m | time: 0.016s\n",
            "| Adam | epoch: 436 | loss: 0.25709 - acc: 0.9899 -- iter: 16/36\n",
            "Training Step: 2178  | total loss: \u001b[1m\u001b[32m0.23995\u001b[0m\u001b[0m | time: 0.021s\n",
            "| Adam | epoch: 436 | loss: 0.23995 - acc: 0.9909 -- iter: 24/36\n",
            "Training Step: 2179  | total loss: \u001b[1m\u001b[32m0.23634\u001b[0m\u001b[0m | time: 0.028s\n",
            "| Adam | epoch: 436 | loss: 0.23634 - acc: 0.9918 -- iter: 32/36\n",
            "Training Step: 2180  | total loss: \u001b[1m\u001b[32m0.23141\u001b[0m\u001b[0m | time: 0.033s\n",
            "| Adam | epoch: 436 | loss: 0.23141 - acc: 0.9926 -- iter: 36/36\n",
            "--\n",
            "Training Step: 2181  | total loss: \u001b[1m\u001b[32m0.22484\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 437 | loss: 0.22484 - acc: 0.9934 -- iter: 08/36\n",
            "Training Step: 2182  | total loss: \u001b[1m\u001b[32m0.22711\u001b[0m\u001b[0m | time: 0.010s\n",
            "| Adam | epoch: 437 | loss: 0.22711 - acc: 0.9940 -- iter: 16/36\n",
            "Training Step: 2183  | total loss: \u001b[1m\u001b[32m0.22711\u001b[0m\u001b[0m | time: 0.017s\n",
            "| Adam | epoch: 437 | loss: 0.22711 - acc: 0.9946 -- iter: 24/36\n",
            "Training Step: 2184  | total loss: \u001b[1m\u001b[32m0.22050\u001b[0m\u001b[0m | time: 0.025s\n",
            "| Adam | epoch: 437 | loss: 0.22050 - acc: 0.9952 -- iter: 32/36\n",
            "Training Step: 2185  | total loss: \u001b[1m\u001b[32m0.20619\u001b[0m\u001b[0m | time: 0.033s\n",
            "| Adam | epoch: 437 | loss: 0.20619 - acc: 0.9956 -- iter: 36/36\n",
            "--\n",
            "Training Step: 2186  | total loss: \u001b[1m\u001b[32m0.20619\u001b[0m\u001b[0m | time: 0.010s\n",
            "| Adam | epoch: 438 | loss: 0.20619 - acc: 0.9961 -- iter: 08/36\n",
            "Training Step: 2187  | total loss: \u001b[1m\u001b[32m0.20054\u001b[0m\u001b[0m | time: 0.019s\n",
            "| Adam | epoch: 438 | loss: 0.20054 - acc: 0.9965 -- iter: 16/36\n",
            "Training Step: 2188  | total loss: \u001b[1m\u001b[32m0.20054\u001b[0m\u001b[0m | time: 0.026s\n",
            "| Adam | epoch: 438 | loss: 0.20054 - acc: 0.9968 -- iter: 24/36\n",
            "Training Step: 2189  | total loss: \u001b[1m\u001b[32m0.19561\u001b[0m\u001b[0m | time: 0.030s\n",
            "| Adam | epoch: 438 | loss: 0.19561 - acc: 0.9971 -- iter: 32/36\n",
            "Training Step: 2190  | total loss: \u001b[1m\u001b[32m0.19561\u001b[0m\u001b[0m | time: 0.038s\n",
            "| Adam | epoch: 438 | loss: 0.19561 - acc: 0.9974 -- iter: 36/36\n",
            "--\n",
            "Training Step: 2191  | total loss: \u001b[1m\u001b[32m0.19868\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 439 | loss: 0.19868 - acc: 0.9977 -- iter: 08/36\n",
            "Training Step: 2192  | total loss: \u001b[1m\u001b[32m0.19868\u001b[0m\u001b[0m | time: 0.015s\n",
            "| Adam | epoch: 439 | loss: 0.19868 - acc: 0.9979 -- iter: 16/36\n",
            "Training Step: 2193  | total loss: \u001b[1m\u001b[32m0.19350\u001b[0m\u001b[0m | time: 0.021s\n",
            "| Adam | epoch: 439 | loss: 0.19350 - acc: 0.9981 -- iter: 24/36\n",
            "Training Step: 2194  | total loss: \u001b[1m\u001b[32m0.19350\u001b[0m\u001b[0m | time: 0.030s\n",
            "| Adam | epoch: 439 | loss: 0.19350 - acc: 0.9983 -- iter: 32/36\n",
            "Training Step: 2195  | total loss: \u001b[1m\u001b[32m0.18885\u001b[0m\u001b[0m | time: 0.036s\n",
            "| Adam | epoch: 439 | loss: 0.18885 - acc: 0.9985 -- iter: 36/36\n",
            "--\n",
            "Training Step: 2196  | total loss: \u001b[1m\u001b[32m0.18885\u001b[0m\u001b[0m | time: 0.008s\n",
            "| Adam | epoch: 440 | loss: 0.18885 - acc: 0.9986 -- iter: 08/36\n",
            "Training Step: 2197  | total loss: \u001b[1m\u001b[32m0.18713\u001b[0m\u001b[0m | time: 0.014s\n",
            "| Adam | epoch: 440 | loss: 0.18713 - acc: 0.9988 -- iter: 16/36\n",
            "Training Step: 2198  | total loss: \u001b[1m\u001b[32m0.18713\u001b[0m\u001b[0m | time: 0.021s\n",
            "| Adam | epoch: 440 | loss: 0.18713 - acc: 0.9989 -- iter: 24/36\n",
            "Training Step: 2199  | total loss: \u001b[1m\u001b[32m0.17900\u001b[0m\u001b[0m | time: 0.027s\n",
            "| Adam | epoch: 440 | loss: 0.17900 - acc: 0.9990 -- iter: 32/36\n",
            "Training Step: 2200  | total loss: \u001b[1m\u001b[32m0.17900\u001b[0m\u001b[0m | time: 0.033s\n",
            "| Adam | epoch: 440 | loss: 0.17900 - acc: 0.9991 -- iter: 36/36\n",
            "--\n",
            "Training Step: 2201  | total loss: \u001b[1m\u001b[32m0.18438\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 441 | loss: 0.18438 - acc: 0.9992 -- iter: 08/36\n",
            "Training Step: 2202  | total loss: \u001b[1m\u001b[32m0.18743\u001b[0m\u001b[0m | time: 0.010s\n",
            "| Adam | epoch: 441 | loss: 0.18743 - acc: 0.9993 -- iter: 16/36\n",
            "Training Step: 2203  | total loss: \u001b[1m\u001b[32m0.18350\u001b[0m\u001b[0m | time: 0.015s\n",
            "| Adam | epoch: 441 | loss: 0.18350 - acc: 0.9993 -- iter: 24/36\n",
            "Training Step: 2204  | total loss: \u001b[1m\u001b[32m0.18427\u001b[0m\u001b[0m | time: 0.021s\n",
            "| Adam | epoch: 441 | loss: 0.18427 - acc: 0.9994 -- iter: 32/36\n",
            "Training Step: 2205  | total loss: \u001b[1m\u001b[32m0.17902\u001b[0m\u001b[0m | time: 0.028s\n",
            "| Adam | epoch: 441 | loss: 0.17902 - acc: 0.9995 -- iter: 36/36\n",
            "--\n",
            "Training Step: 2206  | total loss: \u001b[1m\u001b[32m0.17232\u001b[0m\u001b[0m | time: 0.004s\n",
            "| Adam | epoch: 442 | loss: 0.17232 - acc: 0.9995 -- iter: 08/36\n",
            "Training Step: 2207  | total loss: \u001b[1m\u001b[32m0.17232\u001b[0m\u001b[0m | time: 0.013s\n",
            "| Adam | epoch: 442 | loss: 0.17232 - acc: 0.9996 -- iter: 16/36\n",
            "Training Step: 2208  | total loss: \u001b[1m\u001b[32m0.16988\u001b[0m\u001b[0m | time: 0.020s\n",
            "| Adam | epoch: 442 | loss: 0.16988 - acc: 0.9996 -- iter: 24/36\n",
            "Training Step: 2209  | total loss: \u001b[1m\u001b[32m0.16988\u001b[0m\u001b[0m | time: 0.027s\n",
            "| Adam | epoch: 442 | loss: 0.16988 - acc: 0.9997 -- iter: 32/36\n",
            "Training Step: 2210  | total loss: \u001b[1m\u001b[32m0.17036\u001b[0m\u001b[0m | time: 0.033s\n",
            "| Adam | epoch: 442 | loss: 0.17036 - acc: 0.9997 -- iter: 36/36\n",
            "--\n",
            "Training Step: 2211  | total loss: \u001b[1m\u001b[32m0.17036\u001b[0m\u001b[0m | time: 0.004s\n",
            "| Adam | epoch: 443 | loss: 0.17036 - acc: 0.9997 -- iter: 08/36\n",
            "Training Step: 2212  | total loss: \u001b[1m\u001b[32m0.17590\u001b[0m\u001b[0m | time: 0.010s\n",
            "| Adam | epoch: 443 | loss: 0.17590 - acc: 0.9997 -- iter: 16/36\n",
            "Training Step: 2213  | total loss: \u001b[1m\u001b[32m0.17301\u001b[0m\u001b[0m | time: 0.016s\n",
            "| Adam | epoch: 443 | loss: 0.17301 - acc: 0.9998 -- iter: 24/36\n",
            "Training Step: 2214  | total loss: \u001b[1m\u001b[32m0.16998\u001b[0m\u001b[0m | time: 0.024s\n",
            "| Adam | epoch: 443 | loss: 0.16998 - acc: 0.9998 -- iter: 32/36\n",
            "Training Step: 2215  | total loss: \u001b[1m\u001b[32m0.16718\u001b[0m\u001b[0m | time: 0.029s\n",
            "| Adam | epoch: 443 | loss: 0.16718 - acc: 0.9998 -- iter: 36/36\n",
            "--\n",
            "Training Step: 2216  | total loss: \u001b[1m\u001b[32m0.17041\u001b[0m\u001b[0m | time: 0.007s\n",
            "| Adam | epoch: 444 | loss: 0.17041 - acc: 0.9998 -- iter: 08/36\n",
            "Training Step: 2217  | total loss: \u001b[1m\u001b[32m0.17605\u001b[0m\u001b[0m | time: 0.012s\n",
            "| Adam | epoch: 444 | loss: 0.17605 - acc: 0.9999 -- iter: 16/36\n",
            "Training Step: 2218  | total loss: \u001b[1m\u001b[32m0.16944\u001b[0m\u001b[0m | time: 0.018s\n",
            "| Adam | epoch: 444 | loss: 0.16944 - acc: 0.9999 -- iter: 24/36\n",
            "Training Step: 2219  | total loss: \u001b[1m\u001b[32m0.17148\u001b[0m\u001b[0m | time: 0.026s\n",
            "| Adam | epoch: 444 | loss: 0.17148 - acc: 0.9999 -- iter: 32/36\n",
            "Training Step: 2220  | total loss: \u001b[1m\u001b[32m0.17304\u001b[0m\u001b[0m | time: 0.032s\n",
            "| Adam | epoch: 444 | loss: 0.17304 - acc: 0.9999 -- iter: 36/36\n",
            "--\n",
            "Training Step: 2221  | total loss: \u001b[1m\u001b[32m0.17437\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 445 | loss: 0.17437 - acc: 0.9999 -- iter: 08/36\n",
            "Training Step: 2222  | total loss: \u001b[1m\u001b[32m0.16823\u001b[0m\u001b[0m | time: 0.013s\n",
            "| Adam | epoch: 445 | loss: 0.16823 - acc: 0.9999 -- iter: 16/36\n",
            "Training Step: 2223  | total loss: \u001b[1m\u001b[32m0.16506\u001b[0m\u001b[0m | time: 0.019s\n",
            "| Adam | epoch: 445 | loss: 0.16506 - acc: 0.9999 -- iter: 24/36\n",
            "Training Step: 2224  | total loss: \u001b[1m\u001b[32m0.16966\u001b[0m\u001b[0m | time: 0.030s\n",
            "| Adam | epoch: 445 | loss: 0.16966 - acc: 0.9999 -- iter: 32/36\n",
            "Training Step: 2225  | total loss: \u001b[1m\u001b[32m0.16936\u001b[0m\u001b[0m | time: 0.036s\n",
            "| Adam | epoch: 445 | loss: 0.16936 - acc: 0.9999 -- iter: 36/36\n",
            "--\n",
            "Training Step: 2226  | total loss: \u001b[1m\u001b[32m0.17032\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 446 | loss: 0.17032 - acc: 0.9999 -- iter: 08/36\n",
            "Training Step: 2227  | total loss: \u001b[1m\u001b[32m0.17111\u001b[0m\u001b[0m | time: 0.010s\n",
            "| Adam | epoch: 446 | loss: 0.17111 - acc: 0.9999 -- iter: 16/36\n",
            "Training Step: 2228  | total loss: \u001b[1m\u001b[32m0.17330\u001b[0m\u001b[0m | time: 0.015s\n",
            "| Adam | epoch: 446 | loss: 0.17330 - acc: 1.0000 -- iter: 24/36\n",
            "Training Step: 2229  | total loss: \u001b[1m\u001b[32m0.17213\u001b[0m\u001b[0m | time: 0.024s\n",
            "| Adam | epoch: 446 | loss: 0.17213 - acc: 1.0000 -- iter: 32/36\n",
            "Training Step: 2230  | total loss: \u001b[1m\u001b[32m0.16721\u001b[0m\u001b[0m | time: 0.030s\n",
            "| Adam | epoch: 446 | loss: 0.16721 - acc: 1.0000 -- iter: 36/36\n",
            "--\n",
            "Training Step: 2231  | total loss: \u001b[1m\u001b[32m0.16791\u001b[0m\u001b[0m | time: 0.007s\n",
            "| Adam | epoch: 447 | loss: 0.16791 - acc: 1.0000 -- iter: 08/36\n",
            "Training Step: 2232  | total loss: \u001b[1m\u001b[32m0.15817\u001b[0m\u001b[0m | time: 0.011s\n",
            "| Adam | epoch: 447 | loss: 0.15817 - acc: 1.0000 -- iter: 16/36\n",
            "Training Step: 2233  | total loss: \u001b[1m\u001b[32m0.15817\u001b[0m\u001b[0m | time: 0.021s\n",
            "| Adam | epoch: 447 | loss: 0.15817 - acc: 1.0000 -- iter: 24/36\n",
            "Training Step: 2234  | total loss: \u001b[1m\u001b[32m0.15896\u001b[0m\u001b[0m | time: 0.025s\n",
            "| Adam | epoch: 447 | loss: 0.15896 - acc: 1.0000 -- iter: 32/36\n",
            "Training Step: 2235  | total loss: \u001b[1m\u001b[32m0.15471\u001b[0m\u001b[0m | time: 0.033s\n",
            "| Adam | epoch: 447 | loss: 0.15471 - acc: 1.0000 -- iter: 36/36\n",
            "--\n",
            "Training Step: 2236  | total loss: \u001b[1m\u001b[32m0.15997\u001b[0m\u001b[0m | time: 0.007s\n",
            "| Adam | epoch: 448 | loss: 0.15997 - acc: 1.0000 -- iter: 08/36\n",
            "Training Step: 2237  | total loss: \u001b[1m\u001b[32m0.16069\u001b[0m\u001b[0m | time: 0.013s\n",
            "| Adam | epoch: 448 | loss: 0.16069 - acc: 1.0000 -- iter: 16/36\n",
            "Training Step: 2238  | total loss: \u001b[1m\u001b[32m0.16069\u001b[0m\u001b[0m | time: 0.022s\n",
            "| Adam | epoch: 448 | loss: 0.16069 - acc: 1.0000 -- iter: 24/36\n",
            "Training Step: 2239  | total loss: \u001b[1m\u001b[32m0.16065\u001b[0m\u001b[0m | time: 0.028s\n",
            "| Adam | epoch: 448 | loss: 0.16065 - acc: 1.0000 -- iter: 32/36\n",
            "Training Step: 2240  | total loss: \u001b[1m\u001b[32m0.16065\u001b[0m\u001b[0m | time: 0.036s\n",
            "| Adam | epoch: 448 | loss: 0.16065 - acc: 1.0000 -- iter: 36/36\n",
            "--\n",
            "Training Step: 2241  | total loss: \u001b[1m\u001b[32m0.15966\u001b[0m\u001b[0m | time: 0.007s\n",
            "| Adam | epoch: 449 | loss: 0.15966 - acc: 1.0000 -- iter: 08/36\n",
            "Training Step: 2242  | total loss: \u001b[1m\u001b[32m0.15751\u001b[0m\u001b[0m | time: 0.011s\n",
            "| Adam | epoch: 449 | loss: 0.15751 - acc: 1.0000 -- iter: 16/36\n",
            "Training Step: 2243  | total loss: \u001b[1m\u001b[32m0.15751\u001b[0m\u001b[0m | time: 0.018s\n",
            "| Adam | epoch: 449 | loss: 0.15751 - acc: 1.0000 -- iter: 24/36\n",
            "Training Step: 2244  | total loss: \u001b[1m\u001b[32m0.15469\u001b[0m\u001b[0m | time: 0.027s\n",
            "| Adam | epoch: 449 | loss: 0.15469 - acc: 1.0000 -- iter: 32/36\n",
            "Training Step: 2245  | total loss: \u001b[1m\u001b[32m0.15210\u001b[0m\u001b[0m | time: 0.034s\n",
            "| Adam | epoch: 449 | loss: 0.15210 - acc: 1.0000 -- iter: 36/36\n",
            "--\n",
            "Training Step: 2246  | total loss: \u001b[1m\u001b[32m0.15844\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 450 | loss: 0.15844 - acc: 1.0000 -- iter: 08/36\n",
            "Training Step: 2247  | total loss: \u001b[1m\u001b[32m0.15844\u001b[0m\u001b[0m | time: 0.011s\n",
            "| Adam | epoch: 450 | loss: 0.15844 - acc: 1.0000 -- iter: 16/36\n",
            "Training Step: 2248  | total loss: \u001b[1m\u001b[32m0.98379\u001b[0m\u001b[0m | time: 0.014s\n",
            "| Adam | epoch: 450 | loss: 0.98379 - acc: 0.9125 -- iter: 24/36\n",
            "Training Step: 2249  | total loss: \u001b[1m\u001b[32m0.98379\u001b[0m\u001b[0m | time: 0.020s\n",
            "| Adam | epoch: 450 | loss: 0.98379 - acc: 0.9212 -- iter: 32/36\n",
            "Training Step: 2250  | total loss: \u001b[1m\u001b[32m0.82323\u001b[0m\u001b[0m | time: 0.025s\n",
            "| Adam | epoch: 450 | loss: 0.82323 - acc: 0.9291 -- iter: 36/36\n",
            "--\n",
            "Training Step: 2251  | total loss: \u001b[1m\u001b[32m0.82323\u001b[0m\u001b[0m | time: 0.007s\n",
            "| Adam | epoch: 451 | loss: 0.82323 - acc: 0.9362 -- iter: 08/36\n",
            "Training Step: 2252  | total loss: \u001b[1m\u001b[32m0.75812\u001b[0m\u001b[0m | time: 0.012s\n",
            "| Adam | epoch: 451 | loss: 0.75812 - acc: 0.9426 -- iter: 16/36\n",
            "Training Step: 2253  | total loss: \u001b[1m\u001b[32m0.69736\u001b[0m\u001b[0m | time: 0.020s\n",
            "| Adam | epoch: 451 | loss: 0.69736 - acc: 0.9483 -- iter: 24/36\n",
            "Training Step: 2254  | total loss: \u001b[1m\u001b[32m1.01468\u001b[0m\u001b[0m | time: 0.027s\n",
            "| Adam | epoch: 451 | loss: 1.01468 - acc: 0.8910 -- iter: 32/36\n",
            "Training Step: 2255  | total loss: \u001b[1m\u001b[32m0.93566\u001b[0m\u001b[0m | time: 0.034s\n",
            "| Adam | epoch: 451 | loss: 0.93566 - acc: 0.9019 -- iter: 36/36\n",
            "--\n",
            "Training Step: 2256  | total loss: \u001b[1m\u001b[32m0.86020\u001b[0m\u001b[0m | time: 0.004s\n",
            "| Adam | epoch: 452 | loss: 0.86020 - acc: 0.9117 -- iter: 08/36\n",
            "Training Step: 2257  | total loss: \u001b[1m\u001b[32m0.72371\u001b[0m\u001b[0m | time: 0.009s\n",
            "| Adam | epoch: 452 | loss: 0.72371 - acc: 0.9285 -- iter: 16/36\n",
            "Training Step: 2258  | total loss: \u001b[1m\u001b[32m0.72371\u001b[0m\u001b[0m | time: 0.017s\n",
            "| Adam | epoch: 452 | loss: 0.72371 - acc: 0.9285 -- iter: 24/36\n",
            "Training Step: 2259  | total loss: \u001b[1m\u001b[32m1.32276\u001b[0m\u001b[0m | time: 0.023s\n",
            "| Adam | epoch: 452 | loss: 1.32276 - acc: 0.9356 -- iter: 32/36\n",
            "Training Step: 2260  | total loss: \u001b[1m\u001b[32m1.32276\u001b[0m\u001b[0m | time: 0.031s\n",
            "| Adam | epoch: 452 | loss: 1.32276 - acc: 0.8671 -- iter: 36/36\n",
            "--\n",
            "Training Step: 2261  | total loss: \u001b[1m\u001b[32m1.10338\u001b[0m\u001b[0m | time: 0.006s\n",
            "| Adam | epoch: 453 | loss: 1.10338 - acc: 0.8923 -- iter: 08/36\n",
            "Training Step: 2262  | total loss: \u001b[1m\u001b[32m1.10338\u001b[0m\u001b[0m | time: 0.012s\n",
            "| Adam | epoch: 453 | loss: 1.10338 - acc: 0.8923 -- iter: 16/36\n",
            "Training Step: 2263  | total loss: \u001b[1m\u001b[32m1.00690\u001b[0m\u001b[0m | time: 0.020s\n",
            "| Adam | epoch: 453 | loss: 1.00690 - acc: 0.9031 -- iter: 24/36\n",
            "Training Step: 2264  | total loss: \u001b[1m\u001b[32m0.92076\u001b[0m\u001b[0m | time: 0.025s\n",
            "| Adam | epoch: 453 | loss: 0.92076 - acc: 0.9128 -- iter: 32/36\n",
            "Training Step: 2265  | total loss: \u001b[1m\u001b[32m0.84356\u001b[0m\u001b[0m | time: 0.031s\n",
            "| Adam | epoch: 453 | loss: 0.84356 - acc: 0.9215 -- iter: 36/36\n",
            "--\n",
            "Training Step: 2266  | total loss: \u001b[1m\u001b[32m1.58822\u001b[0m\u001b[0m | time: 0.009s\n",
            "| Adam | epoch: 454 | loss: 1.58822 - acc: 0.8419 -- iter: 08/36\n",
            "Training Step: 2267  | total loss: \u001b[1m\u001b[32m1.44929\u001b[0m\u001b[0m | time: 0.018s\n",
            "| Adam | epoch: 454 | loss: 1.44929 - acc: 0.8577 -- iter: 16/36\n",
            "Training Step: 2268  | total loss: \u001b[1m\u001b[32m1.20163\u001b[0m\u001b[0m | time: 0.024s\n",
            "| Adam | epoch: 454 | loss: 1.20163 - acc: 0.8847 -- iter: 24/36\n",
            "Training Step: 2269  | total loss: \u001b[1m\u001b[32m1.20163\u001b[0m\u001b[0m | time: 0.034s\n",
            "| Adam | epoch: 454 | loss: 1.20163 - acc: 0.8847 -- iter: 32/36\n",
            "Training Step: 2270  | total loss: \u001b[1m\u001b[32m1.09404\u001b[0m\u001b[0m | time: 0.041s\n",
            "| Adam | epoch: 454 | loss: 1.09404 - acc: 0.8962 -- iter: 36/36\n",
            "--\n",
            "Training Step: 2271  | total loss: \u001b[1m\u001b[32m1.40259\u001b[0m\u001b[0m | time: 0.006s\n",
            "| Adam | epoch: 455 | loss: 1.40259 - acc: 0.9066 -- iter: 08/36\n",
            "Training Step: 2272  | total loss: \u001b[1m\u001b[32m1.40259\u001b[0m\u001b[0m | time: 0.013s\n",
            "| Adam | epoch: 455 | loss: 1.40259 - acc: 0.8535 -- iter: 16/36\n",
            "Training Step: 2273  | total loss: \u001b[1m\u001b[32m1.28004\u001b[0m\u001b[0m | time: 0.020s\n",
            "| Adam | epoch: 455 | loss: 1.28004 - acc: 0.8681 -- iter: 24/36\n",
            "Training Step: 2274  | total loss: \u001b[1m\u001b[32m1.06625\u001b[0m\u001b[0m | time: 0.026s\n",
            "| Adam | epoch: 455 | loss: 1.06625 - acc: 0.8932 -- iter: 32/36\n",
            "Training Step: 2275  | total loss: \u001b[1m\u001b[32m1.06625\u001b[0m\u001b[0m | time: 0.035s\n",
            "| Adam | epoch: 455 | loss: 1.06625 - acc: 0.8932 -- iter: 36/36\n",
            "--\n",
            "Training Step: 2276  | total loss: \u001b[1m\u001b[32m0.97749\u001b[0m\u001b[0m | time: 0.006s\n",
            "| Adam | epoch: 456 | loss: 0.97749 - acc: 0.9039 -- iter: 08/36\n",
            "Training Step: 2277  | total loss: \u001b[1m\u001b[32m1.47252\u001b[0m\u001b[0m | time: 0.009s\n",
            "| Adam | epoch: 456 | loss: 1.47252 - acc: 0.9135 -- iter: 16/36\n",
            "Training Step: 2278  | total loss: \u001b[1m\u001b[32m1.47252\u001b[0m\u001b[0m | time: 0.016s\n",
            "| Adam | epoch: 456 | loss: 1.47252 - acc: 0.8221 -- iter: 24/36\n",
            "Training Step: 2279  | total loss: \u001b[1m\u001b[32m1.34240\u001b[0m\u001b[0m | time: 0.025s\n",
            "| Adam | epoch: 456 | loss: 1.34240 - acc: 0.8399 -- iter: 32/36\n",
            "Training Step: 2280  | total loss: \u001b[1m\u001b[32m1.22721\u001b[0m\u001b[0m | time: 0.033s\n",
            "| Adam | epoch: 456 | loss: 1.22721 - acc: 0.8559 -- iter: 36/36\n",
            "--\n",
            "Training Step: 2281  | total loss: \u001b[1m\u001b[32m1.12357\u001b[0m\u001b[0m | time: 0.006s\n",
            "| Adam | epoch: 457 | loss: 1.12357 - acc: 0.8703 -- iter: 08/36\n",
            "Training Step: 2282  | total loss: \u001b[1m\u001b[32m1.03215\u001b[0m\u001b[0m | time: 0.013s\n",
            "| Adam | epoch: 457 | loss: 1.03215 - acc: 0.8833 -- iter: 16/36\n",
            "Training Step: 2283  | total loss: \u001b[1m\u001b[32m0.94345\u001b[0m\u001b[0m | time: 0.020s\n",
            "| Adam | epoch: 457 | loss: 0.94345 - acc: 0.8950 -- iter: 24/36\n",
            "Training Step: 2284  | total loss: \u001b[1m\u001b[32m1.89875\u001b[0m\u001b[0m | time: 0.027s\n",
            "| Adam | epoch: 457 | loss: 1.89875 - acc: 0.8180 -- iter: 32/36\n",
            "Training Step: 2285  | total loss: \u001b[1m\u001b[32m1.72635\u001b[0m\u001b[0m | time: 0.032s\n",
            "| Adam | epoch: 457 | loss: 1.72635 - acc: 0.8362 -- iter: 36/36\n",
            "--\n",
            "Training Step: 2286  | total loss: \u001b[1m\u001b[32m1.43488\u001b[0m\u001b[0m | time: 0.008s\n",
            "| Adam | epoch: 458 | loss: 1.43488 - acc: 0.8526 -- iter: 08/36\n",
            "Training Step: 2287  | total loss: \u001b[1m\u001b[32m1.43488\u001b[0m\u001b[0m | time: 0.014s\n",
            "| Adam | epoch: 458 | loss: 1.43488 - acc: 0.8673 -- iter: 16/36\n",
            "Training Step: 2288  | total loss: \u001b[1m\u001b[32m1.30913\u001b[0m\u001b[0m | time: 0.020s\n",
            "| Adam | epoch: 458 | loss: 1.30913 - acc: 0.8806 -- iter: 24/36\n",
            "Training Step: 2289  | total loss: \u001b[1m\u001b[32m1.19739\u001b[0m\u001b[0m | time: 0.027s\n",
            "| Adam | epoch: 458 | loss: 1.19739 - acc: 0.8925 -- iter: 32/36\n",
            "Training Step: 2290  | total loss: \u001b[1m\u001b[32m1.09091\u001b[0m\u001b[0m | time: 0.033s\n",
            "| Adam | epoch: 458 | loss: 1.09091 - acc: 0.9033 -- iter: 36/36\n",
            "--\n",
            "Training Step: 2291  | total loss: \u001b[1m\u001b[32m1.00132\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 459 | loss: 1.00132 - acc: 0.9129 -- iter: 08/36\n",
            "Training Step: 2292  | total loss: \u001b[1m\u001b[32m0.91567\u001b[0m\u001b[0m | time: 0.010s\n",
            "| Adam | epoch: 459 | loss: 0.91567 - acc: 0.9216 -- iter: 16/36\n",
            "Training Step: 2293  | total loss: \u001b[1m\u001b[32m0.83861\u001b[0m\u001b[0m | time: 0.016s\n",
            "| Adam | epoch: 459 | loss: 0.83861 - acc: 0.9295 -- iter: 24/36\n",
            "Training Step: 2294  | total loss: \u001b[1m\u001b[32m0.76961\u001b[0m\u001b[0m | time: 0.023s\n",
            "| Adam | epoch: 459 | loss: 0.76961 - acc: 0.9365 -- iter: 32/36\n",
            "Training Step: 2295  | total loss: \u001b[1m\u001b[32m0.71299\u001b[0m\u001b[0m | time: 0.030s\n",
            "| Adam | epoch: 459 | loss: 0.71299 - acc: 0.9429 -- iter: 36/36\n",
            "--\n",
            "Training Step: 2296  | total loss: \u001b[1m\u001b[32m0.65778\u001b[0m\u001b[0m | time: 0.006s\n",
            "| Adam | epoch: 460 | loss: 0.65778 - acc: 0.9486 -- iter: 08/36\n",
            "Training Step: 2297  | total loss: \u001b[1m\u001b[32m0.56504\u001b[0m\u001b[0m | time: 0.012s\n",
            "| Adam | epoch: 460 | loss: 0.56504 - acc: 0.9537 -- iter: 16/36\n",
            "Training Step: 2298  | total loss: \u001b[1m\u001b[32m0.56504\u001b[0m\u001b[0m | time: 0.018s\n",
            "| Adam | epoch: 460 | loss: 0.56504 - acc: 0.9584 -- iter: 24/36\n",
            "Training Step: 2299  | total loss: \u001b[1m\u001b[32m0.52776\u001b[0m\u001b[0m | time: 0.027s\n",
            "| Adam | epoch: 460 | loss: 0.52776 - acc: 0.9625 -- iter: 32/36\n",
            "Training Step: 2300  | total loss: \u001b[1m\u001b[32m0.49169\u001b[0m\u001b[0m | time: 0.033s\n",
            "| Adam | epoch: 460 | loss: 0.49169 - acc: 0.9663 -- iter: 36/36\n",
            "--\n",
            "Training Step: 2301  | total loss: \u001b[1m\u001b[32m0.46227\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 461 | loss: 0.46227 - acc: 0.9696 -- iter: 08/36\n",
            "Training Step: 2302  | total loss: \u001b[1m\u001b[32m0.40470\u001b[0m\u001b[0m | time: 0.008s\n",
            "| Adam | epoch: 461 | loss: 0.40470 - acc: 0.9727 -- iter: 16/36\n",
            "Training Step: 2303  | total loss: \u001b[1m\u001b[32m0.40470\u001b[0m\u001b[0m | time: 0.017s\n",
            "| Adam | epoch: 461 | loss: 0.40470 - acc: 0.9754 -- iter: 24/36\n",
            "Training Step: 2304  | total loss: \u001b[1m\u001b[32m0.36898\u001b[0m\u001b[0m | time: 0.022s\n",
            "| Adam | epoch: 461 | loss: 0.36898 - acc: 0.9779 -- iter: 32/36\n",
            "Training Step: 2305  | total loss: \u001b[1m\u001b[32m0.34777\u001b[0m\u001b[0m | time: 0.027s\n",
            "| Adam | epoch: 461 | loss: 0.34777 - acc: 0.9801 -- iter: 36/36\n",
            "--\n",
            "Training Step: 2306  | total loss: \u001b[1m\u001b[32m0.32935\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 462 | loss: 0.32935 - acc: 0.9821 -- iter: 08/36\n",
            "Training Step: 2307  | total loss: \u001b[1m\u001b[32m0.31600\u001b[0m\u001b[0m | time: 0.011s\n",
            "| Adam | epoch: 462 | loss: 0.31600 - acc: 0.9839 -- iter: 16/36\n",
            "Training Step: 2308  | total loss: \u001b[1m\u001b[32m0.30302\u001b[0m\u001b[0m | time: 0.017s\n",
            "| Adam | epoch: 462 | loss: 0.30302 - acc: 0.9855 -- iter: 24/36\n",
            "Training Step: 2309  | total loss: \u001b[1m\u001b[32m0.28292\u001b[0m\u001b[0m | time: 0.026s\n",
            "| Adam | epoch: 462 | loss: 0.28292 - acc: 0.9869 -- iter: 32/36\n",
            "Training Step: 2310  | total loss: \u001b[1m\u001b[32m0.28292\u001b[0m\u001b[0m | time: 0.032s\n",
            "| Adam | epoch: 462 | loss: 0.28292 - acc: 0.9882 -- iter: 36/36\n",
            "--\n",
            "Training Step: 2311  | total loss: \u001b[1m\u001b[32m0.26477\u001b[0m\u001b[0m | time: 0.010s\n",
            "| Adam | epoch: 463 | loss: 0.26477 - acc: 0.9894 -- iter: 08/36\n",
            "Training Step: 2312  | total loss: \u001b[1m\u001b[32m0.25340\u001b[0m\u001b[0m | time: 0.018s\n",
            "| Adam | epoch: 463 | loss: 0.25340 - acc: 0.9905 -- iter: 16/36\n",
            "Training Step: 2313  | total loss: \u001b[1m\u001b[32m0.24029\u001b[0m\u001b[0m | time: 0.022s\n",
            "| Adam | epoch: 463 | loss: 0.24029 - acc: 0.9914 -- iter: 24/36\n",
            "Training Step: 2314  | total loss: \u001b[1m\u001b[32m0.24029\u001b[0m\u001b[0m | time: 0.029s\n",
            "| Adam | epoch: 463 | loss: 0.24029 - acc: 0.9923 -- iter: 32/36\n",
            "Training Step: 2315  | total loss: \u001b[1m\u001b[32m0.23171\u001b[0m\u001b[0m | time: 0.035s\n",
            "| Adam | epoch: 463 | loss: 0.23171 - acc: 0.9931 -- iter: 36/36\n",
            "--\n",
            "Training Step: 2316  | total loss: \u001b[1m\u001b[32m0.23171\u001b[0m\u001b[0m | time: 0.008s\n",
            "| Adam | epoch: 464 | loss: 0.23171 - acc: 0.9937 -- iter: 08/36\n",
            "Training Step: 2317  | total loss: \u001b[1m\u001b[32m0.22022\u001b[0m\u001b[0m | time: 0.014s\n",
            "| Adam | epoch: 464 | loss: 0.22022 - acc: 0.9944 -- iter: 16/36\n",
            "Training Step: 2318  | total loss: \u001b[1m\u001b[32m0.22022\u001b[0m\u001b[0m | time: 0.022s\n",
            "| Adam | epoch: 464 | loss: 0.22022 - acc: 0.9949 -- iter: 24/36\n",
            "Training Step: 2319  | total loss: \u001b[1m\u001b[32m0.21024\u001b[0m\u001b[0m | time: 0.028s\n",
            "| Adam | epoch: 464 | loss: 0.21024 - acc: 0.9954 -- iter: 32/36\n",
            "Training Step: 2320  | total loss: \u001b[1m\u001b[32m0.21051\u001b[0m\u001b[0m | time: 0.035s\n",
            "| Adam | epoch: 464 | loss: 0.21051 - acc: 0.9959 -- iter: 36/36\n",
            "--\n",
            "Training Step: 2321  | total loss: \u001b[1m\u001b[32m0.20555\u001b[0m\u001b[0m | time: 0.006s\n",
            "| Adam | epoch: 465 | loss: 0.20555 - acc: 0.9963 -- iter: 08/36\n",
            "Training Step: 2322  | total loss: \u001b[1m\u001b[32m0.20433\u001b[0m\u001b[0m | time: 0.010s\n",
            "| Adam | epoch: 465 | loss: 0.20433 - acc: 0.9967 -- iter: 16/36\n",
            "Training Step: 2323  | total loss: \u001b[1m\u001b[32m0.19522\u001b[0m\u001b[0m | time: 0.013s\n",
            "| Adam | epoch: 465 | loss: 0.19522 - acc: 0.9970 -- iter: 24/36\n",
            "Training Step: 2324  | total loss: \u001b[1m\u001b[32m0.19558\u001b[0m\u001b[0m | time: 0.018s\n",
            "| Adam | epoch: 465 | loss: 0.19558 - acc: 0.9973 -- iter: 32/36\n",
            "Training Step: 2325  | total loss: \u001b[1m\u001b[32m0.87128\u001b[0m\u001b[0m | time: 0.022s\n",
            "| Adam | epoch: 465 | loss: 0.87128 - acc: 0.9976 -- iter: 36/36\n",
            "--\n",
            "Training Step: 2326  | total loss: \u001b[1m\u001b[32m0.79978\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 466 | loss: 0.79978 - acc: 0.9103 -- iter: 08/36\n",
            "Training Step: 2327  | total loss: \u001b[1m\u001b[32m0.72926\u001b[0m\u001b[0m | time: 0.009s\n",
            "| Adam | epoch: 466 | loss: 0.72926 - acc: 0.9193 -- iter: 16/36\n",
            "Training Step: 2328  | total loss: \u001b[1m\u001b[32m0.66584\u001b[0m\u001b[0m | time: 0.015s\n",
            "| Adam | epoch: 466 | loss: 0.66584 - acc: 0.9274 -- iter: 24/36\n",
            "Training Step: 2329  | total loss: \u001b[1m\u001b[32m0.61509\u001b[0m\u001b[0m | time: 0.019s\n",
            "| Adam | epoch: 466 | loss: 0.61509 - acc: 0.9346 -- iter: 32/36\n",
            "Training Step: 2330  | total loss: \u001b[1m\u001b[32m0.57408\u001b[0m\u001b[0m | time: 0.026s\n",
            "| Adam | epoch: 466 | loss: 0.57408 - acc: 0.9412 -- iter: 36/36\n",
            "--\n",
            "Training Step: 2331  | total loss: \u001b[1m\u001b[32m1.26820\u001b[0m\u001b[0m | time: 0.004s\n",
            "| Adam | epoch: 467 | loss: 1.26820 - acc: 0.9470 -- iter: 08/36\n",
            "Training Step: 2332  | total loss: \u001b[1m\u001b[32m1.15515\u001b[0m\u001b[0m | time: 0.009s\n",
            "| Adam | epoch: 467 | loss: 1.15515 - acc: 0.8773 -- iter: 16/36\n",
            "Training Step: 2333  | total loss: \u001b[1m\u001b[32m1.05912\u001b[0m\u001b[0m | time: 0.013s\n",
            "| Adam | epoch: 467 | loss: 1.05912 - acc: 0.8896 -- iter: 24/36\n",
            "Training Step: 2334  | total loss: \u001b[1m\u001b[32m0.97273\u001b[0m\u001b[0m | time: 0.018s\n",
            "| Adam | epoch: 467 | loss: 0.97273 - acc: 0.9006 -- iter: 32/36\n",
            "Training Step: 2335  | total loss: \u001b[1m\u001b[32m0.89333\u001b[0m\u001b[0m | time: 0.022s\n",
            "| Adam | epoch: 467 | loss: 0.89333 - acc: 0.9106 -- iter: 36/36\n",
            "--\n",
            "Training Step: 2336  | total loss: \u001b[1m\u001b[32m0.82094\u001b[0m\u001b[0m | time: 0.004s\n",
            "| Adam | epoch: 468 | loss: 0.82094 - acc: 0.9195 -- iter: 08/36\n",
            "Training Step: 2337  | total loss: \u001b[1m\u001b[32m0.75479\u001b[0m\u001b[0m | time: 0.008s\n",
            "| Adam | epoch: 468 | loss: 0.75479 - acc: 0.9276 -- iter: 16/36\n",
            "Training Step: 2338  | total loss: \u001b[1m\u001b[32m0.69485\u001b[0m\u001b[0m | time: 0.013s\n",
            "| Adam | epoch: 468 | loss: 0.69485 - acc: 0.9348 -- iter: 24/36\n",
            "Training Step: 2339  | total loss: \u001b[1m\u001b[32m0.64238\u001b[0m\u001b[0m | time: 0.016s\n",
            "| Adam | epoch: 468 | loss: 0.64238 - acc: 0.9413 -- iter: 32/36\n",
            "Training Step: 2340  | total loss: \u001b[1m\u001b[32m0.59514\u001b[0m\u001b[0m | time: 0.019s\n",
            "| Adam | epoch: 468 | loss: 0.59514 - acc: 0.9472 -- iter: 36/36\n",
            "--\n",
            "Training Step: 2341  | total loss: \u001b[1m\u001b[32m0.55113\u001b[0m\u001b[0m | time: 0.004s\n",
            "| Adam | epoch: 469 | loss: 0.55113 - acc: 0.9525 -- iter: 08/36\n",
            "Training Step: 2342  | total loss: \u001b[1m\u001b[32m0.51164\u001b[0m\u001b[0m | time: 0.008s\n",
            "| Adam | epoch: 469 | loss: 0.51164 - acc: 0.9572 -- iter: 16/36\n",
            "Training Step: 2343  | total loss: \u001b[1m\u001b[32m0.47999\u001b[0m\u001b[0m | time: 0.012s\n",
            "| Adam | epoch: 469 | loss: 0.47999 - acc: 0.9615 -- iter: 24/36\n",
            "Training Step: 2344  | total loss: \u001b[1m\u001b[32m0.44800\u001b[0m\u001b[0m | time: 0.016s\n",
            "| Adam | epoch: 469 | loss: 0.44800 - acc: 0.9654 -- iter: 32/36\n",
            "Training Step: 2345  | total loss: \u001b[1m\u001b[32m0.42380\u001b[0m\u001b[0m | time: 0.020s\n",
            "| Adam | epoch: 469 | loss: 0.42380 - acc: 0.9688 -- iter: 36/36\n",
            "--\n",
            "Training Step: 2346  | total loss: \u001b[1m\u001b[32m0.40198\u001b[0m\u001b[0m | time: 0.004s\n",
            "| Adam | epoch: 470 | loss: 0.40198 - acc: 0.9719 -- iter: 08/36\n",
            "Training Step: 2347  | total loss: \u001b[1m\u001b[32m0.37776\u001b[0m\u001b[0m | time: 0.008s\n",
            "| Adam | epoch: 470 | loss: 0.37776 - acc: 0.9747 -- iter: 16/36\n",
            "Training Step: 2348  | total loss: \u001b[1m\u001b[32m0.37776\u001b[0m\u001b[0m | time: 0.013s\n",
            "| Adam | epoch: 470 | loss: 0.37776 - acc: 0.9773 -- iter: 24/36\n",
            "Training Step: 2349  | total loss: \u001b[1m\u001b[32m0.35578\u001b[0m\u001b[0m | time: 0.017s\n",
            "| Adam | epoch: 470 | loss: 0.35578 - acc: 0.9816 -- iter: 32/36\n",
            "Training Step: 2350  | total loss: \u001b[1m\u001b[32m0.33649\u001b[0m\u001b[0m | time: 0.021s\n",
            "| Adam | epoch: 470 | loss: 0.33649 - acc: 0.9834 -- iter: 36/36\n",
            "--\n",
            "Training Step: 2351  | total loss: \u001b[1m\u001b[32m0.31598\u001b[0m\u001b[0m | time: 0.004s\n",
            "| Adam | epoch: 471 | loss: 0.31598 - acc: 0.9851 -- iter: 08/36\n",
            "Training Step: 2352  | total loss: \u001b[1m\u001b[32m0.30682\u001b[0m\u001b[0m | time: 0.008s\n",
            "| Adam | epoch: 471 | loss: 0.30682 - acc: 0.9851 -- iter: 16/36\n",
            "Training Step: 2353  | total loss: \u001b[1m\u001b[32m0.29852\u001b[0m\u001b[0m | time: 0.013s\n",
            "| Adam | epoch: 471 | loss: 0.29852 - acc: 0.9866 -- iter: 24/36\n",
            "Training Step: 2354  | total loss: \u001b[1m\u001b[32m0.28744\u001b[0m\u001b[0m | time: 0.016s\n",
            "| Adam | epoch: 471 | loss: 0.28744 - acc: 0.9879 -- iter: 32/36\n",
            "Training Step: 2355  | total loss: \u001b[1m\u001b[32m0.27312\u001b[0m\u001b[0m | time: 0.020s\n",
            "| Adam | epoch: 471 | loss: 0.27312 - acc: 0.9891 -- iter: 36/36\n",
            "--\n",
            "Training Step: 2356  | total loss: \u001b[1m\u001b[32m0.26183\u001b[0m\u001b[0m | time: 0.003s\n",
            "| Adam | epoch: 472 | loss: 0.26183 - acc: 0.9902 -- iter: 08/36\n",
            "Training Step: 2357  | total loss: \u001b[1m\u001b[32m0.25299\u001b[0m\u001b[0m | time: 0.008s\n",
            "| Adam | epoch: 472 | loss: 0.25299 - acc: 0.9912 -- iter: 16/36\n",
            "Training Step: 2358  | total loss: \u001b[1m\u001b[32m0.24578\u001b[0m\u001b[0m | time: 0.012s\n",
            "| Adam | epoch: 472 | loss: 0.24578 - acc: 0.9921 -- iter: 24/36\n",
            "Training Step: 2359  | total loss: \u001b[1m\u001b[32m0.23920\u001b[0m\u001b[0m | time: 0.016s\n",
            "| Adam | epoch: 472 | loss: 0.23920 - acc: 0.9936 -- iter: 32/36\n",
            "Training Step: 2360  | total loss: \u001b[1m\u001b[32m0.23123\u001b[0m\u001b[0m | time: 0.019s\n",
            "| Adam | epoch: 472 | loss: 0.23123 - acc: 0.9942 -- iter: 36/36\n",
            "--\n",
            "Training Step: 2361  | total loss: \u001b[1m\u001b[32m0.22608\u001b[0m\u001b[0m | time: 0.003s\n",
            "| Adam | epoch: 473 | loss: 0.22608 - acc: 0.9948 -- iter: 08/36\n",
            "Training Step: 2362  | total loss: \u001b[1m\u001b[32m0.21592\u001b[0m\u001b[0m | time: 0.008s\n",
            "| Adam | epoch: 473 | loss: 0.21592 - acc: 0.9953 -- iter: 16/36\n",
            "Training Step: 2363  | total loss: \u001b[1m\u001b[32m0.21163\u001b[0m\u001b[0m | time: 0.012s\n",
            "| Adam | epoch: 473 | loss: 0.21163 - acc: 0.9958 -- iter: 24/36\n",
            "Training Step: 2364  | total loss: \u001b[1m\u001b[32m0.20136\u001b[0m\u001b[0m | time: 0.016s\n",
            "| Adam | epoch: 473 | loss: 0.20136 - acc: 0.9962 -- iter: 32/36\n",
            "Training Step: 2365  | total loss: \u001b[1m\u001b[32m0.19209\u001b[0m\u001b[0m | time: 0.020s\n",
            "| Adam | epoch: 473 | loss: 0.19209 - acc: 0.9966 -- iter: 36/36\n",
            "--\n",
            "Training Step: 2366  | total loss: \u001b[1m\u001b[32m0.19129\u001b[0m\u001b[0m | time: 0.003s\n",
            "| Adam | epoch: 474 | loss: 0.19129 - acc: 0.9969 -- iter: 08/36\n",
            "Training Step: 2367  | total loss: \u001b[1m\u001b[32m0.18835\u001b[0m\u001b[0m | time: 0.007s\n",
            "| Adam | epoch: 474 | loss: 0.18835 - acc: 0.9972 -- iter: 16/36\n",
            "Training Step: 2368  | total loss: \u001b[1m\u001b[32m0.18392\u001b[0m\u001b[0m | time: 0.012s\n",
            "| Adam | epoch: 474 | loss: 0.18392 - acc: 0.9975 -- iter: 24/36\n",
            "Training Step: 2369  | total loss: \u001b[1m\u001b[32m0.17726\u001b[0m\u001b[0m | time: 0.016s\n",
            "| Adam | epoch: 474 | loss: 0.17726 - acc: 0.9978 -- iter: 32/36\n",
            "Training Step: 2370  | total loss: \u001b[1m\u001b[32m0.18430\u001b[0m\u001b[0m | time: 0.021s\n",
            "| Adam | epoch: 474 | loss: 0.18430 - acc: 0.9980 -- iter: 36/36\n",
            "--\n",
            "Training Step: 2371  | total loss: \u001b[1m\u001b[32m0.19054\u001b[0m\u001b[0m | time: 0.004s\n",
            "| Adam | epoch: 475 | loss: 0.19054 - acc: 0.9982 -- iter: 08/36\n",
            "Training Step: 2372  | total loss: \u001b[1m\u001b[32m0.19025\u001b[0m\u001b[0m | time: 0.008s\n",
            "| Adam | epoch: 475 | loss: 0.19025 - acc: 0.9984 -- iter: 16/36\n",
            "Training Step: 2373  | total loss: \u001b[1m\u001b[32m0.18718\u001b[0m\u001b[0m | time: 0.012s\n",
            "| Adam | epoch: 475 | loss: 0.18718 - acc: 0.9985 -- iter: 24/36\n",
            "Training Step: 2374  | total loss: \u001b[1m\u001b[32m0.18042\u001b[0m\u001b[0m | time: 0.015s\n",
            "| Adam | epoch: 475 | loss: 0.18042 - acc: 0.9987 -- iter: 32/36\n",
            "Training Step: 2375  | total loss: \u001b[1m\u001b[32m0.17615\u001b[0m\u001b[0m | time: 0.018s\n",
            "| Adam | epoch: 475 | loss: 0.17615 - acc: 0.9988 -- iter: 36/36\n",
            "--\n",
            "Training Step: 2436  | total loss: \u001b[1m\u001b[32m1.16091\u001b[0m\u001b[0m | time: 0.010s\n",
            "| Adam | epoch: 488 | loss: 1.16091 - acc: 0.8792 -- iter: 08/36\n",
            "Training Step: 2437  | total loss: \u001b[1m\u001b[32m0.96793\u001b[0m\u001b[0m | time: 0.017s\n",
            "| Adam | epoch: 488 | loss: 0.96793 - acc: 0.8913 -- iter: 16/36\n",
            "Training Step: 2438  | total loss: \u001b[1m\u001b[32m0.96793\u001b[0m\u001b[0m | time: 0.024s\n",
            "| Adam | epoch: 488 | loss: 0.96793 - acc: 0.9022 -- iter: 24/36\n",
            "Training Step: 2439  | total loss: \u001b[1m\u001b[32m0.81527\u001b[0m\u001b[0m | time: 0.028s\n",
            "| Adam | epoch: 488 | loss: 0.81527 - acc: 0.9208 -- iter: 32/36\n",
            "Training Step: 2440  | total loss: \u001b[1m\u001b[32m0.81527\u001b[0m\u001b[0m | time: 0.038s\n",
            "| Adam | epoch: 488 | loss: 0.81527 - acc: 0.9208 -- iter: 36/36\n",
            "--\n",
            "Training Step: 2441  | total loss: \u001b[1m\u001b[32m0.68539\u001b[0m\u001b[0m | time: 0.008s\n",
            "| Adam | epoch: 489 | loss: 0.68539 - acc: 0.9358 -- iter: 08/36\n",
            "Training Step: 2442  | total loss: \u001b[1m\u001b[32m0.62812\u001b[0m\u001b[0m | time: 0.016s\n",
            "| Adam | epoch: 489 | loss: 0.62812 - acc: 0.9358 -- iter: 16/36\n",
            "Training Step: 2443  | total loss: \u001b[1m\u001b[32m0.58151\u001b[0m\u001b[0m | time: 0.021s\n",
            "| Adam | epoch: 489 | loss: 0.58151 - acc: 0.9422 -- iter: 24/36\n",
            "Training Step: 2444  | total loss: \u001b[1m\u001b[32m0.54503\u001b[0m\u001b[0m | time: 0.026s\n",
            "| Adam | epoch: 489 | loss: 0.54503 - acc: 0.9480 -- iter: 32/36\n",
            "Training Step: 2445  | total loss: \u001b[1m\u001b[32m1.48551\u001b[0m\u001b[0m | time: 0.034s\n",
            "| Adam | epoch: 489 | loss: 1.48551 - acc: 0.9532 -- iter: 36/36\n",
            "--\n",
            "Training Step: 2446  | total loss: \u001b[1m\u001b[32m1.35449\u001b[0m\u001b[0m | time: 0.004s\n",
            "| Adam | epoch: 490 | loss: 1.35449 - acc: 0.8579 -- iter: 08/36\n",
            "Training Step: 2447  | total loss: \u001b[1m\u001b[32m1.35449\u001b[0m\u001b[0m | time: 0.009s\n",
            "| Adam | epoch: 490 | loss: 1.35449 - acc: 0.8721 -- iter: 16/36\n",
            "Training Step: 2448  | total loss: \u001b[1m\u001b[32m1.23411\u001b[0m\u001b[0m | time: 0.015s\n",
            "| Adam | epoch: 490 | loss: 1.23411 - acc: 0.8849 -- iter: 24/36\n",
            "Training Step: 2449  | total loss: \u001b[1m\u001b[32m1.02820\u001b[0m\u001b[0m | time: 0.020s\n",
            "| Adam | epoch: 490 | loss: 1.02820 - acc: 0.8964 -- iter: 32/36\n",
            "Training Step: 2450  | total loss: \u001b[1m\u001b[32m0.94142\u001b[0m\u001b[0m | time: 0.025s\n",
            "| Adam | epoch: 490 | loss: 0.94142 - acc: 0.9068 -- iter: 36/36\n",
            "--\n",
            "Training Step: 2451  | total loss: \u001b[1m\u001b[32m0.86289\u001b[0m\u001b[0m | time: 0.006s\n",
            "| Adam | epoch: 491 | loss: 0.86289 - acc: 0.9161 -- iter: 08/36\n",
            "Training Step: 2452  | total loss: \u001b[1m\u001b[32m0.79015\u001b[0m\u001b[0m | time: 0.011s\n",
            "| Adam | epoch: 491 | loss: 0.79015 - acc: 0.9245 -- iter: 16/36\n",
            "Training Step: 2453  | total loss: \u001b[1m\u001b[32m0.73139\u001b[0m\u001b[0m | time: 0.016s\n",
            "| Adam | epoch: 491 | loss: 0.73139 - acc: 0.9320 -- iter: 24/36\n",
            "Training Step: 2454  | total loss: \u001b[1m\u001b[32m0.67847\u001b[0m\u001b[0m | time: 0.022s\n",
            "| Adam | epoch: 491 | loss: 0.67847 - acc: 0.9388 -- iter: 32/36\n",
            "Training Step: 2455  | total loss: \u001b[1m\u001b[32m0.62256\u001b[0m\u001b[0m | time: 0.027s\n",
            "| Adam | epoch: 491 | loss: 0.62256 - acc: 0.9449 -- iter: 36/36\n",
            "--\n",
            "Training Step: 2456  | total loss: \u001b[1m\u001b[32m0.57915\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 492 | loss: 0.57915 - acc: 0.9504 -- iter: 08/36\n",
            "Training Step: 2457  | total loss: \u001b[1m\u001b[32m1.44051\u001b[0m\u001b[0m | time: 0.009s\n",
            "| Adam | epoch: 492 | loss: 1.44051 - acc: 0.9554 -- iter: 16/36\n",
            "Training Step: 2458  | total loss: \u001b[1m\u001b[32m1.31564\u001b[0m\u001b[0m | time: 0.014s\n",
            "| Adam | epoch: 492 | loss: 1.31564 - acc: 0.8724 -- iter: 24/36\n",
            "Training Step: 2459  | total loss: \u001b[1m\u001b[32m1.19805\u001b[0m\u001b[0m | time: 0.018s\n",
            "| Adam | epoch: 492 | loss: 1.19805 - acc: 0.8851 -- iter: 32/36\n",
            "Training Step: 2460  | total loss: \u001b[1m\u001b[32m1.09227\u001b[0m\u001b[0m | time: 0.023s\n",
            "| Adam | epoch: 492 | loss: 1.09227 - acc: 0.8966 -- iter: 36/36\n",
            "--\n",
            "Training Step: 2461  | total loss: \u001b[1m\u001b[32m1.00216\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 493 | loss: 1.00216 - acc: 0.9070 -- iter: 08/36\n",
            "Training Step: 2462  | total loss: \u001b[1m\u001b[32m0.91856\u001b[0m\u001b[0m | time: 0.009s\n",
            "| Adam | epoch: 493 | loss: 0.91856 - acc: 0.9163 -- iter: 16/36\n",
            "Training Step: 2463  | total loss: \u001b[1m\u001b[32m1.39976\u001b[0m\u001b[0m | time: 0.015s\n",
            "| Adam | epoch: 493 | loss: 1.39976 - acc: 0.9246 -- iter: 24/36\n",
            "Training Step: 2464  | total loss: \u001b[1m\u001b[32m1.27671\u001b[0m\u001b[0m | time: 0.020s\n",
            "| Adam | epoch: 493 | loss: 1.27671 - acc: 0.8572 -- iter: 32/36\n",
            "Training Step: 2465  | total loss: \u001b[1m\u001b[32m1.16466\u001b[0m\u001b[0m | time: 0.024s\n",
            "| Adam | epoch: 493 | loss: 1.16466 - acc: 0.8715 -- iter: 36/36\n",
            "--\n",
            "Training Step: 2466  | total loss: \u001b[1m\u001b[32m1.06385\u001b[0m\u001b[0m | time: 0.004s\n",
            "| Adam | epoch: 494 | loss: 1.06385 - acc: 0.8843 -- iter: 08/36\n",
            "Training Step: 2467  | total loss: \u001b[1m\u001b[32m0.96890\u001b[0m\u001b[0m | time: 0.009s\n",
            "| Adam | epoch: 494 | loss: 0.96890 - acc: 0.9063 -- iter: 16/36\n",
            "Training Step: 2468  | total loss: \u001b[1m\u001b[32m0.89352\u001b[0m\u001b[0m | time: 0.013s\n",
            "| Adam | epoch: 494 | loss: 0.89352 - acc: 0.9157 -- iter: 24/36\n",
            "Training Step: 2469  | total loss: \u001b[1m\u001b[32m1.36155\u001b[0m\u001b[0m | time: 0.017s\n",
            "| Adam | epoch: 494 | loss: 1.36155 - acc: 0.9157 -- iter: 32/36\n",
            "Training Step: 2470  | total loss: \u001b[1m\u001b[32m1.23614\u001b[0m\u001b[0m | time: 0.022s\n",
            "| Adam | epoch: 494 | loss: 1.23614 - acc: 0.8616 -- iter: 36/36\n",
            "--\n",
            "Training Step: 2471  | total loss: \u001b[1m\u001b[32m1.12716\u001b[0m\u001b[0m | time: 0.004s\n",
            "| Adam | epoch: 495 | loss: 1.12716 - acc: 0.8754 -- iter: 08/36\n",
            "Training Step: 2472  | total loss: \u001b[1m\u001b[32m1.02898\u001b[0m\u001b[0m | time: 0.008s\n",
            "| Adam | epoch: 495 | loss: 1.02898 - acc: 0.8879 -- iter: 16/36\n",
            "Training Step: 2473  | total loss: \u001b[1m\u001b[32m0.95058\u001b[0m\u001b[0m | time: 0.014s\n",
            "| Adam | epoch: 495 | loss: 0.95058 - acc: 0.8991 -- iter: 24/36\n",
            "Training Step: 2474  | total loss: \u001b[1m\u001b[32m0.87415\u001b[0m\u001b[0m | time: 0.020s\n",
            "| Adam | epoch: 495 | loss: 0.87415 - acc: 0.9092 -- iter: 32/36\n",
            "Training Step: 2475  | total loss: \u001b[1m\u001b[32m0.80079\u001b[0m\u001b[0m | time: 0.024s\n",
            "| Adam | epoch: 495 | loss: 0.80079 - acc: 0.9183 -- iter: 36/36\n",
            "--\n",
            "Training Step: 2476  | total loss: \u001b[1m\u001b[32m0.73814\u001b[0m\u001b[0m | time: 0.004s\n",
            "| Adam | epoch: 496 | loss: 0.73814 - acc: 0.9264 -- iter: 08/36\n",
            "Training Step: 2477  | total loss: \u001b[1m\u001b[32m0.68116\u001b[0m\u001b[0m | time: 0.009s\n",
            "| Adam | epoch: 496 | loss: 0.68116 - acc: 0.9338 -- iter: 16/36\n",
            "Training Step: 2478  | total loss: \u001b[1m\u001b[32m0.62981\u001b[0m\u001b[0m | time: 0.012s\n",
            "| Adam | epoch: 496 | loss: 0.62981 - acc: 0.9464 -- iter: 24/36\n",
            "Training Step: 2479  | total loss: \u001b[1m\u001b[32m0.58060\u001b[0m\u001b[0m | time: 0.017s\n",
            "| Adam | epoch: 496 | loss: 0.58060 - acc: 0.9517 -- iter: 32/36\n",
            "Training Step: 2480  | total loss: \u001b[1m\u001b[32m0.53919\u001b[0m\u001b[0m | time: 0.021s\n",
            "| Adam | epoch: 496 | loss: 0.53919 - acc: 0.9517 -- iter: 36/36\n",
            "--\n",
            "Training Step: 2481  | total loss: \u001b[1m\u001b[32m0.50502\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 497 | loss: 0.50502 - acc: 0.9566 -- iter: 08/36\n",
            "Training Step: 2482  | total loss: \u001b[1m\u001b[32m0.46931\u001b[0m\u001b[0m | time: 0.012s\n",
            "| Adam | epoch: 497 | loss: 0.46931 - acc: 0.9609 -- iter: 16/36\n",
            "Training Step: 2483  | total loss: \u001b[1m\u001b[32m0.44833\u001b[0m\u001b[0m | time: 0.017s\n",
            "| Adam | epoch: 497 | loss: 0.44833 - acc: 0.9648 -- iter: 24/36\n",
            "Training Step: 2484  | total loss: \u001b[1m\u001b[32m0.42938\u001b[0m\u001b[0m | time: 0.020s\n",
            "| Adam | epoch: 497 | loss: 0.42938 - acc: 0.9683 -- iter: 32/36\n",
            "Training Step: 2485  | total loss: \u001b[1m\u001b[32m0.40480\u001b[0m\u001b[0m | time: 0.027s\n",
            "| Adam | epoch: 497 | loss: 0.40480 - acc: 0.9715 -- iter: 36/36\n",
            "--\n",
            "Training Step: 2486  | total loss: \u001b[1m\u001b[32m0.37880\u001b[0m\u001b[0m | time: 0.004s\n",
            "| Adam | epoch: 498 | loss: 0.37880 - acc: 0.9744 -- iter: 08/36\n",
            "Training Step: 2487  | total loss: \u001b[1m\u001b[32m0.35593\u001b[0m\u001b[0m | time: 0.010s\n",
            "| Adam | epoch: 498 | loss: 0.35593 - acc: 0.9769 -- iter: 16/36\n",
            "Training Step: 2488  | total loss: \u001b[1m\u001b[32m0.33772\u001b[0m\u001b[0m | time: 0.015s\n",
            "| Adam | epoch: 498 | loss: 0.33772 - acc: 0.9792 -- iter: 24/36\n",
            "Training Step: 2489  | total loss: \u001b[1m\u001b[32m0.32048\u001b[0m\u001b[0m | time: 0.020s\n",
            "| Adam | epoch: 498 | loss: 0.32048 - acc: 0.9813 -- iter: 32/36\n",
            "Training Step: 2490  | total loss: \u001b[1m\u001b[32m0.30487\u001b[0m\u001b[0m | time: 0.025s\n",
            "| Adam | epoch: 498 | loss: 0.30487 - acc: 0.9832 -- iter: 36/36\n",
            "--\n",
            "Training Step: 2491  | total loss: \u001b[1m\u001b[32m0.29177\u001b[0m\u001b[0m | time: 0.004s\n",
            "| Adam | epoch: 499 | loss: 0.29177 - acc: 0.9849 -- iter: 08/36\n",
            "Training Step: 2492  | total loss: \u001b[1m\u001b[32m0.27904\u001b[0m\u001b[0m | time: 0.009s\n",
            "| Adam | epoch: 499 | loss: 0.27904 - acc: 0.9864 -- iter: 16/36\n",
            "Training Step: 2493  | total loss: \u001b[1m\u001b[32m0.26635\u001b[0m\u001b[0m | time: 0.014s\n",
            "| Adam | epoch: 499 | loss: 0.26635 - acc: 0.9877 -- iter: 24/36\n",
            "Training Step: 2494  | total loss: \u001b[1m\u001b[32m0.25639\u001b[0m\u001b[0m | time: 0.017s\n",
            "| Adam | epoch: 499 | loss: 0.25639 - acc: 0.9890 -- iter: 32/36\n",
            "Training Step: 2495  | total loss: \u001b[1m\u001b[32m0.24494\u001b[0m\u001b[0m | time: 0.022s\n",
            "| Adam | epoch: 499 | loss: 0.24494 - acc: 0.9901 -- iter: 36/36\n",
            "--\n",
            "Training Step: 2496  | total loss: \u001b[1m\u001b[32m0.23458\u001b[0m\u001b[0m | time: 0.004s\n",
            "| Adam | epoch: 500 | loss: 0.23458 - acc: 0.9911 -- iter: 08/36\n",
            "Training Step: 2497  | total loss: \u001b[1m\u001b[32m0.23458\u001b[0m\u001b[0m | time: 0.007s\n",
            "| Adam | epoch: 500 | loss: 0.23458 - acc: 0.9920 -- iter: 16/36\n",
            "Training Step: 2498  | total loss: \u001b[1m\u001b[32m0.22795\u001b[0m\u001b[0m | time: 0.013s\n",
            "| Adam | epoch: 500 | loss: 0.22795 - acc: 0.9928 -- iter: 24/36\n",
            "Training Step: 2499  | total loss: \u001b[1m\u001b[32m0.22584\u001b[0m\u001b[0m | time: 0.016s\n",
            "| Adam | epoch: 500 | loss: 0.22584 - acc: 0.9935 -- iter: 32/36\n",
            "Training Step: 2500  | total loss: \u001b[1m\u001b[32m0.21559\u001b[0m\u001b[0m | time: 0.021s\n",
            "| Adam | epoch: 500 | loss: 0.21559 - acc: 0.9941 -- iter: 36/36\n",
            "--\n",
            "Training Step: 2501  | total loss: \u001b[1m\u001b[32m0.21234\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 501 | loss: 0.21234 - acc: 0.9947 -- iter: 08/36\n",
            "Training Step: 2502  | total loss: \u001b[1m\u001b[32m0.21312\u001b[0m\u001b[0m | time: 0.010s\n",
            "| Adam | epoch: 501 | loss: 0.21312 - acc: 0.9952 -- iter: 16/36\n",
            "Training Step: 2503  | total loss: \u001b[1m\u001b[32m0.21373\u001b[0m\u001b[0m | time: 0.014s\n",
            "| Adam | epoch: 501 | loss: 0.21373 - acc: 0.9957 -- iter: 24/36\n",
            "Training Step: 2504  | total loss: \u001b[1m\u001b[32m0.20838\u001b[0m\u001b[0m | time: 0.019s\n",
            "| Adam | epoch: 501 | loss: 0.20838 - acc: 0.9962 -- iter: 32/36\n",
            "Training Step: 2505  | total loss: \u001b[1m\u001b[32m0.19812\u001b[0m\u001b[0m | time: 0.022s\n",
            "| Adam | epoch: 501 | loss: 0.19812 - acc: 0.9965 -- iter: 36/36\n",
            "--\n",
            "Training Step: 2506  | total loss: \u001b[1m\u001b[32m0.19478\u001b[0m\u001b[0m | time: 0.007s\n",
            "| Adam | epoch: 502 | loss: 0.19478 - acc: 0.9969 -- iter: 08/36\n",
            "Training Step: 2507  | total loss: \u001b[1m\u001b[32m0.18907\u001b[0m\u001b[0m | time: 0.009s\n",
            "| Adam | epoch: 502 | loss: 0.18907 - acc: 0.9972 -- iter: 16/36\n",
            "Training Step: 2508  | total loss: \u001b[1m\u001b[32m0.18953\u001b[0m\u001b[0m | time: 0.014s\n",
            "| Adam | epoch: 502 | loss: 0.18953 - acc: 0.9975 -- iter: 24/36\n",
            "Training Step: 2509  | total loss: \u001b[1m\u001b[32m0.18985\u001b[0m\u001b[0m | time: 0.018s\n",
            "| Adam | epoch: 502 | loss: 0.18985 - acc: 0.9977 -- iter: 32/36\n",
            "Training Step: 2510  | total loss: \u001b[1m\u001b[32m0.18632\u001b[0m\u001b[0m | time: 0.022s\n",
            "| Adam | epoch: 502 | loss: 0.18632 - acc: 0.9980 -- iter: 36/36\n",
            "--\n",
            "Training Step: 2511  | total loss: \u001b[1m\u001b[32m0.18033\u001b[0m\u001b[0m | time: 0.004s\n",
            "| Adam | epoch: 503 | loss: 0.18033 - acc: 0.9983 -- iter: 08/36\n",
            "Training Step: 2512  | total loss: \u001b[1m\u001b[32m0.18195\u001b[0m\u001b[0m | time: 0.009s\n",
            "| Adam | epoch: 503 | loss: 0.18195 - acc: 0.9983 -- iter: 16/36\n",
            "Training Step: 2513  | total loss: \u001b[1m\u001b[32m0.18132\u001b[0m\u001b[0m | time: 0.014s\n",
            "| Adam | epoch: 503 | loss: 0.18132 - acc: 0.9987 -- iter: 24/36\n",
            "Training Step: 2514  | total loss: \u001b[1m\u001b[32m0.17619\u001b[0m\u001b[0m | time: 0.020s\n",
            "| Adam | epoch: 503 | loss: 0.17619 - acc: 0.9988 -- iter: 32/36\n",
            "Training Step: 2515  | total loss: \u001b[1m\u001b[32m0.17154\u001b[0m\u001b[0m | time: 0.026s\n",
            "| Adam | epoch: 503 | loss: 0.17154 - acc: 0.9989 -- iter: 36/36\n",
            "--\n",
            "Training Step: 2516  | total loss: \u001b[1m\u001b[32m0.16618\u001b[0m\u001b[0m | time: 0.004s\n",
            "| Adam | epoch: 504 | loss: 0.16618 - acc: 0.9990 -- iter: 08/36\n",
            "Training Step: 2517  | total loss: \u001b[1m\u001b[32m0.16934\u001b[0m\u001b[0m | time: 0.008s\n",
            "| Adam | epoch: 504 | loss: 0.16934 - acc: 0.9991 -- iter: 16/36\n",
            "Training Step: 2518  | total loss: \u001b[1m\u001b[32m0.16669\u001b[0m\u001b[0m | time: 0.012s\n",
            "| Adam | epoch: 504 | loss: 0.16669 - acc: 0.9992 -- iter: 24/36\n",
            "Training Step: 2519  | total loss: \u001b[1m\u001b[32m0.16678\u001b[0m\u001b[0m | time: 0.017s\n",
            "| Adam | epoch: 504 | loss: 0.16678 - acc: 0.9993 -- iter: 32/36\n",
            "Training Step: 2520  | total loss: \u001b[1m\u001b[32m0.17027\u001b[0m\u001b[0m | time: 0.022s\n",
            "| Adam | epoch: 504 | loss: 0.17027 - acc: 0.9994 -- iter: 36/36\n",
            "--\n",
            "Training Step: 2521  | total loss: \u001b[1m\u001b[32m0.17331\u001b[0m\u001b[0m | time: 0.003s\n",
            "| Adam | epoch: 505 | loss: 0.17331 - acc: 0.9994 -- iter: 08/36\n",
            "Training Step: 2522  | total loss: \u001b[1m\u001b[32m0.16629\u001b[0m\u001b[0m | time: 0.007s\n",
            "| Adam | epoch: 505 | loss: 0.16629 - acc: 0.9995 -- iter: 16/36\n",
            "Training Step: 2523  | total loss: \u001b[1m\u001b[32m0.16689\u001b[0m\u001b[0m | time: 0.011s\n",
            "| Adam | epoch: 505 | loss: 0.16689 - acc: 0.9995 -- iter: 24/36\n",
            "Training Step: 2524  | total loss: \u001b[1m\u001b[32m0.16492\u001b[0m\u001b[0m | time: 0.017s\n",
            "| Adam | epoch: 505 | loss: 0.16492 - acc: 0.9996 -- iter: 32/36\n",
            "Training Step: 2525  | total loss: \u001b[1m\u001b[32m0.16770\u001b[0m\u001b[0m | time: 0.022s\n",
            "| Adam | epoch: 505 | loss: 0.16770 - acc: 0.9996 -- iter: 36/36\n",
            "--\n",
            "Training Step: 2526  | total loss: \u001b[1m\u001b[32m0.16303\u001b[0m\u001b[0m | time: 0.007s\n",
            "| Adam | epoch: 506 | loss: 0.16303 - acc: 0.9997 -- iter: 08/36\n",
            "Training Step: 2527  | total loss: \u001b[1m\u001b[32m0.15877\u001b[0m\u001b[0m | time: 0.011s\n",
            "| Adam | epoch: 506 | loss: 0.15877 - acc: 0.9997 -- iter: 16/36\n",
            "Training Step: 2528  | total loss: \u001b[1m\u001b[32m0.15517\u001b[0m\u001b[0m | time: 0.015s\n",
            "| Adam | epoch: 506 | loss: 0.15517 - acc: 0.9997 -- iter: 24/36\n",
            "Training Step: 2529  | total loss: \u001b[1m\u001b[32m0.15431\u001b[0m\u001b[0m | time: 0.021s\n",
            "| Adam | epoch: 506 | loss: 0.15431 - acc: 0.9997 -- iter: 32/36\n",
            "Training Step: 2530  | total loss: \u001b[1m\u001b[32m0.15445\u001b[0m\u001b[0m | time: 0.025s\n",
            "| Adam | epoch: 506 | loss: 0.15445 - acc: 0.9998 -- iter: 36/36\n",
            "--\n",
            "Training Step: 2531  | total loss: \u001b[1m\u001b[32m0.15104\u001b[0m\u001b[0m | time: 0.004s\n",
            "| Adam | epoch: 507 | loss: 0.15104 - acc: 0.9998 -- iter: 08/36\n",
            "Training Step: 2532  | total loss: \u001b[1m\u001b[32m0.15953\u001b[0m\u001b[0m | time: 0.008s\n",
            "| Adam | epoch: 507 | loss: 0.15953 - acc: 0.9998 -- iter: 16/36\n",
            "Training Step: 2533  | total loss: \u001b[1m\u001b[32m0.16707\u001b[0m\u001b[0m | time: 0.012s\n",
            "| Adam | epoch: 507 | loss: 0.16707 - acc: 0.9998 -- iter: 24/36\n",
            "Training Step: 2534  | total loss: \u001b[1m\u001b[32m0.16549\u001b[0m\u001b[0m | time: 0.016s\n",
            "| Adam | epoch: 507 | loss: 0.16549 - acc: 0.9998 -- iter: 32/36\n",
            "Training Step: 2535  | total loss: \u001b[1m\u001b[32m0.16290\u001b[0m\u001b[0m | time: 0.020s\n",
            "| Adam | epoch: 507 | loss: 0.16290 - acc: 0.9999 -- iter: 36/36\n",
            "--\n",
            "Training Step: 2536  | total loss: \u001b[1m\u001b[32m0.16062\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 508 | loss: 0.16062 - acc: 0.9999 -- iter: 08/36\n",
            "Training Step: 2537  | total loss: \u001b[1m\u001b[32m0.16118\u001b[0m\u001b[0m | time: 0.009s\n",
            "| Adam | epoch: 508 | loss: 0.16118 - acc: 0.9999 -- iter: 16/36\n",
            "Training Step: 2538  | total loss: \u001b[1m\u001b[32m0.15988\u001b[0m\u001b[0m | time: 0.014s\n",
            "| Adam | epoch: 508 | loss: 0.15988 - acc: 0.9999 -- iter: 24/36\n",
            "Training Step: 2539  | total loss: \u001b[1m\u001b[32m0.15864\u001b[0m\u001b[0m | time: 0.018s\n",
            "| Adam | epoch: 508 | loss: 0.15864 - acc: 0.9999 -- iter: 32/36\n",
            "Training Step: 2540  | total loss: \u001b[1m\u001b[32m0.15673\u001b[0m\u001b[0m | time: 0.023s\n",
            "| Adam | epoch: 508 | loss: 0.15673 - acc: 0.9999 -- iter: 36/36\n",
            "--\n",
            "Training Step: 2541  | total loss: \u001b[1m\u001b[32m0.15855\u001b[0m\u001b[0m | time: 0.004s\n",
            "| Adam | epoch: 509 | loss: 0.15855 - acc: 0.9999 -- iter: 08/36\n",
            "Training Step: 2542  | total loss: \u001b[1m\u001b[32m0.15308\u001b[0m\u001b[0m | time: 0.008s\n",
            "| Adam | epoch: 509 | loss: 0.15308 - acc: 0.9999 -- iter: 16/36\n",
            "Training Step: 2543  | total loss: \u001b[1m\u001b[32m0.14998\u001b[0m\u001b[0m | time: 0.013s\n",
            "| Adam | epoch: 509 | loss: 0.14998 - acc: 0.9999 -- iter: 24/36\n",
            "Training Step: 2544  | total loss: \u001b[1m\u001b[32m0.14943\u001b[0m\u001b[0m | time: 0.017s\n",
            "| Adam | epoch: 509 | loss: 0.14943 - acc: 0.9999 -- iter: 32/36\n",
            "Training Step: 2545  | total loss: \u001b[1m\u001b[32m0.15110\u001b[0m\u001b[0m | time: 0.022s\n",
            "| Adam | epoch: 509 | loss: 0.15110 - acc: 1.0000 -- iter: 36/36\n",
            "--\n",
            "Training Step: 2546  | total loss: \u001b[1m\u001b[32m0.15215\u001b[0m\u001b[0m | time: 0.004s\n",
            "| Adam | epoch: 510 | loss: 0.15215 - acc: 1.0000 -- iter: 08/36\n",
            "Training Step: 2547  | total loss: \u001b[1m\u001b[32m0.14896\u001b[0m\u001b[0m | time: 0.009s\n",
            "| Adam | epoch: 510 | loss: 0.14896 - acc: 1.0000 -- iter: 16/36\n",
            "Training Step: 2548  | total loss: \u001b[1m\u001b[32m0.14824\u001b[0m\u001b[0m | time: 0.013s\n",
            "| Adam | epoch: 510 | loss: 0.14824 - acc: 1.0000 -- iter: 24/36\n",
            "Training Step: 2549  | total loss: \u001b[1m\u001b[32m0.14434\u001b[0m\u001b[0m | time: 0.020s\n",
            "| Adam | epoch: 510 | loss: 0.14434 - acc: 1.0000 -- iter: 32/36\n",
            "Training Step: 2550  | total loss: \u001b[1m\u001b[32m0.14077\u001b[0m\u001b[0m | time: 0.024s\n",
            "| Adam | epoch: 510 | loss: 0.14077 - acc: 1.0000 -- iter: 36/36\n",
            "--\n",
            "Training Step: 2551  | total loss: \u001b[1m\u001b[32m0.14464\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 511 | loss: 0.14464 - acc: 1.0000 -- iter: 08/36\n",
            "Training Step: 2552  | total loss: \u001b[1m\u001b[32m0.14232\u001b[0m\u001b[0m | time: 0.010s\n",
            "| Adam | epoch: 511 | loss: 0.14232 - acc: 1.0000 -- iter: 16/36\n",
            "Training Step: 2553  | total loss: \u001b[1m\u001b[32m0.14221\u001b[0m\u001b[0m | time: 0.015s\n",
            "| Adam | epoch: 511 | loss: 0.14221 - acc: 1.0000 -- iter: 24/36\n",
            "Training Step: 2554  | total loss: \u001b[1m\u001b[32m0.14316\u001b[0m\u001b[0m | time: 0.020s\n",
            "| Adam | epoch: 511 | loss: 0.14316 - acc: 1.0000 -- iter: 32/36\n",
            "Training Step: 2555  | total loss: \u001b[1m\u001b[32m0.13605\u001b[0m\u001b[0m | time: 0.025s\n",
            "| Adam | epoch: 511 | loss: 0.13605 - acc: 1.0000 -- iter: 36/36\n",
            "--\n",
            "Training Step: 2556  | total loss: \u001b[1m\u001b[32m0.12959\u001b[0m\u001b[0m | time: 0.004s\n",
            "| Adam | epoch: 512 | loss: 0.12959 - acc: 1.0000 -- iter: 08/36\n",
            "Training Step: 2557  | total loss: \u001b[1m\u001b[32m0.12923\u001b[0m\u001b[0m | time: 0.008s\n",
            "| Adam | epoch: 512 | loss: 0.12923 - acc: 1.0000 -- iter: 16/36\n",
            "Training Step: 2558  | total loss: \u001b[1m\u001b[32m0.13114\u001b[0m\u001b[0m | time: 0.013s\n",
            "| Adam | epoch: 512 | loss: 0.13114 - acc: 1.0000 -- iter: 24/36\n",
            "Training Step: 2559  | total loss: \u001b[1m\u001b[32m0.13464\u001b[0m\u001b[0m | time: 0.018s\n",
            "| Adam | epoch: 512 | loss: 0.13464 - acc: 1.0000 -- iter: 32/36\n",
            "Training Step: 2560  | total loss: \u001b[1m\u001b[32m0.13131\u001b[0m\u001b[0m | time: 0.022s\n",
            "| Adam | epoch: 512 | loss: 0.13131 - acc: 1.0000 -- iter: 36/36\n",
            "--\n",
            "Training Step: 2561  | total loss: \u001b[1m\u001b[32m0.12737\u001b[0m\u001b[0m | time: 0.006s\n",
            "| Adam | epoch: 513 | loss: 0.12737 - acc: 1.0000 -- iter: 08/36\n",
            "Training Step: 2562  | total loss: \u001b[1m\u001b[32m0.12370\u001b[0m\u001b[0m | time: 0.009s\n",
            "| Adam | epoch: 513 | loss: 0.12370 - acc: 1.0000 -- iter: 16/36\n",
            "Training Step: 2563  | total loss: \u001b[1m\u001b[32m0.12246\u001b[0m\u001b[0m | time: 0.015s\n",
            "| Adam | epoch: 513 | loss: 0.12246 - acc: 1.0000 -- iter: 24/36\n",
            "Training Step: 2564  | total loss: \u001b[1m\u001b[32m0.13025\u001b[0m\u001b[0m | time: 0.019s\n",
            "| Adam | epoch: 513 | loss: 0.13025 - acc: 1.0000 -- iter: 32/36\n",
            "Training Step: 2565  | total loss: \u001b[1m\u001b[32m0.13025\u001b[0m\u001b[0m | time: 0.025s\n",
            "| Adam | epoch: 513 | loss: 0.13025 - acc: 1.0000 -- iter: 36/36\n",
            "--\n",
            "Training Step: 2566  | total loss: \u001b[1m\u001b[32m0.13325\u001b[0m\u001b[0m | time: 0.004s\n",
            "| Adam | epoch: 514 | loss: 0.13325 - acc: 1.0000 -- iter: 08/36\n",
            "Training Step: 2567  | total loss: \u001b[1m\u001b[32m0.13135\u001b[0m\u001b[0m | time: 0.008s\n",
            "| Adam | epoch: 514 | loss: 0.13135 - acc: 1.0000 -- iter: 16/36\n",
            "Training Step: 2568  | total loss: \u001b[1m\u001b[32m0.13020\u001b[0m\u001b[0m | time: 0.013s\n",
            "| Adam | epoch: 514 | loss: 0.13020 - acc: 1.0000 -- iter: 24/36\n",
            "Training Step: 2569  | total loss: \u001b[1m\u001b[32m0.12909\u001b[0m\u001b[0m | time: 0.018s\n",
            "| Adam | epoch: 514 | loss: 0.12909 - acc: 1.0000 -- iter: 32/36\n",
            "Training Step: 2570  | total loss: \u001b[1m\u001b[32m0.12713\u001b[0m\u001b[0m | time: 0.022s\n",
            "| Adam | epoch: 514 | loss: 0.12713 - acc: 1.0000 -- iter: 36/36\n",
            "--\n",
            "Training Step: 2571  | total loss: \u001b[1m\u001b[32m0.13320\u001b[0m\u001b[0m | time: 0.004s\n",
            "| Adam | epoch: 515 | loss: 0.13320 - acc: 1.0000 -- iter: 08/36\n",
            "Training Step: 2572  | total loss: \u001b[1m\u001b[32m0.13354\u001b[0m\u001b[0m | time: 0.008s\n",
            "| Adam | epoch: 515 | loss: 0.13354 - acc: 1.0000 -- iter: 16/36\n",
            "Training Step: 2573  | total loss: \u001b[1m\u001b[32m0.13116\u001b[0m\u001b[0m | time: 0.013s\n",
            "| Adam | epoch: 515 | loss: 0.13116 - acc: 1.0000 -- iter: 24/36\n",
            "Training Step: 2574  | total loss: \u001b[1m\u001b[32m0.13091\u001b[0m\u001b[0m | time: 0.018s\n",
            "| Adam | epoch: 515 | loss: 0.13091 - acc: 1.0000 -- iter: 32/36\n",
            "Training Step: 2575  | total loss: \u001b[1m\u001b[32m0.13064\u001b[0m\u001b[0m | time: 0.021s\n",
            "| Adam | epoch: 515 | loss: 0.13064 - acc: 1.0000 -- iter: 36/36\n",
            "--\n",
            "Training Step: 2576  | total loss: \u001b[1m\u001b[32m0.13434\u001b[0m\u001b[0m | time: 0.004s\n",
            "| Adam | epoch: 516 | loss: 0.13434 - acc: 1.0000 -- iter: 08/36\n",
            "Training Step: 2577  | total loss: \u001b[1m\u001b[32m0.12952\u001b[0m\u001b[0m | time: 0.009s\n",
            "| Adam | epoch: 516 | loss: 0.12952 - acc: 1.0000 -- iter: 16/36\n",
            "Training Step: 2578  | total loss: \u001b[1m\u001b[32m0.13388\u001b[0m\u001b[0m | time: 0.015s\n",
            "| Adam | epoch: 516 | loss: 0.13388 - acc: 1.0000 -- iter: 24/36\n",
            "Training Step: 2579  | total loss: \u001b[1m\u001b[32m0.13564\u001b[0m\u001b[0m | time: 0.019s\n",
            "| Adam | epoch: 516 | loss: 0.13564 - acc: 1.0000 -- iter: 32/36\n",
            "Training Step: 2580  | total loss: \u001b[1m\u001b[32m0.13065\u001b[0m\u001b[0m | time: 0.024s\n",
            "| Adam | epoch: 516 | loss: 0.13065 - acc: 1.0000 -- iter: 36/36\n",
            "--\n",
            "Training Step: 2581  | total loss: \u001b[1m\u001b[32m0.12611\u001b[0m\u001b[0m | time: 0.004s\n",
            "| Adam | epoch: 517 | loss: 0.12611 - acc: 1.0000 -- iter: 08/36\n",
            "Training Step: 2582  | total loss: \u001b[1m\u001b[32m0.12806\u001b[0m\u001b[0m | time: 0.008s\n",
            "| Adam | epoch: 517 | loss: 0.12806 - acc: 1.0000 -- iter: 16/36\n",
            "Training Step: 2583  | total loss: \u001b[1m\u001b[32m0.12799\u001b[0m\u001b[0m | time: 0.014s\n",
            "| Adam | epoch: 517 | loss: 0.12799 - acc: 1.0000 -- iter: 24/36\n",
            "Training Step: 2584  | total loss: \u001b[1m\u001b[32m0.12765\u001b[0m\u001b[0m | time: 0.019s\n",
            "| Adam | epoch: 517 | loss: 0.12765 - acc: 1.0000 -- iter: 32/36\n",
            "Training Step: 2585  | total loss: \u001b[1m\u001b[32m0.12670\u001b[0m\u001b[0m | time: 0.023s\n",
            "| Adam | epoch: 517 | loss: 0.12670 - acc: 1.0000 -- iter: 36/36\n",
            "--\n",
            "Training Step: 2586  | total loss: \u001b[1m\u001b[32m0.12819\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 518 | loss: 0.12819 - acc: 1.0000 -- iter: 08/36\n",
            "Training Step: 2587  | total loss: \u001b[1m\u001b[32m0.12946\u001b[0m\u001b[0m | time: 0.010s\n",
            "| Adam | epoch: 518 | loss: 0.12946 - acc: 1.0000 -- iter: 16/36\n",
            "Training Step: 2588  | total loss: \u001b[1m\u001b[32m0.13188\u001b[0m\u001b[0m | time: 0.016s\n",
            "| Adam | epoch: 518 | loss: 0.13188 - acc: 1.0000 -- iter: 24/36\n",
            "Training Step: 2589  | total loss: \u001b[1m\u001b[32m0.13125\u001b[0m\u001b[0m | time: 0.020s\n",
            "| Adam | epoch: 518 | loss: 0.13125 - acc: 1.0000 -- iter: 32/36\n",
            "Training Step: 2590  | total loss: \u001b[1m\u001b[32m0.12973\u001b[0m\u001b[0m | time: 0.025s\n",
            "| Adam | epoch: 518 | loss: 0.12973 - acc: 1.0000 -- iter: 36/36\n",
            "--\n",
            "Training Step: 2591  | total loss: \u001b[1m\u001b[32m0.12250\u001b[0m\u001b[0m | time: 0.004s\n",
            "| Adam | epoch: 519 | loss: 0.12250 - acc: 1.0000 -- iter: 08/36\n",
            "Training Step: 2592  | total loss: \u001b[1m\u001b[32m0.12250\u001b[0m\u001b[0m | time: 0.009s\n",
            "| Adam | epoch: 519 | loss: 0.12250 - acc: 1.0000 -- iter: 16/36\n",
            "Training Step: 2593  | total loss: \u001b[1m\u001b[32m0.12258\u001b[0m\u001b[0m | time: 0.014s\n",
            "| Adam | epoch: 519 | loss: 0.12258 - acc: 1.0000 -- iter: 24/36\n",
            "Training Step: 2594  | total loss: \u001b[1m\u001b[32m0.12670\u001b[0m\u001b[0m | time: 0.019s\n",
            "| Adam | epoch: 519 | loss: 0.12670 - acc: 1.0000 -- iter: 32/36\n",
            "Training Step: 2595  | total loss: \u001b[1m\u001b[32m0.12707\u001b[0m\u001b[0m | time: 0.023s\n",
            "| Adam | epoch: 519 | loss: 0.12707 - acc: 1.0000 -- iter: 36/36\n",
            "--\n",
            "Training Step: 2596  | total loss: \u001b[1m\u001b[32m0.13005\u001b[0m\u001b[0m | time: 0.004s\n",
            "| Adam | epoch: 520 | loss: 0.13005 - acc: 1.0000 -- iter: 08/36\n",
            "Training Step: 2597  | total loss: \u001b[1m\u001b[32m0.12557\u001b[0m\u001b[0m | time: 0.009s\n",
            "| Adam | epoch: 520 | loss: 0.12557 - acc: 1.0000 -- iter: 16/36\n",
            "Training Step: 2598  | total loss: \u001b[1m\u001b[32m0.12148\u001b[0m\u001b[0m | time: 0.014s\n",
            "| Adam | epoch: 520 | loss: 0.12148 - acc: 1.0000 -- iter: 24/36\n",
            "Training Step: 2599  | total loss: \u001b[1m\u001b[32m0.12513\u001b[0m\u001b[0m | time: 0.018s\n",
            "| Adam | epoch: 520 | loss: 0.12513 - acc: 1.0000 -- iter: 32/36\n",
            "Training Step: 2600  | total loss: \u001b[1m\u001b[32m0.12486\u001b[0m\u001b[0m | time: 0.023s\n",
            "| Adam | epoch: 520 | loss: 0.12486 - acc: 1.0000 -- iter: 36/36\n",
            "--\n",
            "Training Step: 2601  | total loss: \u001b[1m\u001b[32m0.12147\u001b[0m\u001b[0m | time: 0.004s\n",
            "| Adam | epoch: 521 | loss: 0.12147 - acc: 1.0000 -- iter: 08/36\n",
            "Training Step: 2602  | total loss: \u001b[1m\u001b[32m0.12153\u001b[0m\u001b[0m | time: 0.009s\n",
            "| Adam | epoch: 521 | loss: 0.12153 - acc: 1.0000 -- iter: 16/36\n",
            "Training Step: 2603  | total loss: \u001b[1m\u001b[32m0.12326\u001b[0m\u001b[0m | time: 0.013s\n",
            "| Adam | epoch: 521 | loss: 0.12326 - acc: 1.0000 -- iter: 24/36\n",
            "Training Step: 2604  | total loss: \u001b[1m\u001b[32m0.12478\u001b[0m\u001b[0m | time: 0.018s\n",
            "| Adam | epoch: 521 | loss: 0.12478 - acc: 1.0000 -- iter: 32/36\n",
            "Training Step: 2605  | total loss: \u001b[1m\u001b[32m0.12384\u001b[0m\u001b[0m | time: 0.022s\n",
            "| Adam | epoch: 521 | loss: 0.12384 - acc: 1.0000 -- iter: 36/36\n",
            "--\n",
            "Training Step: 2606  | total loss: \u001b[1m\u001b[32m0.12501\u001b[0m\u001b[0m | time: 0.003s\n",
            "| Adam | epoch: 522 | loss: 0.12501 - acc: 1.0000 -- iter: 08/36\n",
            "Training Step: 2607  | total loss: \u001b[1m\u001b[32m0.12448\u001b[0m\u001b[0m | time: 0.009s\n",
            "| Adam | epoch: 522 | loss: 0.12448 - acc: 1.0000 -- iter: 16/36\n",
            "Training Step: 2608  | total loss: \u001b[1m\u001b[32m0.12814\u001b[0m\u001b[0m | time: 0.016s\n",
            "| Adam | epoch: 522 | loss: 0.12814 - acc: 1.0000 -- iter: 24/36\n",
            "Training Step: 2609  | total loss: \u001b[1m\u001b[32m0.12612\u001b[0m\u001b[0m | time: 0.020s\n",
            "| Adam | epoch: 522 | loss: 0.12612 - acc: 1.0000 -- iter: 32/36\n",
            "Training Step: 2610  | total loss: \u001b[1m\u001b[32m0.12425\u001b[0m\u001b[0m | time: 0.026s\n",
            "| Adam | epoch: 522 | loss: 0.12425 - acc: 1.0000 -- iter: 36/36\n",
            "--\n",
            "Training Step: 2611  | total loss: \u001b[1m\u001b[32m0.12258\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 523 | loss: 0.12258 - acc: 1.0000 -- iter: 08/36\n",
            "Training Step: 2612  | total loss: \u001b[1m\u001b[32m0.11803\u001b[0m\u001b[0m | time: 0.009s\n",
            "| Adam | epoch: 523 | loss: 0.11803 - acc: 1.0000 -- iter: 16/36\n",
            "Training Step: 2613  | total loss: \u001b[1m\u001b[32m0.11803\u001b[0m\u001b[0m | time: 0.013s\n",
            "| Adam | epoch: 523 | loss: 0.11803 - acc: 1.0000 -- iter: 24/36\n",
            "Training Step: 2614  | total loss: \u001b[1m\u001b[32m0.12177\u001b[0m\u001b[0m | time: 0.017s\n",
            "| Adam | epoch: 523 | loss: 0.12177 - acc: 1.0000 -- iter: 32/36\n",
            "Training Step: 2615  | total loss: \u001b[1m\u001b[32m0.12256\u001b[0m\u001b[0m | time: 0.023s\n",
            "| Adam | epoch: 523 | loss: 0.12256 - acc: 1.0000 -- iter: 36/36\n",
            "--\n",
            "Training Step: 2616  | total loss: \u001b[1m\u001b[32m0.12956\u001b[0m\u001b[0m | time: 0.003s\n",
            "| Adam | epoch: 524 | loss: 0.12956 - acc: 1.0000 -- iter: 08/36\n",
            "Training Step: 2617  | total loss: \u001b[1m\u001b[32m0.13581\u001b[0m\u001b[0m | time: 0.007s\n",
            "| Adam | epoch: 524 | loss: 0.13581 - acc: 1.0000 -- iter: 16/36\n",
            "Training Step: 2618  | total loss: \u001b[1m\u001b[32m0.13097\u001b[0m\u001b[0m | time: 0.011s\n",
            "| Adam | epoch: 524 | loss: 0.13097 - acc: 1.0000 -- iter: 24/36\n",
            "Training Step: 2619  | total loss: \u001b[1m\u001b[32m0.13175\u001b[0m\u001b[0m | time: 0.017s\n",
            "| Adam | epoch: 524 | loss: 0.13175 - acc: 1.0000 -- iter: 32/36\n",
            "Training Step: 2620  | total loss: \u001b[1m\u001b[32m0.12807\u001b[0m\u001b[0m | time: 0.022s\n",
            "| Adam | epoch: 524 | loss: 0.12807 - acc: 1.0000 -- iter: 36/36\n",
            "--\n",
            "Training Step: 2621  | total loss: \u001b[1m\u001b[32m0.12879\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 525 | loss: 0.12879 - acc: 1.0000 -- iter: 08/36\n",
            "Training Step: 2622  | total loss: \u001b[1m\u001b[32m0.13194\u001b[0m\u001b[0m | time: 0.009s\n",
            "| Adam | epoch: 525 | loss: 0.13194 - acc: 1.0000 -- iter: 16/36\n",
            "Training Step: 2623  | total loss: \u001b[1m\u001b[32m0.12720\u001b[0m\u001b[0m | time: 0.015s\n",
            "| Adam | epoch: 525 | loss: 0.12720 - acc: 1.0000 -- iter: 24/36\n",
            "Training Step: 2624  | total loss: \u001b[1m\u001b[32m0.12565\u001b[0m\u001b[0m | time: 0.019s\n",
            "| Adam | epoch: 525 | loss: 0.12565 - acc: 1.0000 -- iter: 32/36\n",
            "Training Step: 2625  | total loss: \u001b[1m\u001b[32m0.12938\u001b[0m\u001b[0m | time: 0.024s\n",
            "| Adam | epoch: 525 | loss: 0.12938 - acc: 1.0000 -- iter: 36/36\n",
            "--\n",
            "Training Step: 2626  | total loss: \u001b[1m\u001b[32m0.13239\u001b[0m\u001b[0m | time: 0.004s\n",
            "| Adam | epoch: 526 | loss: 0.13239 - acc: 1.0000 -- iter: 08/36\n",
            "Training Step: 2627  | total loss: \u001b[1m\u001b[32m0.13177\u001b[0m\u001b[0m | time: 0.009s\n",
            "| Adam | epoch: 526 | loss: 0.13177 - acc: 1.0000 -- iter: 16/36\n",
            "Training Step: 2628  | total loss: \u001b[1m\u001b[32m0.13117\u001b[0m\u001b[0m | time: 0.014s\n",
            "| Adam | epoch: 526 | loss: 0.13117 - acc: 1.0000 -- iter: 24/36\n",
            "Training Step: 2629  | total loss: \u001b[1m\u001b[32m0.13160\u001b[0m\u001b[0m | time: 0.018s\n",
            "| Adam | epoch: 526 | loss: 0.13160 - acc: 1.0000 -- iter: 32/36\n",
            "Training Step: 2630  | total loss: \u001b[1m\u001b[32m0.12563\u001b[0m\u001b[0m | time: 0.024s\n",
            "| Adam | epoch: 526 | loss: 0.12563 - acc: 1.0000 -- iter: 36/36\n",
            "--\n",
            "Training Step: 2631  | total loss: \u001b[1m\u001b[32m0.12336\u001b[0m\u001b[0m | time: 0.004s\n",
            "| Adam | epoch: 527 | loss: 0.12336 - acc: 1.0000 -- iter: 08/36\n",
            "Training Step: 2632  | total loss: \u001b[1m\u001b[32m0.12144\u001b[0m\u001b[0m | time: 0.007s\n",
            "| Adam | epoch: 527 | loss: 0.12144 - acc: 1.0000 -- iter: 16/36\n",
            "Training Step: 2633  | total loss: \u001b[1m\u001b[32m0.11968\u001b[0m\u001b[0m | time: 0.013s\n",
            "| Adam | epoch: 527 | loss: 0.11968 - acc: 1.0000 -- iter: 24/36\n",
            "Training Step: 2634  | total loss: \u001b[1m\u001b[32m0.11805\u001b[0m\u001b[0m | time: 0.017s\n",
            "| Adam | epoch: 527 | loss: 0.11805 - acc: 1.0000 -- iter: 32/36\n",
            "Training Step: 2635  | total loss: \u001b[1m\u001b[32m0.11925\u001b[0m\u001b[0m | time: 0.022s\n",
            "| Adam | epoch: 527 | loss: 0.11925 - acc: 1.0000 -- iter: 36/36\n",
            "--\n",
            "Training Step: 2636  | total loss: \u001b[1m\u001b[32m0.12067\u001b[0m\u001b[0m | time: 0.004s\n",
            "| Adam | epoch: 528 | loss: 0.12067 - acc: 1.0000 -- iter: 08/36\n",
            "Training Step: 2637  | total loss: \u001b[1m\u001b[32m0.11912\u001b[0m\u001b[0m | time: 0.009s\n",
            "| Adam | epoch: 528 | loss: 0.11912 - acc: 1.0000 -- iter: 16/36\n",
            "Training Step: 2638  | total loss: \u001b[1m\u001b[32m0.11833\u001b[0m\u001b[0m | time: 0.015s\n",
            "| Adam | epoch: 528 | loss: 0.11833 - acc: 1.0000 -- iter: 24/36\n",
            "Training Step: 2639  | total loss: \u001b[1m\u001b[32m0.11775\u001b[0m\u001b[0m | time: 0.018s\n",
            "| Adam | epoch: 528 | loss: 0.11775 - acc: 1.0000 -- iter: 32/36\n",
            "Training Step: 2640  | total loss: \u001b[1m\u001b[32m0.11718\u001b[0m\u001b[0m | time: 0.023s\n",
            "| Adam | epoch: 528 | loss: 0.11718 - acc: 1.0000 -- iter: 36/36\n",
            "--\n",
            "Training Step: 2641  | total loss: \u001b[1m\u001b[32m0.11798\u001b[0m\u001b[0m | time: 0.003s\n",
            "| Adam | epoch: 529 | loss: 0.11798 - acc: 1.0000 -- iter: 08/36\n",
            "Training Step: 2642  | total loss: \u001b[1m\u001b[32m0.11681\u001b[0m\u001b[0m | time: 0.009s\n",
            "| Adam | epoch: 529 | loss: 0.11681 - acc: 1.0000 -- iter: 16/36\n",
            "Training Step: 2643  | total loss: \u001b[1m\u001b[32m0.11701\u001b[0m\u001b[0m | time: 0.013s\n",
            "| Adam | epoch: 529 | loss: 0.11701 - acc: 1.0000 -- iter: 24/36\n",
            "Training Step: 2644  | total loss: \u001b[1m\u001b[32m0.11836\u001b[0m\u001b[0m | time: 0.018s\n",
            "| Adam | epoch: 529 | loss: 0.11836 - acc: 1.0000 -- iter: 32/36\n",
            "Training Step: 2645  | total loss: \u001b[1m\u001b[32m0.11340\u001b[0m\u001b[0m | time: 0.023s\n",
            "| Adam | epoch: 529 | loss: 0.11340 - acc: 1.0000 -- iter: 36/36\n",
            "--\n",
            "Training Step: 2646  | total loss: \u001b[1m\u001b[32m0.10890\u001b[0m\u001b[0m | time: 0.004s\n",
            "| Adam | epoch: 530 | loss: 0.10890 - acc: 1.0000 -- iter: 08/36\n",
            "Training Step: 2647  | total loss: \u001b[1m\u001b[32m0.11112\u001b[0m\u001b[0m | time: 0.008s\n",
            "| Adam | epoch: 530 | loss: 0.11112 - acc: 1.0000 -- iter: 16/36\n",
            "Training Step: 2648  | total loss: \u001b[1m\u001b[32m0.11374\u001b[0m\u001b[0m | time: 0.012s\n",
            "| Adam | epoch: 530 | loss: 0.11374 - acc: 1.0000 -- iter: 24/36\n",
            "Training Step: 2649  | total loss: \u001b[1m\u001b[32m0.98979\u001b[0m\u001b[0m | time: 0.017s\n",
            "| Adam | epoch: 530 | loss: 0.98979 - acc: 1.0000 -- iter: 32/36\n",
            "Training Step: 2650  | total loss: \u001b[1m\u001b[32m0.89958\u001b[0m\u001b[0m | time: 0.021s\n",
            "| Adam | epoch: 530 | loss: 0.89958 - acc: 0.9125 -- iter: 36/36\n",
            "--\n",
            "Training Step: 2651  | total loss: \u001b[1m\u001b[32m0.82590\u001b[0m\u001b[0m | time: 0.004s\n",
            "| Adam | epoch: 531 | loss: 0.82590 - acc: 0.9212 -- iter: 08/36\n",
            "Training Step: 2652  | total loss: \u001b[1m\u001b[32m0.75957\u001b[0m\u001b[0m | time: 0.010s\n",
            "| Adam | epoch: 531 | loss: 0.75957 - acc: 0.9291 -- iter: 16/36\n",
            "Training Step: 2653  | total loss: \u001b[1m\u001b[32m0.69904\u001b[0m\u001b[0m | time: 0.014s\n",
            "| Adam | epoch: 531 | loss: 0.69904 - acc: 0.9426 -- iter: 24/36\n",
            "Training Step: 2654  | total loss: \u001b[1m\u001b[32m0.63949\u001b[0m\u001b[0m | time: 0.018s\n",
            "| Adam | epoch: 531 | loss: 0.63949 - acc: 0.9483 -- iter: 32/36\n",
            "Training Step: 2655  | total loss: \u001b[1m\u001b[32m0.58387\u001b[0m\u001b[0m | time: 0.024s\n",
            "| Adam | epoch: 531 | loss: 0.58387 - acc: 0.9535 -- iter: 36/36\n",
            "--\n",
            "Training Step: 2656  | total loss: \u001b[1m\u001b[32m0.53972\u001b[0m\u001b[0m | time: 0.004s\n",
            "| Adam | epoch: 532 | loss: 0.53972 - acc: 0.9581 -- iter: 08/36\n",
            "Training Step: 2657  | total loss: \u001b[1m\u001b[32m0.49711\u001b[0m\u001b[0m | time: 0.010s\n",
            "| Adam | epoch: 532 | loss: 0.49711 - acc: 0.9623 -- iter: 16/36\n",
            "Training Step: 2658  | total loss: \u001b[1m\u001b[32m0.45874\u001b[0m\u001b[0m | time: 0.015s\n",
            "| Adam | epoch: 532 | loss: 0.45874 - acc: 0.9661 -- iter: 24/36\n",
            "Training Step: 2659  | total loss: \u001b[1m\u001b[32m0.42312\u001b[0m\u001b[0m | time: 0.019s\n",
            "| Adam | epoch: 532 | loss: 0.42312 - acc: 0.9695 -- iter: 32/36\n",
            "Training Step: 2660  | total loss: \u001b[1m\u001b[32m0.39036\u001b[0m\u001b[0m | time: 0.023s\n",
            "| Adam | epoch: 532 | loss: 0.39036 - acc: 0.9695 -- iter: 36/36\n",
            "--\n",
            "Training Step: 2661  | total loss: \u001b[1m\u001b[32m0.36289\u001b[0m\u001b[0m | time: 0.004s\n",
            "| Adam | epoch: 533 | loss: 0.36289 - acc: 0.9725 -- iter: 08/36\n",
            "Training Step: 2662  | total loss: \u001b[1m\u001b[32m0.34089\u001b[0m\u001b[0m | time: 0.010s\n",
            "| Adam | epoch: 533 | loss: 0.34089 - acc: 0.9753 -- iter: 16/36\n",
            "Training Step: 2663  | total loss: \u001b[1m\u001b[32m0.31617\u001b[0m\u001b[0m | time: 0.014s\n",
            "| Adam | epoch: 533 | loss: 0.31617 - acc: 0.9778 -- iter: 24/36\n",
            "Training Step: 2664  | total loss: \u001b[1m\u001b[32m0.29391\u001b[0m\u001b[0m | time: 0.019s\n",
            "| Adam | epoch: 533 | loss: 0.29391 - acc: 0.9800 -- iter: 32/36\n",
            "Training Step: 2665  | total loss: \u001b[1m\u001b[32m0.27602\u001b[0m\u001b[0m | time: 0.026s\n",
            "| Adam | epoch: 533 | loss: 0.27602 - acc: 0.9820 -- iter: 36/36\n",
            "--\n",
            "Training Step: 2666  | total loss: \u001b[1m\u001b[32m0.25821\u001b[0m\u001b[0m | time: 0.004s\n",
            "| Adam | epoch: 534 | loss: 0.25821 - acc: 0.9838 -- iter: 08/36\n",
            "Training Step: 2667  | total loss: \u001b[1m\u001b[32m0.24335\u001b[0m\u001b[0m | time: 0.010s\n",
            "| Adam | epoch: 534 | loss: 0.24335 - acc: 0.9854 -- iter: 16/36\n",
            "Training Step: 2668  | total loss: \u001b[1m\u001b[32m0.23129\u001b[0m\u001b[0m | time: 0.014s\n",
            "| Adam | epoch: 534 | loss: 0.23129 - acc: 0.9869 -- iter: 24/36\n",
            "Training Step: 2669  | total loss: \u001b[1m\u001b[32m0.21806\u001b[0m\u001b[0m | time: 0.019s\n",
            "| Adam | epoch: 534 | loss: 0.21806 - acc: 0.9882 -- iter: 32/36\n",
            "Training Step: 2670  | total loss: \u001b[1m\u001b[32m0.20613\u001b[0m\u001b[0m | time: 0.024s\n",
            "| Adam | epoch: 534 | loss: 0.20613 - acc: 0.9904 -- iter: 36/36\n",
            "--\n",
            "Training Step: 2671  | total loss: \u001b[1m\u001b[32m0.19824\u001b[0m\u001b[0m | time: 0.006s\n",
            "| Adam | epoch: 535 | loss: 0.19824 - acc: 0.9904 -- iter: 08/36\n",
            "Training Step: 2672  | total loss: \u001b[1m\u001b[32m0.18755\u001b[0m\u001b[0m | time: 0.010s\n",
            "| Adam | epoch: 535 | loss: 0.18755 - acc: 0.9914 -- iter: 16/36\n",
            "Training Step: 2673  | total loss: \u001b[1m\u001b[32m0.18050\u001b[0m\u001b[0m | time: 0.019s\n",
            "| Adam | epoch: 535 | loss: 0.18050 - acc: 0.9922 -- iter: 24/36\n",
            "Training Step: 2674  | total loss: \u001b[1m\u001b[32m0.17355\u001b[0m\u001b[0m | time: 0.024s\n",
            "| Adam | epoch: 535 | loss: 0.17355 - acc: 0.9930 -- iter: 32/36\n",
            "Training Step: 2675  | total loss: \u001b[1m\u001b[32m0.16474\u001b[0m\u001b[0m | time: 0.027s\n",
            "| Adam | epoch: 535 | loss: 0.16474 - acc: 0.9937 -- iter: 36/36\n",
            "--\n",
            "Training Step: 2676  | total loss: \u001b[1m\u001b[32m0.15676\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 536 | loss: 0.15676 - acc: 0.9943 -- iter: 08/36\n",
            "Training Step: 2677  | total loss: \u001b[1m\u001b[32m0.15383\u001b[0m\u001b[0m | time: 0.009s\n",
            "| Adam | epoch: 536 | loss: 0.15383 - acc: 0.9949 -- iter: 16/36\n",
            "Training Step: 2678  | total loss: \u001b[1m\u001b[32m0.14649\u001b[0m\u001b[0m | time: 0.016s\n",
            "| Adam | epoch: 536 | loss: 0.14649 - acc: 0.9954 -- iter: 24/36\n",
            "Training Step: 2679  | total loss: \u001b[1m\u001b[32m0.14604\u001b[0m\u001b[0m | time: 0.022s\n",
            "| Adam | epoch: 536 | loss: 0.14604 - acc: 0.9959 -- iter: 32/36\n",
            "Training Step: 2680  | total loss: \u001b[1m\u001b[32m0.14107\u001b[0m\u001b[0m | time: 0.025s\n",
            "| Adam | epoch: 536 | loss: 0.14107 - acc: 0.9963 -- iter: 36/36\n",
            "--\n",
            "Training Step: 2681  | total loss: \u001b[1m\u001b[32m0.13588\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 537 | loss: 0.13588 - acc: 0.9967 -- iter: 08/36\n",
            "Training Step: 2682  | total loss: \u001b[1m\u001b[32m0.13118\u001b[0m\u001b[0m | time: 0.010s\n",
            "| Adam | epoch: 537 | loss: 0.13118 - acc: 0.9970 -- iter: 16/36\n",
            "Training Step: 2683  | total loss: \u001b[1m\u001b[32m0.13315\u001b[0m\u001b[0m | time: 0.015s\n",
            "| Adam | epoch: 537 | loss: 0.13315 - acc: 0.9973 -- iter: 24/36\n",
            "Training Step: 2684  | total loss: \u001b[1m\u001b[32m0.12962\u001b[0m\u001b[0m | time: 0.021s\n",
            "| Adam | epoch: 537 | loss: 0.12962 - acc: 0.9976 -- iter: 32/36\n",
            "Training Step: 2685  | total loss: \u001b[1m\u001b[32m0.12734\u001b[0m\u001b[0m | time: 0.026s\n",
            "| Adam | epoch: 537 | loss: 0.12734 - acc: 0.9980 -- iter: 36/36\n",
            "--\n",
            "Training Step: 2686  | total loss: \u001b[1m\u001b[32m0.12920\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 538 | loss: 0.12920 - acc: 0.9982 -- iter: 08/36\n",
            "Training Step: 2687  | total loss: \u001b[1m\u001b[32m0.12569\u001b[0m\u001b[0m | time: 0.010s\n",
            "| Adam | epoch: 538 | loss: 0.12569 - acc: 0.9984 -- iter: 16/36\n",
            "Training Step: 2688  | total loss: \u001b[1m\u001b[32m0.12250\u001b[0m\u001b[0m | time: 0.015s\n",
            "| Adam | epoch: 538 | loss: 0.12250 - acc: 0.9986 -- iter: 24/36\n",
            "Training Step: 2689  | total loss: \u001b[1m\u001b[32m0.11845\u001b[0m\u001b[0m | time: 0.019s\n",
            "| Adam | epoch: 538 | loss: 0.11845 - acc: 0.9987 -- iter: 32/36\n",
            "Training Step: 2690  | total loss: \u001b[1m\u001b[32m0.11845\u001b[0m\u001b[0m | time: 0.023s\n",
            "| Adam | epoch: 538 | loss: 0.11845 - acc: 0.9988 -- iter: 36/36\n",
            "--\n",
            "Training Step: 2691  | total loss: \u001b[1m\u001b[32m0.11734\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 539 | loss: 0.11734 - acc: 0.9990 -- iter: 08/36\n",
            "Training Step: 2692  | total loss: \u001b[1m\u001b[32m0.11641\u001b[0m\u001b[0m | time: 0.011s\n",
            "| Adam | epoch: 539 | loss: 0.11641 - acc: 0.9991 -- iter: 16/36\n",
            "Training Step: 2693  | total loss: \u001b[1m\u001b[32m0.11561\u001b[0m\u001b[0m | time: 0.015s\n",
            "| Adam | epoch: 539 | loss: 0.11561 - acc: 0.9992 -- iter: 24/36\n",
            "Training Step: 2694  | total loss: \u001b[1m\u001b[32m0.11050\u001b[0m\u001b[0m | time: 0.018s\n",
            "| Adam | epoch: 539 | loss: 0.11050 - acc: 0.9992 -- iter: 32/36\n",
            "Training Step: 2695  | total loss: \u001b[1m\u001b[32m0.10585\u001b[0m\u001b[0m | time: 0.023s\n",
            "| Adam | epoch: 539 | loss: 0.10585 - acc: 0.9992 -- iter: 36/36\n",
            "--\n",
            "Training Step: 2696  | total loss: \u001b[1m\u001b[32m0.10363\u001b[0m\u001b[0m | time: 0.004s\n",
            "| Adam | epoch: 540 | loss: 0.10363 - acc: 0.9993 -- iter: 08/36\n",
            "Training Step: 2697  | total loss: \u001b[1m\u001b[32m0.10691\u001b[0m\u001b[0m | time: 0.009s\n",
            "| Adam | epoch: 540 | loss: 0.10691 - acc: 0.9994 -- iter: 16/36\n",
            "Training Step: 2698  | total loss: \u001b[1m\u001b[32m0.10859\u001b[0m\u001b[0m | time: 0.012s\n",
            "| Adam | epoch: 540 | loss: 0.10859 - acc: 0.9994 -- iter: 24/36\n",
            "Training Step: 2699  | total loss: \u001b[1m\u001b[32m0.11158\u001b[0m\u001b[0m | time: 0.017s\n",
            "| Adam | epoch: 540 | loss: 0.11158 - acc: 0.9995 -- iter: 32/36\n",
            "Training Step: 2700  | total loss: \u001b[1m\u001b[32m0.11459\u001b[0m\u001b[0m | time: 0.023s\n",
            "| Adam | epoch: 540 | loss: 0.11459 - acc: 0.9995 -- iter: 36/36\n",
            "--\n",
            "Training Step: 2701  | total loss: \u001b[1m\u001b[32m0.11727\u001b[0m\u001b[0m | time: 0.004s\n",
            "| Adam | epoch: 541 | loss: 0.11727 - acc: 0.9996 -- iter: 08/36\n",
            "Training Step: 2702  | total loss: \u001b[1m\u001b[32m0.11321\u001b[0m\u001b[0m | time: 0.008s\n",
            "| Adam | epoch: 541 | loss: 0.11321 - acc: 0.9996 -- iter: 16/36\n",
            "Training Step: 2703  | total loss: \u001b[1m\u001b[32m0.11270\u001b[0m\u001b[0m | time: 0.016s\n",
            "| Adam | epoch: 541 | loss: 0.11270 - acc: 0.9997 -- iter: 24/36\n",
            "Training Step: 2704  | total loss: \u001b[1m\u001b[32m0.10979\u001b[0m\u001b[0m | time: 0.022s\n",
            "| Adam | epoch: 541 | loss: 0.10979 - acc: 0.9997 -- iter: 32/36\n",
            "Training Step: 2705  | total loss: \u001b[1m\u001b[32m0.11227\u001b[0m\u001b[0m | time: 0.026s\n",
            "| Adam | epoch: 541 | loss: 0.11227 - acc: 0.9998 -- iter: 36/36\n",
            "--\n",
            "Training Step: 2706  | total loss: \u001b[1m\u001b[32m0.10885\u001b[0m\u001b[0m | time: 0.004s\n",
            "| Adam | epoch: 542 | loss: 0.10885 - acc: 0.9998 -- iter: 08/36\n",
            "Training Step: 2707  | total loss: \u001b[1m\u001b[32m0.10571\u001b[0m\u001b[0m | time: 0.009s\n",
            "| Adam | epoch: 542 | loss: 0.10571 - acc: 0.9998 -- iter: 16/36\n",
            "Training Step: 2708  | total loss: \u001b[1m\u001b[32m0.10808\u001b[0m\u001b[0m | time: 0.013s\n",
            "| Adam | epoch: 542 | loss: 0.10808 - acc: 0.9998 -- iter: 24/36\n",
            "Training Step: 2709  | total loss: \u001b[1m\u001b[32m0.10334\u001b[0m\u001b[0m | time: 0.017s\n",
            "| Adam | epoch: 542 | loss: 0.10334 - acc: 0.9998 -- iter: 32/36\n",
            "Training Step: 2710  | total loss: \u001b[1m\u001b[32m0.10314\u001b[0m\u001b[0m | time: 0.022s\n",
            "| Adam | epoch: 542 | loss: 0.10314 - acc: 0.9998 -- iter: 36/36\n",
            "--\n",
            "Training Step: 2711  | total loss: \u001b[1m\u001b[32m0.10362\u001b[0m\u001b[0m | time: 0.004s\n",
            "| Adam | epoch: 543 | loss: 0.10362 - acc: 0.9999 -- iter: 08/36\n",
            "Training Step: 2712  | total loss: \u001b[1m\u001b[32m0.10402\u001b[0m\u001b[0m | time: 0.010s\n",
            "| Adam | epoch: 543 | loss: 0.10402 - acc: 0.9999 -- iter: 16/36\n",
            "Training Step: 3066  | total loss: \u001b[1m\u001b[32m1.01521\u001b[0m\u001b[0m | time: 0.006s\n",
            "| Adam | epoch: 614 | loss: 1.01521 - acc: 0.8823 -- iter: 08/36\n",
            "Training Step: 3067  | total loss: \u001b[1m\u001b[32m0.93073\u001b[0m\u001b[0m | time: 0.010s\n",
            "| Adam | epoch: 614 | loss: 0.93073 - acc: 0.8941 -- iter: 16/36\n",
            "Training Step: 3068  | total loss: \u001b[1m\u001b[32m0.85575\u001b[0m\u001b[0m | time: 0.014s\n",
            "| Adam | epoch: 614 | loss: 0.85575 - acc: 0.9046 -- iter: 24/36\n",
            "Training Step: 3069  | total loss: \u001b[1m\u001b[32m1.12242\u001b[0m\u001b[0m | time: 0.019s\n",
            "| Adam | epoch: 614 | loss: 1.12242 - acc: 0.8517 -- iter: 32/36\n",
            "Training Step: 3070  | total loss: \u001b[1m\u001b[32m1.02482\u001b[0m\u001b[0m | time: 0.023s\n",
            "| Adam | epoch: 614 | loss: 1.02482 - acc: 0.8665 -- iter: 36/36\n",
            "--\n",
            "Training Step: 3071  | total loss: \u001b[1m\u001b[32m1.02482\u001b[0m\u001b[0m | time: 0.003s\n",
            "| Adam | epoch: 615 | loss: 1.02482 - acc: 0.8665 -- iter: 08/36\n",
            "Training Step: 3072  | total loss: \u001b[1m\u001b[32m0.87110\u001b[0m\u001b[0m | time: 0.009s\n",
            "| Adam | epoch: 615 | loss: 0.87110 - acc: 0.8919 -- iter: 16/36\n",
            "Training Step: 3073  | total loss: \u001b[1m\u001b[32m0.80188\u001b[0m\u001b[0m | time: 0.013s\n",
            "| Adam | epoch: 615 | loss: 0.80188 - acc: 0.9027 -- iter: 24/36\n",
            "Training Step: 3074  | total loss: \u001b[1m\u001b[32m0.74273\u001b[0m\u001b[0m | time: 0.017s\n",
            "| Adam | epoch: 615 | loss: 0.74273 - acc: 0.9027 -- iter: 32/36\n",
            "Training Step: 3075  | total loss: \u001b[1m\u001b[32m1.26123\u001b[0m\u001b[0m | time: 0.023s\n",
            "| Adam | epoch: 615 | loss: 1.26123 - acc: 0.9124 -- iter: 36/36\n",
            "--\n",
            "Training Step: 3076  | total loss: \u001b[1m\u001b[32m1.15578\u001b[0m\u001b[0m | time: 0.004s\n",
            "| Adam | epoch: 616 | loss: 1.15578 - acc: 0.8462 -- iter: 08/36\n",
            "Training Step: 3077  | total loss: \u001b[1m\u001b[32m1.05617\u001b[0m\u001b[0m | time: 0.009s\n",
            "| Adam | epoch: 616 | loss: 1.05617 - acc: 0.8616 -- iter: 16/36\n",
            "Training Step: 3078  | total loss: \u001b[1m\u001b[32m0.96648\u001b[0m\u001b[0m | time: 0.013s\n",
            "| Adam | epoch: 616 | loss: 0.96648 - acc: 0.8754 -- iter: 24/36\n",
            "Training Step: 3079  | total loss: \u001b[1m\u001b[32m0.89066\u001b[0m\u001b[0m | time: 0.018s\n",
            "| Adam | epoch: 616 | loss: 0.89066 - acc: 0.8879 -- iter: 32/36\n",
            "Training Step: 3080  | total loss: \u001b[1m\u001b[32m0.81928\u001b[0m\u001b[0m | time: 0.024s\n",
            "| Adam | epoch: 616 | loss: 0.81928 - acc: 0.8991 -- iter: 36/36\n",
            "--\n",
            "Training Step: 3081  | total loss: \u001b[1m\u001b[32m1.39451\u001b[0m\u001b[0m | time: 0.004s\n",
            "| Adam | epoch: 617 | loss: 1.39451 - acc: 0.9092 -- iter: 08/36\n",
            "Training Step: 3082  | total loss: \u001b[1m\u001b[32m1.27768\u001b[0m\u001b[0m | time: 0.010s\n",
            "| Adam | epoch: 617 | loss: 1.27768 - acc: 0.8433 -- iter: 16/36\n",
            "Training Step: 3083  | total loss: \u001b[1m\u001b[32m1.17014\u001b[0m\u001b[0m | time: 0.014s\n",
            "| Adam | epoch: 617 | loss: 1.17014 - acc: 0.8730 -- iter: 24/36\n",
            "Training Step: 3084  | total loss: \u001b[1m\u001b[32m1.17014\u001b[0m\u001b[0m | time: 0.020s\n",
            "| Adam | epoch: 617 | loss: 1.17014 - acc: 0.8730 -- iter: 32/36\n",
            "Training Step: 3085  | total loss: \u001b[1m\u001b[32m1.07317\u001b[0m\u001b[0m | time: 0.026s\n",
            "| Adam | epoch: 617 | loss: 1.07317 - acc: 0.8857 -- iter: 36/36\n",
            "--\n",
            "Training Step: 3086  | total loss: \u001b[1m\u001b[32m0.90537\u001b[0m\u001b[0m | time: 0.004s\n",
            "| Adam | epoch: 618 | loss: 0.90537 - acc: 0.9074 -- iter: 08/36\n",
            "Training Step: 3087  | total loss: \u001b[1m\u001b[32m1.35845\u001b[0m\u001b[0m | time: 0.007s\n",
            "| Adam | epoch: 618 | loss: 1.35845 - acc: 0.8417 -- iter: 16/36\n",
            "Training Step: 3088  | total loss: \u001b[1m\u001b[32m1.24253\u001b[0m\u001b[0m | time: 0.010s\n",
            "| Adam | epoch: 618 | loss: 1.24253 - acc: 0.8575 -- iter: 24/36\n",
            "Training Step: 3089  | total loss: \u001b[1m\u001b[32m1.13877\u001b[0m\u001b[0m | time: 0.013s\n",
            "| Adam | epoch: 618 | loss: 1.13877 - acc: 0.8718 -- iter: 32/36\n",
            "Training Step: 3090  | total loss: \u001b[1m\u001b[32m1.04535\u001b[0m\u001b[0m | time: 0.020s\n",
            "| Adam | epoch: 618 | loss: 1.04535 - acc: 0.8846 -- iter: 36/36\n",
            "--\n",
            "Training Step: 3091  | total loss: \u001b[1m\u001b[32m0.96229\u001b[0m\u001b[0m | time: 0.003s\n",
            "| Adam | epoch: 619 | loss: 0.96229 - acc: 0.8961 -- iter: 08/36\n",
            "Training Step: 3092  | total loss: \u001b[1m\u001b[32m0.89124\u001b[0m\u001b[0m | time: 0.009s\n",
            "| Adam | epoch: 619 | loss: 0.89124 - acc: 0.9065 -- iter: 16/36\n",
            "Training Step: 3093  | total loss: \u001b[1m\u001b[32m0.81625\u001b[0m\u001b[0m | time: 0.015s\n",
            "| Adam | epoch: 619 | loss: 0.81625 - acc: 0.9159 -- iter: 24/36\n",
            "Training Step: 3094  | total loss: \u001b[1m\u001b[32m0.75307\u001b[0m\u001b[0m | time: 0.019s\n",
            "| Adam | epoch: 619 | loss: 0.75307 - acc: 0.9243 -- iter: 32/36\n",
            "Training Step: 3095  | total loss: \u001b[1m\u001b[32m0.70085\u001b[0m\u001b[0m | time: 0.026s\n",
            "| Adam | epoch: 619 | loss: 0.70085 - acc: 0.9319 -- iter: 36/36\n",
            "--\n",
            "Training Step: 3096  | total loss: \u001b[1m\u001b[32m0.65383\u001b[0m\u001b[0m | time: 0.004s\n",
            "| Adam | epoch: 620 | loss: 0.65383 - acc: 0.9387 -- iter: 08/36\n",
            "Training Step: 3097  | total loss: \u001b[1m\u001b[32m0.61217\u001b[0m\u001b[0m | time: 0.010s\n",
            "| Adam | epoch: 620 | loss: 0.61217 - acc: 0.9448 -- iter: 16/36\n",
            "Training Step: 3098  | total loss: \u001b[1m\u001b[32m0.56897\u001b[0m\u001b[0m | time: 0.016s\n",
            "| Adam | epoch: 620 | loss: 0.56897 - acc: 0.9503 -- iter: 24/36\n",
            "Training Step: 3099  | total loss: \u001b[1m\u001b[32m1.06934\u001b[0m\u001b[0m | time: 0.023s\n",
            "| Adam | epoch: 620 | loss: 1.06934 - acc: 0.8803 -- iter: 32/36\n",
            "Training Step: 3100  | total loss: \u001b[1m\u001b[32m0.98344\u001b[0m\u001b[0m | time: 0.028s\n",
            "| Adam | epoch: 620 | loss: 0.98344 - acc: 0.8923 -- iter: 36/36\n",
            "--\n",
            "Training Step: 3101  | total loss: \u001b[1m\u001b[32m0.90237\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 621 | loss: 0.90237 - acc: 0.9030 -- iter: 08/36\n",
            "Training Step: 3102  | total loss: \u001b[1m\u001b[32m0.82941\u001b[0m\u001b[0m | time: 0.010s\n",
            "| Adam | epoch: 621 | loss: 0.82941 - acc: 0.9127 -- iter: 16/36\n",
            "Training Step: 3103  | total loss: \u001b[1m\u001b[32m0.77065\u001b[0m\u001b[0m | time: 0.014s\n",
            "| Adam | epoch: 621 | loss: 0.77065 - acc: 0.9215 -- iter: 24/36\n",
            "Training Step: 3104  | total loss: \u001b[1m\u001b[32m0.71066\u001b[0m\u001b[0m | time: 0.019s\n",
            "| Adam | epoch: 621 | loss: 0.71066 - acc: 0.9293 -- iter: 32/36\n",
            "Training Step: 3105  | total loss: \u001b[1m\u001b[32m0.65954\u001b[0m\u001b[0m | time: 0.022s\n",
            "| Adam | epoch: 621 | loss: 0.65954 - acc: 0.9293 -- iter: 36/36\n",
            "--\n",
            "Training Step: 3106  | total loss: \u001b[1m\u001b[32m0.61275\u001b[0m\u001b[0m | time: 0.007s\n",
            "| Adam | epoch: 622 | loss: 0.61275 - acc: 0.9364 -- iter: 08/36\n",
            "Training Step: 3107  | total loss: \u001b[1m\u001b[32m0.57072\u001b[0m\u001b[0m | time: 0.011s\n",
            "| Adam | epoch: 622 | loss: 0.57072 - acc: 0.9427 -- iter: 16/36\n",
            "Training Step: 3108  | total loss: \u001b[1m\u001b[32m0.53284\u001b[0m\u001b[0m | time: 0.015s\n",
            "| Adam | epoch: 622 | loss: 0.53284 - acc: 0.9485 -- iter: 24/36\n",
            "Training Step: 3109  | total loss: \u001b[1m\u001b[32m0.50177\u001b[0m\u001b[0m | time: 0.020s\n",
            "| Adam | epoch: 622 | loss: 0.50177 - acc: 0.9536 -- iter: 32/36\n",
            "Training Step: 3110  | total loss: \u001b[1m\u001b[32m0.47168\u001b[0m\u001b[0m | time: 0.024s\n",
            "| Adam | epoch: 622 | loss: 0.47168 - acc: 0.9583 -- iter: 36/36\n",
            "--\n",
            "Training Step: 3111  | total loss: \u001b[1m\u001b[32m1.13399\u001b[0m\u001b[0m | time: 0.006s\n",
            "| Adam | epoch: 623 | loss: 1.13399 - acc: 0.9624 -- iter: 08/36\n",
            "Training Step: 3112  | total loss: \u001b[1m\u001b[32m1.03855\u001b[0m\u001b[0m | time: 0.010s\n",
            "| Adam | epoch: 623 | loss: 1.03855 - acc: 0.8662 -- iter: 16/36\n",
            "Training Step: 3113  | total loss: \u001b[1m\u001b[32m0.95825\u001b[0m\u001b[0m | time: 0.015s\n",
            "| Adam | epoch: 623 | loss: 0.95825 - acc: 0.8796 -- iter: 24/36\n",
            "Training Step: 3114  | total loss: \u001b[1m\u001b[32m0.88598\u001b[0m\u001b[0m | time: 0.019s\n",
            "| Adam | epoch: 623 | loss: 0.88598 - acc: 0.9025 -- iter: 32/36\n",
            "Training Step: 3115  | total loss: \u001b[1m\u001b[32m0.81653\u001b[0m\u001b[0m | time: 0.023s\n",
            "| Adam | epoch: 623 | loss: 0.81653 - acc: 0.9025 -- iter: 36/36\n",
            "--\n",
            "Training Step: 3116  | total loss: \u001b[1m\u001b[32m0.75466\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 624 | loss: 0.75466 - acc: 0.9122 -- iter: 08/36\n",
            "Training Step: 3117  | total loss: \u001b[1m\u001b[32m0.70074\u001b[0m\u001b[0m | time: 0.009s\n",
            "| Adam | epoch: 624 | loss: 0.70074 - acc: 0.9210 -- iter: 16/36\n",
            "Training Step: 3118  | total loss: \u001b[1m\u001b[32m0.64946\u001b[0m\u001b[0m | time: 0.014s\n",
            "| Adam | epoch: 624 | loss: 0.64946 - acc: 0.9289 -- iter: 24/36\n",
            "Training Step: 3119  | total loss: \u001b[1m\u001b[32m0.60901\u001b[0m\u001b[0m | time: 0.019s\n",
            "| Adam | epoch: 624 | loss: 0.60901 - acc: 0.9360 -- iter: 32/36\n",
            "Training Step: 3120  | total loss: \u001b[1m\u001b[32m0.57251\u001b[0m\u001b[0m | time: 0.023s\n",
            "| Adam | epoch: 624 | loss: 0.57251 - acc: 0.9424 -- iter: 36/36\n",
            "--\n",
            "Training Step: 3121  | total loss: \u001b[1m\u001b[32m0.53769\u001b[0m\u001b[0m | time: 0.003s\n",
            "| Adam | epoch: 625 | loss: 0.53769 - acc: 0.9482 -- iter: 08/36\n",
            "Training Step: 3122  | total loss: \u001b[1m\u001b[32m0.50060\u001b[0m\u001b[0m | time: 0.007s\n",
            "| Adam | epoch: 625 | loss: 0.50060 - acc: 0.9533 -- iter: 16/36\n",
            "Training Step: 3123  | total loss: \u001b[1m\u001b[32m1.14517\u001b[0m\u001b[0m | time: 0.012s\n",
            "| Adam | epoch: 625 | loss: 1.14517 - acc: 0.9580 -- iter: 24/36\n",
            "Training Step: 3124  | total loss: \u001b[1m\u001b[32m1.05221\u001b[0m\u001b[0m | time: 0.017s\n",
            "| Adam | epoch: 625 | loss: 1.05221 - acc: 0.8747 -- iter: 32/36\n",
            "Training Step: 3125  | total loss: \u001b[1m\u001b[32m0.97033\u001b[0m\u001b[0m | time: 0.020s\n",
            "| Adam | epoch: 625 | loss: 0.97033 - acc: 0.8985 -- iter: 36/36\n",
            "--\n",
            "Training Step: 3126  | total loss: \u001b[1m\u001b[32m0.89660\u001b[0m\u001b[0m | time: 0.004s\n",
            "| Adam | epoch: 626 | loss: 0.89660 - acc: 0.9087 -- iter: 08/36\n",
            "Training Step: 3127  | total loss: \u001b[1m\u001b[32m0.82356\u001b[0m\u001b[0m | time: 0.008s\n",
            "| Adam | epoch: 626 | loss: 0.82356 - acc: 0.9178 -- iter: 16/36\n",
            "Training Step: 3128  | total loss: \u001b[1m\u001b[32m0.76025\u001b[0m\u001b[0m | time: 0.013s\n",
            "| Adam | epoch: 626 | loss: 0.76025 - acc: 0.9178 -- iter: 24/36\n",
            "Training Step: 3129  | total loss: \u001b[1m\u001b[32m0.70535\u001b[0m\u001b[0m | time: 0.017s\n",
            "| Adam | epoch: 626 | loss: 0.70535 - acc: 0.9260 -- iter: 32/36\n",
            "Training Step: 3130  | total loss: \u001b[1m\u001b[32m0.65316\u001b[0m\u001b[0m | time: 0.023s\n",
            "| Adam | epoch: 626 | loss: 0.65316 - acc: 0.9334 -- iter: 36/36\n",
            "--\n",
            "Training Step: 3131  | total loss: \u001b[1m\u001b[32m0.60278\u001b[0m\u001b[0m | time: 0.004s\n",
            "| Adam | epoch: 627 | loss: 0.60278 - acc: 0.9401 -- iter: 08/36\n",
            "Training Step: 3132  | total loss: \u001b[1m\u001b[32m0.55740\u001b[0m\u001b[0m | time: 0.009s\n",
            "| Adam | epoch: 627 | loss: 0.55740 - acc: 0.9461 -- iter: 16/36\n",
            "Training Step: 3133  | total loss: \u001b[1m\u001b[32m0.52453\u001b[0m\u001b[0m | time: 0.016s\n",
            "| Adam | epoch: 627 | loss: 0.52453 - acc: 0.9515 -- iter: 24/36\n",
            "Training Step: 3134  | total loss: \u001b[1m\u001b[32m0.49166\u001b[0m\u001b[0m | time: 0.020s\n",
            "| Adam | epoch: 627 | loss: 0.49166 - acc: 0.9607 -- iter: 32/36\n",
            "Training Step: 3135  | total loss: \u001b[1m\u001b[32m0.46419\u001b[0m\u001b[0m | time: 0.024s\n",
            "| Adam | epoch: 627 | loss: 0.46419 - acc: 0.9646 -- iter: 36/36\n",
            "--\n",
            "Training Step: 3136  | total loss: \u001b[1m\u001b[32m0.43757\u001b[0m\u001b[0m | time: 0.003s\n",
            "| Adam | epoch: 628 | loss: 0.43757 - acc: 0.9646 -- iter: 08/36\n",
            "Training Step: 3137  | total loss: \u001b[1m\u001b[32m0.41207\u001b[0m\u001b[0m | time: 0.007s\n",
            "| Adam | epoch: 628 | loss: 0.41207 - acc: 0.9682 -- iter: 16/36\n",
            "Training Step: 3138  | total loss: \u001b[1m\u001b[32m0.38905\u001b[0m\u001b[0m | time: 0.014s\n",
            "| Adam | epoch: 628 | loss: 0.38905 - acc: 0.9713 -- iter: 24/36\n",
            "Training Step: 3139  | total loss: \u001b[1m\u001b[32m0.37147\u001b[0m\u001b[0m | time: 0.020s\n",
            "| Adam | epoch: 628 | loss: 0.37147 - acc: 0.9742 -- iter: 32/36\n",
            "Training Step: 3140  | total loss: \u001b[1m\u001b[32m0.35391\u001b[0m\u001b[0m | time: 0.024s\n",
            "| Adam | epoch: 628 | loss: 0.35391 - acc: 0.9768 -- iter: 36/36\n",
            "--\n",
            "Training Step: 3141  | total loss: \u001b[1m\u001b[32m0.33781\u001b[0m\u001b[0m | time: 0.003s\n",
            "| Adam | epoch: 629 | loss: 0.33781 - acc: 0.9791 -- iter: 08/36\n",
            "Training Step: 3142  | total loss: \u001b[1m\u001b[32m0.32363\u001b[0m\u001b[0m | time: 0.007s\n",
            "| Adam | epoch: 629 | loss: 0.32363 - acc: 0.9812 -- iter: 16/36\n",
            "Training Step: 3143  | total loss: \u001b[1m\u001b[32m0.31082\u001b[0m\u001b[0m | time: 0.012s\n",
            "| Adam | epoch: 629 | loss: 0.31082 - acc: 0.9831 -- iter: 24/36\n",
            "Training Step: 3144  | total loss: \u001b[1m\u001b[32m0.29917\u001b[0m\u001b[0m | time: 0.017s\n",
            "| Adam | epoch: 629 | loss: 0.29917 - acc: 0.9848 -- iter: 32/36\n",
            "Training Step: 3145  | total loss: \u001b[1m\u001b[32m0.28889\u001b[0m\u001b[0m | time: 0.020s\n",
            "| Adam | epoch: 629 | loss: 0.28889 - acc: 0.9877 -- iter: 36/36\n",
            "--\n",
            "Training Step: 3146  | total loss: \u001b[1m\u001b[32m0.27411\u001b[0m\u001b[0m | time: 0.004s\n",
            "| Adam | epoch: 630 | loss: 0.27411 - acc: 0.9889 -- iter: 08/36\n",
            "Training Step: 3147  | total loss: \u001b[1m\u001b[32m0.27150\u001b[0m\u001b[0m | time: 0.008s\n",
            "| Adam | epoch: 630 | loss: 0.27150 - acc: 0.9900 -- iter: 16/36\n",
            "Training Step: 3148  | total loss: \u001b[1m\u001b[32m0.26117\u001b[0m\u001b[0m | time: 0.015s\n",
            "| Adam | epoch: 630 | loss: 0.26117 - acc: 0.9910 -- iter: 24/36\n",
            "Training Step: 3149  | total loss: \u001b[1m\u001b[32m0.25413\u001b[0m\u001b[0m | time: 0.019s\n",
            "| Adam | epoch: 630 | loss: 0.25413 - acc: 0.9919 -- iter: 32/36\n",
            "Training Step: 3150  | total loss: \u001b[1m\u001b[32m0.24771\u001b[0m\u001b[0m | time: 0.023s\n",
            "| Adam | epoch: 630 | loss: 0.24771 - acc: 0.9919 -- iter: 36/36\n",
            "--\n",
            "Training Step: 3151  | total loss: \u001b[1m\u001b[32m0.24106\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 631 | loss: 0.24106 - acc: 0.9927 -- iter: 08/36\n",
            "Training Step: 3152  | total loss: \u001b[1m\u001b[32m0.23761\u001b[0m\u001b[0m | time: 0.009s\n",
            "| Adam | epoch: 631 | loss: 0.23761 - acc: 0.9934 -- iter: 16/36\n",
            "Training Step: 3153  | total loss: \u001b[1m\u001b[32m0.23504\u001b[0m\u001b[0m | time: 0.015s\n",
            "| Adam | epoch: 631 | loss: 0.23504 - acc: 0.9941 -- iter: 24/36\n",
            "Training Step: 3154  | total loss: \u001b[1m\u001b[32m0.23085\u001b[0m\u001b[0m | time: 0.019s\n",
            "| Adam | epoch: 631 | loss: 0.23085 - acc: 0.9947 -- iter: 32/36\n",
            "Training Step: 3155  | total loss: \u001b[1m\u001b[32m0.22109\u001b[0m\u001b[0m | time: 0.024s\n",
            "| Adam | epoch: 631 | loss: 0.22109 - acc: 0.9952 -- iter: 36/36\n",
            "--\n",
            "Training Step: 3156  | total loss: \u001b[1m\u001b[32m0.21223\u001b[0m\u001b[0m | time: 0.007s\n",
            "| Adam | epoch: 632 | loss: 0.21223 - acc: 0.9957 -- iter: 08/36\n",
            "Training Step: 3157  | total loss: \u001b[1m\u001b[32m0.21304\u001b[0m\u001b[0m | time: 0.012s\n",
            "| Adam | epoch: 632 | loss: 0.21304 - acc: 0.9961 -- iter: 16/36\n",
            "Training Step: 3158  | total loss: \u001b[1m\u001b[32m0.21208\u001b[0m\u001b[0m | time: 0.017s\n",
            "| Adam | epoch: 632 | loss: 0.21208 - acc: 0.9965 -- iter: 24/36\n",
            "Training Step: 3159  | total loss: \u001b[1m\u001b[32m0.20729\u001b[0m\u001b[0m | time: 0.024s\n",
            "| Adam | epoch: 632 | loss: 0.20729 - acc: 0.9969 -- iter: 32/36\n",
            "Training Step: 3160  | total loss: \u001b[1m\u001b[32m0.20681\u001b[0m\u001b[0m | time: 0.030s\n",
            "| Adam | epoch: 632 | loss: 0.20681 - acc: 0.9972 -- iter: 36/36\n",
            "--\n",
            "Training Step: 3161  | total loss: \u001b[1m\u001b[32m0.20354\u001b[0m\u001b[0m | time: 0.004s\n",
            "| Adam | epoch: 633 | loss: 0.20354 - acc: 0.9975 -- iter: 08/36\n",
            "Training Step: 3162  | total loss: \u001b[1m\u001b[32m0.20048\u001b[0m\u001b[0m | time: 0.008s\n",
            "| Adam | epoch: 633 | loss: 0.20048 - acc: 0.9979 -- iter: 16/36\n",
            "Training Step: 3163  | total loss: \u001b[1m\u001b[32m0.20102\u001b[0m\u001b[0m | time: 0.015s\n",
            "| Adam | epoch: 633 | loss: 0.20102 - acc: 0.9981 -- iter: 24/36\n",
            "Training Step: 3164  | total loss: \u001b[1m\u001b[32m0.19889\u001b[0m\u001b[0m | time: 0.019s\n",
            "| Adam | epoch: 633 | loss: 0.19889 - acc: 0.9983 -- iter: 32/36\n",
            "Training Step: 3165  | total loss: \u001b[1m\u001b[32m0.66468\u001b[0m\u001b[0m | time: 0.026s\n",
            "| Adam | epoch: 633 | loss: 0.66468 - acc: 0.9235 -- iter: 36/36\n",
            "--\n",
            "Training Step: 3166  | total loss: \u001b[1m\u001b[32m0.61752\u001b[0m\u001b[0m | time: 0.004s\n",
            "| Adam | epoch: 634 | loss: 0.61752 - acc: 0.9311 -- iter: 08/36\n",
            "Training Step: 3167  | total loss: \u001b[1m\u001b[32m0.57449\u001b[0m\u001b[0m | time: 0.009s\n",
            "| Adam | epoch: 634 | loss: 0.57449 - acc: 0.9380 -- iter: 16/36\n",
            "Training Step: 3168  | total loss: \u001b[1m\u001b[32m0.53564\u001b[0m\u001b[0m | time: 0.013s\n",
            "| Adam | epoch: 634 | loss: 0.53564 - acc: 0.9442 -- iter: 24/36\n",
            "Training Step: 3169  | total loss: \u001b[1m\u001b[32m0.50099\u001b[0m\u001b[0m | time: 0.019s\n",
            "| Adam | epoch: 634 | loss: 0.50099 - acc: 0.9498 -- iter: 32/36\n",
            "Training Step: 3170  | total loss: \u001b[1m\u001b[32m0.46721\u001b[0m\u001b[0m | time: 0.024s\n",
            "| Adam | epoch: 634 | loss: 0.46721 - acc: 0.9548 -- iter: 36/36\n",
            "--\n",
            "Training Step: 3171  | total loss: \u001b[1m\u001b[32m1.14808\u001b[0m\u001b[0m | time: 0.004s\n",
            "| Adam | epoch: 635 | loss: 1.14808 - acc: 0.9548 -- iter: 08/36\n",
            "Training Step: 3172  | total loss: \u001b[1m\u001b[32m1.05445\u001b[0m\u001b[0m | time: 0.008s\n",
            "| Adam | epoch: 635 | loss: 1.05445 - acc: 0.8718 -- iter: 16/36\n",
            "Training Step: 3173  | total loss: \u001b[1m\u001b[32m0.97082\u001b[0m\u001b[0m | time: 0.012s\n",
            "| Adam | epoch: 635 | loss: 0.97082 - acc: 0.8847 -- iter: 24/36\n",
            "Training Step: 3174  | total loss: \u001b[1m\u001b[32m0.89548\u001b[0m\u001b[0m | time: 0.017s\n",
            "| Adam | epoch: 635 | loss: 0.89548 - acc: 0.8962 -- iter: 32/36\n",
            "Training Step: 3175  | total loss: \u001b[1m\u001b[32m0.82107\u001b[0m\u001b[0m | time: 0.024s\n",
            "| Adam | epoch: 635 | loss: 0.82107 - acc: 0.9066 -- iter: 36/36\n",
            "--\n",
            "Training Step: 3176  | total loss: \u001b[1m\u001b[32m0.75526\u001b[0m\u001b[0m | time: 0.004s\n",
            "| Adam | epoch: 636 | loss: 0.75526 - acc: 0.9159 -- iter: 08/36\n",
            "Training Step: 3177  | total loss: \u001b[1m\u001b[32m1.20532\u001b[0m\u001b[0m | time: 0.008s\n",
            "| Adam | epoch: 636 | loss: 1.20532 - acc: 0.9243 -- iter: 16/36\n",
            "Training Step: 3178  | total loss: \u001b[1m\u001b[32m1.10355\u001b[0m\u001b[0m | time: 0.013s\n",
            "| Adam | epoch: 636 | loss: 1.10355 - acc: 0.8444 -- iter: 24/36\n",
            "Training Step: 3179  | total loss: \u001b[1m\u001b[32m1.00692\u001b[0m\u001b[0m | time: 0.018s\n",
            "| Adam | epoch: 636 | loss: 1.00692 - acc: 0.8600 -- iter: 32/36\n",
            "Training Step: 3180  | total loss: \u001b[1m\u001b[32m1.00692\u001b[0m\u001b[0m | time: 0.022s\n",
            "| Adam | epoch: 636 | loss: 1.00692 - acc: 0.8740 -- iter: 36/36\n",
            "--\n",
            "Training Step: 3181  | total loss: \u001b[1m\u001b[32m0.92009\u001b[0m\u001b[0m | time: 0.004s\n",
            "| Adam | epoch: 637 | loss: 0.92009 - acc: 0.8866 -- iter: 08/36\n",
            "Training Step: 3182  | total loss: \u001b[1m\u001b[32m0.84590\u001b[0m\u001b[0m | time: 0.009s\n",
            "| Adam | epoch: 637 | loss: 0.84590 - acc: 0.8979 -- iter: 16/36\n",
            "Training Step: 3183  | total loss: \u001b[1m\u001b[32m1.39422\u001b[0m\u001b[0m | time: 0.013s\n",
            "| Adam | epoch: 637 | loss: 1.39422 - acc: 0.9081 -- iter: 24/36\n",
            "Training Step: 3184  | total loss: \u001b[1m\u001b[32m1.27463\u001b[0m\u001b[0m | time: 0.017s\n",
            "| Adam | epoch: 637 | loss: 1.27463 - acc: 0.8298 -- iter: 32/36\n",
            "Training Step: 3185  | total loss: \u001b[1m\u001b[32m1.16412\u001b[0m\u001b[0m | time: 0.021s\n",
            "| Adam | epoch: 637 | loss: 1.16412 - acc: 0.8468 -- iter: 36/36\n",
            "--\n",
            "Training Step: 3186  | total loss: \u001b[1m\u001b[32m1.06475\u001b[0m\u001b[0m | time: 0.006s\n",
            "| Adam | epoch: 638 | loss: 1.06475 - acc: 0.8621 -- iter: 08/36\n",
            "Training Step: 3187  | total loss: \u001b[1m\u001b[32m0.97672\u001b[0m\u001b[0m | time: 0.010s\n",
            "| Adam | epoch: 638 | loss: 0.97672 - acc: 0.8759 -- iter: 16/36\n",
            "Training Step: 3188  | total loss: \u001b[1m\u001b[32m0.89976\u001b[0m\u001b[0m | time: 0.013s\n",
            "| Adam | epoch: 638 | loss: 0.89976 - acc: 0.8883 -- iter: 24/36\n",
            "Training Step: 3189  | total loss: \u001b[1m\u001b[32m0.82784\u001b[0m\u001b[0m | time: 0.017s\n",
            "| Adam | epoch: 638 | loss: 0.82784 - acc: 0.8995 -- iter: 32/36\n",
            "Training Step: 3190  | total loss: \u001b[1m\u001b[32m0.76292\u001b[0m\u001b[0m | time: 0.022s\n",
            "| Adam | epoch: 638 | loss: 0.76292 - acc: 0.9096 -- iter: 36/36\n",
            "--\n",
            "Training Step: 3191  | total loss: \u001b[1m\u001b[32m0.71010\u001b[0m\u001b[0m | time: 0.003s\n",
            "| Adam | epoch: 639 | loss: 0.71010 - acc: 0.9186 -- iter: 08/36\n",
            "Training Step: 3192  | total loss: \u001b[1m\u001b[32m0.66253\u001b[0m\u001b[0m | time: 0.007s\n",
            "| Adam | epoch: 639 | loss: 0.66253 - acc: 0.9267 -- iter: 16/36\n",
            "Training Step: 3193  | total loss: \u001b[1m\u001b[32m0.61848\u001b[0m\u001b[0m | time: 0.011s\n",
            "| Adam | epoch: 639 | loss: 0.61848 - acc: 0.9341 -- iter: 24/36\n",
            "Training Step: 3194  | total loss: \u001b[1m\u001b[32m0.57519\u001b[0m\u001b[0m | time: 0.015s\n",
            "| Adam | epoch: 639 | loss: 0.57519 - acc: 0.9407 -- iter: 32/36\n",
            "Training Step: 3195  | total loss: \u001b[1m\u001b[32m1.01964\u001b[0m\u001b[0m | time: 0.019s\n",
            "| Adam | epoch: 639 | loss: 1.01964 - acc: 0.9466 -- iter: 36/36\n",
            "--\n",
            "Training Step: 3196  | total loss: \u001b[1m\u001b[32m0.93548\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 640 | loss: 0.93548 - acc: 0.8644 -- iter: 08/36\n",
            "Training Step: 3197  | total loss: \u001b[1m\u001b[32m0.85527\u001b[0m\u001b[0m | time: 0.009s\n",
            "| Adam | epoch: 640 | loss: 0.85527 - acc: 0.8780 -- iter: 16/36\n",
            "Training Step: 3198  | total loss: \u001b[1m\u001b[32m0.78311\u001b[0m\u001b[0m | time: 0.013s\n",
            "| Adam | epoch: 640 | loss: 0.78311 - acc: 0.9012 -- iter: 24/36\n",
            "Training Step: 3199  | total loss: \u001b[1m\u001b[32m0.72733\u001b[0m\u001b[0m | time: 0.018s\n",
            "| Adam | epoch: 640 | loss: 0.72733 - acc: 0.9111 -- iter: 32/36\n",
            "Training Step: 3200  | total loss: \u001b[1m\u001b[32m0.67513\u001b[0m\u001b[0m | time: 0.023s\n",
            "| Adam | epoch: 640 | loss: 0.67513 - acc: 0.9199 -- iter: 36/36\n",
            "--\n",
            "Training Step: 3201  | total loss: \u001b[1m\u001b[32m0.62640\u001b[0m\u001b[0m | time: 0.004s\n",
            "| Adam | epoch: 641 | loss: 0.62640 - acc: 0.9280 -- iter: 08/36\n",
            "Training Step: 3202  | total loss: \u001b[1m\u001b[32m0.58434\u001b[0m\u001b[0m | time: 0.007s\n",
            "| Adam | epoch: 641 | loss: 0.58434 - acc: 0.9352 -- iter: 16/36\n",
            "Training Step: 3203  | total loss: \u001b[1m\u001b[32m0.54696\u001b[0m\u001b[0m | time: 0.011s\n",
            "| Adam | epoch: 641 | loss: 0.54696 - acc: 0.9416 -- iter: 24/36\n",
            "Training Step: 3204  | total loss: \u001b[1m\u001b[32m0.51325\u001b[0m\u001b[0m | time: 0.023s\n",
            "| Adam | epoch: 641 | loss: 0.51325 - acc: 0.9475 -- iter: 32/36\n",
            "Training Step: 3205  | total loss: \u001b[1m\u001b[32m0.48397\u001b[0m\u001b[0m | time: 0.028s\n",
            "| Adam | epoch: 641 | loss: 0.48397 - acc: 0.9527 -- iter: 36/36\n",
            "--\n",
            "Training Step: 3206  | total loss: \u001b[1m\u001b[32m0.45557\u001b[0m\u001b[0m | time: 0.006s\n",
            "| Adam | epoch: 642 | loss: 0.45557 - acc: 0.9575 -- iter: 08/36\n",
            "Training Step: 3207  | total loss: \u001b[1m\u001b[32m1.07949\u001b[0m\u001b[0m | time: 0.012s\n",
            "| Adam | epoch: 642 | loss: 1.07949 - acc: 0.8617 -- iter: 16/36\n",
            "Training Step: 3208  | total loss: \u001b[1m\u001b[32m0.99079\u001b[0m\u001b[0m | time: 0.016s\n",
            "| Adam | epoch: 642 | loss: 0.99079 - acc: 0.8755 -- iter: 24/36\n",
            "Training Step: 3209  | total loss: \u001b[1m\u001b[32m0.91031\u001b[0m\u001b[0m | time: 0.021s\n",
            "| Adam | epoch: 642 | loss: 0.91031 - acc: 0.8880 -- iter: 32/36\n",
            "Training Step: 3210  | total loss: \u001b[1m\u001b[32m0.83789\u001b[0m\u001b[0m | time: 0.026s\n",
            "| Adam | epoch: 642 | loss: 0.83789 - acc: 0.8992 -- iter: 36/36\n",
            "--\n",
            "Training Step: 3211  | total loss: \u001b[1m\u001b[32m0.77394\u001b[0m\u001b[0m | time: 0.003s\n",
            "| Adam | epoch: 643 | loss: 0.77394 - acc: 0.9093 -- iter: 08/36\n",
            "Training Step: 3212  | total loss: \u001b[1m\u001b[32m0.71556\u001b[0m\u001b[0m | time: 0.007s\n",
            "| Adam | epoch: 643 | loss: 0.71556 - acc: 0.9183 -- iter: 16/36\n",
            "Training Step: 3213  | total loss: \u001b[1m\u001b[32m1.50514\u001b[0m\u001b[0m | time: 0.012s\n",
            "| Adam | epoch: 643 | loss: 1.50514 - acc: 0.8265 -- iter: 24/36\n",
            "Training Step: 3214  | total loss: \u001b[1m\u001b[32m1.37455\u001b[0m\u001b[0m | time: 0.019s\n",
            "| Adam | epoch: 643 | loss: 1.37455 - acc: 0.8439 -- iter: 32/36\n",
            "Training Step: 3215  | total loss: \u001b[1m\u001b[32m1.25473\u001b[0m\u001b[0m | time: 0.023s\n",
            "| Adam | epoch: 643 | loss: 1.25473 - acc: 0.8595 -- iter: 36/36\n",
            "--\n",
            "Training Step: 3216  | total loss: \u001b[1m\u001b[32m1.14688\u001b[0m\u001b[0m | time: 0.004s\n",
            "| Adam | epoch: 644 | loss: 1.14688 - acc: 0.8595 -- iter: 08/36\n",
            "Training Step: 3217  | total loss: \u001b[1m\u001b[32m1.05314\u001b[0m\u001b[0m | time: 0.009s\n",
            "| Adam | epoch: 644 | loss: 1.05314 - acc: 0.8735 -- iter: 16/36\n",
            "Training Step: 3218  | total loss: \u001b[1m\u001b[32m0.96572\u001b[0m\u001b[0m | time: 0.013s\n",
            "| Adam | epoch: 644 | loss: 0.96572 - acc: 0.8862 -- iter: 24/36\n",
            "Training Step: 3219  | total loss: \u001b[1m\u001b[32m1.52701\u001b[0m\u001b[0m | time: 0.017s\n",
            "| Adam | epoch: 644 | loss: 1.52701 - acc: 0.8976 -- iter: 32/36\n",
            "Training Step: 3220  | total loss: \u001b[1m\u001b[32m1.39378\u001b[0m\u001b[0m | time: 0.023s\n",
            "| Adam | epoch: 644 | loss: 1.39378 - acc: 0.8203 -- iter: 36/36\n",
            "--\n",
            "Training Step: 3221  | total loss: \u001b[1m\u001b[32m1.27664\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 645 | loss: 1.27664 - acc: 0.8383 -- iter: 08/36\n",
            "Training Step: 3222  | total loss: \u001b[1m\u001b[32m1.17123\u001b[0m\u001b[0m | time: 0.010s\n",
            "| Adam | epoch: 645 | loss: 1.17123 - acc: 0.8544 -- iter: 16/36\n",
            "Training Step: 3223  | total loss: \u001b[1m\u001b[32m1.07178\u001b[0m\u001b[0m | time: 0.015s\n",
            "| Adam | epoch: 645 | loss: 1.07178 - acc: 0.8690 -- iter: 24/36\n",
            "Training Step: 3224  | total loss: \u001b[1m\u001b[32m0.98684\u001b[0m\u001b[0m | time: 0.021s\n",
            "| Adam | epoch: 645 | loss: 0.98684 - acc: 0.8939 -- iter: 32/36\n",
            "Training Step: 3225  | total loss: \u001b[1m\u001b[32m0.90619\u001b[0m\u001b[0m | time: 0.027s\n",
            "| Adam | epoch: 645 | loss: 0.90619 - acc: 0.9045 -- iter: 36/36\n",
            "--\n",
            "Training Step: 3226  | total loss: \u001b[1m\u001b[32m0.83413\u001b[0m\u001b[0m | time: 0.004s\n",
            "| Adam | epoch: 646 | loss: 0.83413 - acc: 0.9140 -- iter: 08/36\n",
            "Training Step: 3227  | total loss: \u001b[1m\u001b[32m0.77101\u001b[0m\u001b[0m | time: 0.009s\n",
            "| Adam | epoch: 646 | loss: 0.77101 - acc: 0.9226 -- iter: 16/36\n",
            "Training Step: 3228  | total loss: \u001b[1m\u001b[32m0.71416\u001b[0m\u001b[0m | time: 0.014s\n",
            "| Adam | epoch: 646 | loss: 0.71416 - acc: 0.9304 -- iter: 24/36\n",
            "Training Step: 3229  | total loss: \u001b[1m\u001b[32m0.66276\u001b[0m\u001b[0m | time: 0.019s\n",
            "| Adam | epoch: 646 | loss: 0.66276 - acc: 0.9373 -- iter: 32/36\n",
            "Training Step: 3230  | total loss: \u001b[1m\u001b[32m0.61498\u001b[0m\u001b[0m | time: 0.023s\n",
            "| Adam | epoch: 646 | loss: 0.61498 - acc: 0.9436 -- iter: 36/36\n",
            "--\n",
            "Training Step: 3231  | total loss: \u001b[1m\u001b[32m1.41340\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 647 | loss: 1.41340 - acc: 0.8492 -- iter: 08/36\n",
            "Training Step: 3232  | total loss: \u001b[1m\u001b[32m1.29014\u001b[0m\u001b[0m | time: 0.009s\n",
            "| Adam | epoch: 647 | loss: 1.29014 - acc: 0.8643 -- iter: 16/36\n",
            "Training Step: 3233  | total loss: \u001b[1m\u001b[32m1.17484\u001b[0m\u001b[0m | time: 0.013s\n",
            "| Adam | epoch: 647 | loss: 1.17484 - acc: 0.8779 -- iter: 24/36\n",
            "Training Step: 3234  | total loss: \u001b[1m\u001b[32m1.07113\u001b[0m\u001b[0m | time: 0.017s\n",
            "| Adam | epoch: 647 | loss: 1.07113 - acc: 0.8901 -- iter: 32/36\n",
            "Training Step: 3235  | total loss: \u001b[1m\u001b[32m0.98824\u001b[0m\u001b[0m | time: 0.022s\n",
            "| Adam | epoch: 647 | loss: 0.98824 - acc: 0.9011 -- iter: 36/36\n",
            "--\n",
            "Training Step: 3236  | total loss: \u001b[1m\u001b[32m0.90851\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 648 | loss: 0.90851 - acc: 0.9110 -- iter: 08/36\n",
            "Training Step: 3237  | total loss: \u001b[1m\u001b[32m0.83826\u001b[0m\u001b[0m | time: 0.010s\n",
            "| Adam | epoch: 648 | loss: 0.83826 - acc: 0.9199 -- iter: 16/36\n",
            "Training Step: 3238  | total loss: \u001b[1m\u001b[32m0.77673\u001b[0m\u001b[0m | time: 0.016s\n",
            "| Adam | epoch: 648 | loss: 0.77673 - acc: 0.9279 -- iter: 24/36\n",
            "Training Step: 3239  | total loss: \u001b[1m\u001b[32m0.71723\u001b[0m\u001b[0m | time: 0.021s\n",
            "| Adam | epoch: 648 | loss: 0.71723 - acc: 0.9351 -- iter: 32/36\n",
            "Training Step: 3240  | total loss: \u001b[1m\u001b[32m0.66365\u001b[0m\u001b[0m | time: 0.025s\n",
            "| Adam | epoch: 648 | loss: 0.66365 - acc: 0.9416 -- iter: 36/36\n",
            "--\n",
            "Training Step: 3241  | total loss: \u001b[1m\u001b[32m0.61781\u001b[0m\u001b[0m | time: 0.004s\n",
            "| Adam | epoch: 649 | loss: 0.61781 - acc: 0.9474 -- iter: 08/36\n",
            "Training Step: 3242  | total loss: \u001b[1m\u001b[32m0.57597\u001b[0m\u001b[0m | time: 0.009s\n",
            "| Adam | epoch: 649 | loss: 0.57597 - acc: 0.9527 -- iter: 16/36\n",
            "Training Step: 3243  | total loss: \u001b[1m\u001b[32m1.04354\u001b[0m\u001b[0m | time: 0.013s\n",
            "| Adam | epoch: 649 | loss: 1.04354 - acc: 0.8574 -- iter: 24/36\n",
            "Training Step: 3244  | total loss: \u001b[1m\u001b[32m0.95926\u001b[0m\u001b[0m | time: 0.018s\n",
            "| Adam | epoch: 649 | loss: 0.95926 - acc: 0.8717 -- iter: 32/36\n",
            "Training Step: 3245  | total loss: \u001b[1m\u001b[32m0.88762\u001b[0m\u001b[0m | time: 0.022s\n",
            "| Adam | epoch: 649 | loss: 0.88762 - acc: 0.8845 -- iter: 36/36\n",
            "--\n",
            "Training Step: 3246  | total loss: \u001b[1m\u001b[32m0.82313\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 650 | loss: 0.82313 - acc: 0.8961 -- iter: 08/36\n",
            "Training Step: 3247  | total loss: \u001b[1m\u001b[32m0.75812\u001b[0m\u001b[0m | time: 0.009s\n",
            "| Adam | epoch: 650 | loss: 0.75812 - acc: 0.9065 -- iter: 16/36\n",
            "Training Step: 3248  | total loss: \u001b[1m\u001b[32m0.70150\u001b[0m\u001b[0m | time: 0.015s\n",
            "| Adam | epoch: 650 | loss: 0.70150 - acc: 0.9158 -- iter: 24/36\n",
            "Training Step: 3249  | total loss: \u001b[1m\u001b[32m0.65138\u001b[0m\u001b[0m | time: 0.019s\n",
            "| Adam | epoch: 650 | loss: 0.65138 - acc: 0.9242 -- iter: 32/36\n",
            "Training Step: 3250  | total loss: \u001b[1m\u001b[32m0.65138\u001b[0m\u001b[0m | time: 0.023s\n",
            "| Adam | epoch: 650 | loss: 0.65138 - acc: 0.9242 -- iter: 36/36\n",
            "--\n",
            "Training Step: 3251  | total loss: \u001b[1m\u001b[32m0.56663\u001b[0m\u001b[0m | time: 0.004s\n",
            "| Adam | epoch: 651 | loss: 0.56663 - acc: 0.9386 -- iter: 08/36\n",
            "Training Step: 3252  | total loss: \u001b[1m\u001b[32m0.52928\u001b[0m\u001b[0m | time: 0.009s\n",
            "| Adam | epoch: 651 | loss: 0.52928 - acc: 0.9448 -- iter: 16/36\n",
            "Training Step: 3253  | total loss: \u001b[1m\u001b[32m0.49390\u001b[0m\u001b[0m | time: 0.014s\n",
            "| Adam | epoch: 651 | loss: 0.49390 - acc: 0.9503 -- iter: 24/36\n",
            "Training Step: 3254  | total loss: \u001b[1m\u001b[32m0.46526\u001b[0m\u001b[0m | time: 0.018s\n",
            "| Adam | epoch: 651 | loss: 0.46526 - acc: 0.9553 -- iter: 32/36\n",
            "Training Step: 3255  | total loss: \u001b[1m\u001b[32m0.97863\u001b[0m\u001b[0m | time: 0.023s\n",
            "| Adam | epoch: 651 | loss: 0.97863 - acc: 0.8597 -- iter: 36/36\n",
            "--\n",
            "Training Step: 3256  | total loss: \u001b[1m\u001b[32m0.97863\u001b[0m\u001b[0m | time: 0.004s\n",
            "| Adam | epoch: 652 | loss: 0.97863 - acc: 0.8597 -- iter: 08/36\n",
            "Training Step: 3257  | total loss: \u001b[1m\u001b[32m0.90278\u001b[0m\u001b[0m | time: 0.008s\n",
            "| Adam | epoch: 652 | loss: 0.90278 - acc: 0.8738 -- iter: 16/36\n",
            "Training Step: 3258  | total loss: \u001b[1m\u001b[32m0.76258\u001b[0m\u001b[0m | time: 0.012s\n",
            "| Adam | epoch: 652 | loss: 0.76258 - acc: 0.8977 -- iter: 24/36\n",
            "Training Step: 3259  | total loss: \u001b[1m\u001b[32m0.70549\u001b[0m\u001b[0m | time: 0.016s\n",
            "| Adam | epoch: 652 | loss: 0.70549 - acc: 0.9080 -- iter: 32/36\n",
            "Training Step: 3260  | total loss: \u001b[1m\u001b[32m0.65405\u001b[0m\u001b[0m | time: 0.021s\n",
            "| Adam | epoch: 652 | loss: 0.65405 - acc: 0.9172 -- iter: 36/36\n",
            "--\n",
            "Training Step: 3261  | total loss: \u001b[1m\u001b[32m1.10588\u001b[0m\u001b[0m | time: 0.004s\n",
            "| Adam | epoch: 653 | loss: 1.10588 - acc: 0.8255 -- iter: 08/36\n",
            "Training Step: 3262  | total loss: \u001b[1m\u001b[32m1.01493\u001b[0m\u001b[0m | time: 0.011s\n",
            "| Adam | epoch: 653 | loss: 1.01493 - acc: 0.8255 -- iter: 16/36\n",
            "Training Step: 3263  | total loss: \u001b[1m\u001b[32m0.93108\u001b[0m\u001b[0m | time: 0.015s\n",
            "| Adam | epoch: 653 | loss: 0.93108 - acc: 0.8429 -- iter: 24/36\n",
            "Training Step: 3264  | total loss: \u001b[1m\u001b[32m0.85562\u001b[0m\u001b[0m | time: 0.020s\n",
            "| Adam | epoch: 653 | loss: 0.85562 - acc: 0.8586 -- iter: 32/36\n",
            "Training Step: 3265  | total loss: \u001b[1m\u001b[32m0.79203\u001b[0m\u001b[0m | time: 0.024s\n",
            "| Adam | epoch: 653 | loss: 0.79203 - acc: 0.8728 -- iter: 36/36\n",
            "--\n",
            "Training Step: 3266  | total loss: \u001b[1m\u001b[32m0.73298\u001b[0m\u001b[0m | time: 0.004s\n",
            "| Adam | epoch: 654 | loss: 0.73298 - acc: 0.8855 -- iter: 08/36\n",
            "Training Step: 3267  | total loss: \u001b[1m\u001b[32m0.67860\u001b[0m\u001b[0m | time: 0.009s\n",
            "| Adam | epoch: 654 | loss: 0.67860 - acc: 0.8969 -- iter: 16/36\n",
            "Training Step: 3268  | total loss: \u001b[1m\u001b[32m0.63008\u001b[0m\u001b[0m | time: 0.015s\n",
            "| Adam | epoch: 654 | loss: 0.63008 - acc: 0.9072 -- iter: 24/36\n",
            "Training Step: 3269  | total loss: \u001b[1m\u001b[32m0.58922\u001b[0m\u001b[0m | time: 0.020s\n",
            "| Adam | epoch: 654 | loss: 0.58922 - acc: 0.9165 -- iter: 32/36\n",
            "Training Step: 3270  | total loss: \u001b[1m\u001b[32m0.55223\u001b[0m\u001b[0m | time: 0.024s\n",
            "| Adam | epoch: 654 | loss: 0.55223 - acc: 0.9249 -- iter: 36/36\n",
            "--\n",
            "Training Step: 3271  | total loss: \u001b[1m\u001b[32m0.55223\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 655 | loss: 0.55223 - acc: 0.9324 -- iter: 08/36\n",
            "Training Step: 3272  | total loss: \u001b[1m\u001b[32m0.48628\u001b[0m\u001b[0m | time: 0.009s\n",
            "| Adam | epoch: 655 | loss: 0.48628 - acc: 0.9391 -- iter: 16/36\n",
            "Training Step: 3273  | total loss: \u001b[1m\u001b[32m0.93910\u001b[0m\u001b[0m | time: 0.014s\n",
            "| Adam | epoch: 655 | loss: 0.93910 - acc: 0.9452 -- iter: 24/36\n",
            "Training Step: 3274  | total loss: \u001b[1m\u001b[32m0.86462\u001b[0m\u001b[0m | time: 0.018s\n",
            "| Adam | epoch: 655 | loss: 0.86462 - acc: 0.8507 -- iter: 32/36\n",
            "Training Step: 3275  | total loss: \u001b[1m\u001b[32m0.79976\u001b[0m\u001b[0m | time: 0.022s\n",
            "| Adam | epoch: 655 | loss: 0.79976 - acc: 0.8656 -- iter: 36/36\n",
            "--\n",
            "Training Step: 3276  | total loss: \u001b[1m\u001b[32m0.74129\u001b[0m\u001b[0m | time: 0.004s\n",
            "| Adam | epoch: 656 | loss: 0.74129 - acc: 0.8791 -- iter: 08/36\n",
            "Training Step: 3277  | total loss: \u001b[1m\u001b[32m0.74129\u001b[0m\u001b[0m | time: 0.009s\n",
            "| Adam | epoch: 656 | loss: 0.74129 - acc: 0.8912 -- iter: 16/36\n",
            "Training Step: 3278  | total loss: \u001b[1m\u001b[32m0.69182\u001b[0m\u001b[0m | time: 0.013s\n",
            "| Adam | epoch: 656 | loss: 0.69182 - acc: 0.9020 -- iter: 24/36\n",
            "Training Step: 3279  | total loss: \u001b[1m\u001b[32m0.63908\u001b[0m\u001b[0m | time: 0.017s\n",
            "| Adam | epoch: 656 | loss: 0.63908 - acc: 0.9118 -- iter: 32/36\n",
            "Training Step: 3280  | total loss: \u001b[1m\u001b[32m1.39951\u001b[0m\u001b[0m | time: 0.022s\n",
            "| Adam | epoch: 656 | loss: 1.39951 - acc: 0.8457 -- iter: 36/36\n",
            "--\n",
            "Training Step: 3281  | total loss: \u001b[1m\u001b[32m1.27819\u001b[0m\u001b[0m | time: 0.003s\n",
            "| Adam | epoch: 657 | loss: 1.27819 - acc: 0.8611 -- iter: 08/36\n",
            "Training Step: 3282  | total loss: \u001b[1m\u001b[32m1.16623\u001b[0m\u001b[0m | time: 0.006s\n",
            "| Adam | epoch: 657 | loss: 1.16623 - acc: 0.8875 -- iter: 16/36\n",
            "Training Step: 3283  | total loss: \u001b[1m\u001b[32m1.06545\u001b[0m\u001b[0m | time: 0.010s\n",
            "| Adam | epoch: 657 | loss: 1.06545 - acc: 0.8987 -- iter: 24/36\n",
            "Training Step: 3284  | total loss: \u001b[1m\u001b[32m0.98294\u001b[0m\u001b[0m | time: 0.015s\n",
            "| Adam | epoch: 657 | loss: 0.98294 - acc: 0.9089 -- iter: 32/36\n",
            "Training Step: 3285  | total loss: \u001b[1m\u001b[32m0.90302\u001b[0m\u001b[0m | time: 0.019s\n",
            "| Adam | epoch: 657 | loss: 0.90302 - acc: 0.9180 -- iter: 36/36\n",
            "--\n",
            "Training Step: 3286  | total loss: \u001b[1m\u001b[32m0.83369\u001b[0m\u001b[0m | time: 0.003s\n",
            "| Adam | epoch: 658 | loss: 0.83369 - acc: 0.9262 -- iter: 08/36\n",
            "Training Step: 3287  | total loss: \u001b[1m\u001b[32m0.76969\u001b[0m\u001b[0m | time: 0.007s\n",
            "| Adam | epoch: 658 | loss: 0.76969 - acc: 0.9336 -- iter: 16/36\n",
            "Training Step: 3288  | total loss: \u001b[1m\u001b[32m0.71433\u001b[0m\u001b[0m | time: 0.012s\n",
            "| Adam | epoch: 658 | loss: 0.71433 - acc: 0.9402 -- iter: 24/36\n",
            "Training Step: 3289  | total loss: \u001b[1m\u001b[32m0.66452\u001b[0m\u001b[0m | time: 0.017s\n",
            "| Adam | epoch: 658 | loss: 0.66452 - acc: 0.9462 -- iter: 32/36\n",
            "Training Step: 3290  | total loss: \u001b[1m\u001b[32m0.61766\u001b[0m\u001b[0m | time: 0.021s\n",
            "| Adam | epoch: 658 | loss: 0.61766 - acc: 0.9462 -- iter: 36/36\n",
            "--\n",
            "Training Step: 3291  | total loss: \u001b[1m\u001b[32m0.57738\u001b[0m\u001b[0m | time: 0.004s\n",
            "| Adam | epoch: 659 | loss: 0.57738 - acc: 0.9516 -- iter: 08/36\n",
            "Training Step: 3292  | total loss: \u001b[1m\u001b[32m1.11287\u001b[0m\u001b[0m | time: 0.008s\n",
            "| Adam | epoch: 659 | loss: 1.11287 - acc: 0.8814 -- iter: 16/36\n",
            "Training Step: 3293  | total loss: \u001b[1m\u001b[32m1.02539\u001b[0m\u001b[0m | time: 0.012s\n",
            "| Adam | epoch: 659 | loss: 1.02539 - acc: 0.8933 -- iter: 24/36\n",
            "Training Step: 3294  | total loss: \u001b[1m\u001b[32m0.94620\u001b[0m\u001b[0m | time: 0.017s\n",
            "| Adam | epoch: 659 | loss: 0.94620 - acc: 0.9039 -- iter: 32/36\n",
            "Training Step: 3295  | total loss: \u001b[1m\u001b[32m0.87490\u001b[0m\u001b[0m | time: 0.022s\n",
            "| Adam | epoch: 659 | loss: 0.87490 - acc: 0.9135 -- iter: 36/36\n",
            "--\n",
            "Training Step: 3296  | total loss: \u001b[1m\u001b[32m0.80766\u001b[0m\u001b[0m | time: 0.004s\n",
            "| Adam | epoch: 660 | loss: 0.80766 - acc: 0.9222 -- iter: 08/36\n",
            "Training Step: 3297  | total loss: \u001b[1m\u001b[32m0.74353\u001b[0m\u001b[0m | time: 0.007s\n",
            "| Adam | epoch: 660 | loss: 0.74353 - acc: 0.9300 -- iter: 16/36\n",
            "Training Step: 3298  | total loss: \u001b[1m\u001b[32m1.11613\u001b[0m\u001b[0m | time: 0.010s\n",
            "| Adam | epoch: 660 | loss: 1.11613 - acc: 0.8745 -- iter: 24/36\n",
            "Training Step: 3299  | total loss: \u001b[1m\u001b[32m1.02670\u001b[0m\u001b[0m | time: 0.013s\n",
            "| Adam | epoch: 660 | loss: 1.02670 - acc: 0.8870 -- iter: 32/36\n",
            "Training Step: 3300  | total loss: \u001b[1m\u001b[32m0.95066\u001b[0m\u001b[0m | time: 0.018s\n",
            "| Adam | epoch: 660 | loss: 0.95066 - acc: 0.8983 -- iter: 36/36\n",
            "--\n",
            "Training Step: 3301  | total loss: \u001b[1m\u001b[32m0.88226\u001b[0m\u001b[0m | time: 0.004s\n",
            "| Adam | epoch: 661 | loss: 0.88226 - acc: 0.9085 -- iter: 08/36\n",
            "Training Step: 3302  | total loss: \u001b[1m\u001b[32m0.81525\u001b[0m\u001b[0m | time: 0.009s\n",
            "| Adam | epoch: 661 | loss: 0.81525 - acc: 0.9176 -- iter: 16/36\n",
            "Training Step: 3303  | total loss: \u001b[1m\u001b[32m0.75087\u001b[0m\u001b[0m | time: 0.014s\n",
            "| Adam | epoch: 661 | loss: 0.75087 - acc: 0.9259 -- iter: 24/36\n",
            "Training Step: 3304  | total loss: \u001b[1m\u001b[32m0.69242\u001b[0m\u001b[0m | time: 0.019s\n",
            "| Adam | epoch: 661 | loss: 0.69242 - acc: 0.9333 -- iter: 32/36\n",
            "Training Step: 3305  | total loss: \u001b[1m\u001b[32m0.64525\u001b[0m\u001b[0m | time: 0.022s\n",
            "| Adam | epoch: 661 | loss: 0.64525 - acc: 0.9400 -- iter: 36/36\n",
            "--\n",
            "Training Step: 3306  | total loss: \u001b[1m\u001b[32m0.60159\u001b[0m\u001b[0m | time: 0.004s\n",
            "| Adam | epoch: 662 | loss: 0.60159 - acc: 0.9460 -- iter: 08/36\n",
            "Training Step: 3307  | total loss: \u001b[1m\u001b[32m0.56223\u001b[0m\u001b[0m | time: 0.008s\n",
            "| Adam | epoch: 662 | loss: 0.56223 - acc: 0.9514 -- iter: 16/36\n",
            "Training Step: 3308  | total loss: \u001b[1m\u001b[32m0.52569\u001b[0m\u001b[0m | time: 0.013s\n",
            "| Adam | epoch: 662 | loss: 0.52569 - acc: 0.9562 -- iter: 24/36\n",
            "Training Step: 3309  | total loss: \u001b[1m\u001b[32m0.49323\u001b[0m\u001b[0m | time: 0.017s\n",
            "| Adam | epoch: 662 | loss: 0.49323 - acc: 0.9606 -- iter: 32/36\n",
            "Training Step: 3310  | total loss: \u001b[1m\u001b[32m0.46250\u001b[0m\u001b[0m | time: 0.023s\n",
            "| Adam | epoch: 662 | loss: 0.46250 - acc: 0.9645 -- iter: 36/36\n",
            "--\n",
            "Training Step: 3311  | total loss: \u001b[1m\u001b[32m0.43724\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 663 | loss: 0.43724 - acc: 0.9681 -- iter: 08/36\n",
            "Training Step: 3312  | total loss: \u001b[1m\u001b[32m0.40671\u001b[0m\u001b[0m | time: 0.009s\n",
            "| Adam | epoch: 663 | loss: 0.40671 - acc: 0.9713 -- iter: 16/36\n",
            "Training Step: 3313  | total loss: \u001b[1m\u001b[32m0.37918\u001b[0m\u001b[0m | time: 0.013s\n",
            "| Adam | epoch: 663 | loss: 0.37918 - acc: 0.9742 -- iter: 24/36\n",
            "Training Step: 3314  | total loss: \u001b[1m\u001b[32m0.35953\u001b[0m\u001b[0m | time: 0.018s\n",
            "| Adam | epoch: 663 | loss: 0.35953 - acc: 0.9767 -- iter: 32/36\n",
            "Training Step: 3315  | total loss: \u001b[1m\u001b[32m0.34607\u001b[0m\u001b[0m | time: 0.023s\n",
            "| Adam | epoch: 663 | loss: 0.34607 - acc: 0.9791 -- iter: 36/36\n",
            "--\n",
            "Training Step: 3316  | total loss: \u001b[1m\u001b[32m0.33307\u001b[0m\u001b[0m | time: 0.003s\n",
            "| Adam | epoch: 664 | loss: 0.33307 - acc: 0.9812 -- iter: 08/36\n",
            "Training Step: 3317  | total loss: \u001b[1m\u001b[32m0.32103\u001b[0m\u001b[0m | time: 0.008s\n",
            "| Adam | epoch: 664 | loss: 0.32103 - acc: 0.9830 -- iter: 16/36\n",
            "Training Step: 3318  | total loss: \u001b[1m\u001b[32m0.31155\u001b[0m\u001b[0m | time: 0.014s\n",
            "| Adam | epoch: 664 | loss: 0.31155 - acc: 0.9863 -- iter: 24/36\n",
            "Training Step: 3319  | total loss: \u001b[1m\u001b[32m0.30294\u001b[0m\u001b[0m | time: 0.019s\n",
            "| Adam | epoch: 664 | loss: 0.30294 - acc: 0.9876 -- iter: 32/36\n",
            "Training Step: 3320  | total loss: \u001b[1m\u001b[32m0.28914\u001b[0m\u001b[0m | time: 0.025s\n",
            "| Adam | epoch: 664 | loss: 0.28914 - acc: 0.9889 -- iter: 36/36\n",
            "--\n",
            "Training Step: 3321  | total loss: \u001b[1m\u001b[32m0.28050\u001b[0m\u001b[0m | time: 0.004s\n",
            "| Adam | epoch: 665 | loss: 0.28050 - acc: 0.9900 -- iter: 08/36\n",
            "Training Step: 3322  | total loss: \u001b[1m\u001b[32m0.27183\u001b[0m\u001b[0m | time: 0.009s\n",
            "| Adam | epoch: 665 | loss: 0.27183 - acc: 0.9910 -- iter: 16/36\n",
            "Training Step: 3323  | total loss: \u001b[1m\u001b[32m0.26002\u001b[0m\u001b[0m | time: 0.012s\n",
            "| Adam | epoch: 665 | loss: 0.26002 - acc: 0.9919 -- iter: 24/36\n",
            "Training Step: 3324  | total loss: \u001b[1m\u001b[32m0.25858\u001b[0m\u001b[0m | time: 0.017s\n",
            "| Adam | epoch: 665 | loss: 0.25858 - acc: 0.9927 -- iter: 32/36\n",
            "Training Step: 3325  | total loss: \u001b[1m\u001b[32m0.25555\u001b[0m\u001b[0m | time: 0.022s\n",
            "| Adam | epoch: 665 | loss: 0.25555 - acc: 0.9934 -- iter: 36/36\n",
            "--\n",
            "Training Step: 3326  | total loss: \u001b[1m\u001b[32m0.24705\u001b[0m\u001b[0m | time: 0.004s\n",
            "| Adam | epoch: 666 | loss: 0.24705 - acc: 0.9941 -- iter: 08/36\n",
            "Training Step: 3327  | total loss: \u001b[1m\u001b[32m0.24059\u001b[0m\u001b[0m | time: 0.008s\n",
            "| Adam | epoch: 666 | loss: 0.24059 - acc: 0.9947 -- iter: 16/36\n",
            "Training Step: 3328  | total loss: \u001b[1m\u001b[32m0.23443\u001b[0m\u001b[0m | time: 0.012s\n",
            "| Adam | epoch: 666 | loss: 0.23443 - acc: 0.9952 -- iter: 24/36\n",
            "Training Step: 3329  | total loss: \u001b[1m\u001b[32m0.22789\u001b[0m\u001b[0m | time: 0.016s\n",
            "| Adam | epoch: 666 | loss: 0.22789 - acc: 0.9957 -- iter: 32/36\n",
            "Training Step: 3330  | total loss: \u001b[1m\u001b[32m0.22192\u001b[0m\u001b[0m | time: 0.020s\n",
            "| Adam | epoch: 666 | loss: 0.22192 - acc: 0.9961 -- iter: 36/36\n",
            "--\n",
            "Training Step: 3331  | total loss: \u001b[1m\u001b[32m0.21931\u001b[0m\u001b[0m | time: 0.004s\n",
            "| Adam | epoch: 667 | loss: 0.21931 - acc: 0.9961 -- iter: 08/36\n",
            "Training Step: 3332  | total loss: \u001b[1m\u001b[32m0.21911\u001b[0m\u001b[0m | time: 0.009s\n",
            "| Adam | epoch: 667 | loss: 0.21911 - acc: 0.9965 -- iter: 16/36\n",
            "Training Step: 3333  | total loss: \u001b[1m\u001b[32m0.21510\u001b[0m\u001b[0m | time: 0.015s\n",
            "| Adam | epoch: 667 | loss: 0.21510 - acc: 0.9969 -- iter: 24/36\n",
            "Training Step: 3334  | total loss: \u001b[1m\u001b[32m0.21091\u001b[0m\u001b[0m | time: 0.020s\n",
            "| Adam | epoch: 667 | loss: 0.21091 - acc: 0.9972 -- iter: 32/36\n",
            "Training Step: 3335  | total loss: \u001b[1m\u001b[32m0.21422\u001b[0m\u001b[0m | time: 0.024s\n",
            "| Adam | epoch: 667 | loss: 0.21422 - acc: 0.9975 -- iter: 36/36\n",
            "--\n",
            "Training Step: 3336  | total loss: \u001b[1m\u001b[32m0.21707\u001b[0m\u001b[0m | time: 0.006s\n",
            "| Adam | epoch: 668 | loss: 0.21707 - acc: 0.9977 -- iter: 08/36\n",
            "Training Step: 3337  | total loss: \u001b[1m\u001b[32m0.21275\u001b[0m\u001b[0m | time: 0.009s\n",
            "| Adam | epoch: 668 | loss: 0.21275 - acc: 0.9979 -- iter: 16/36\n",
            "Training Step: 3338  | total loss: \u001b[1m\u001b[32m0.21181\u001b[0m\u001b[0m | time: 0.013s\n",
            "| Adam | epoch: 668 | loss: 0.21181 - acc: 0.9981 -- iter: 24/36\n",
            "Training Step: 3339  | total loss: \u001b[1m\u001b[32m0.93410\u001b[0m\u001b[0m | time: 0.016s\n",
            "| Adam | epoch: 668 | loss: 0.93410 - acc: 0.9983 -- iter: 32/36\n",
            "Training Step: 3340  | total loss: \u001b[1m\u001b[32m0.85573\u001b[0m\u001b[0m | time: 0.022s\n",
            "| Adam | epoch: 668 | loss: 0.85573 - acc: 0.8985 -- iter: 36/36\n",
            "--\n",
            "Training Step: 3341  | total loss: \u001b[1m\u001b[32m0.78963\u001b[0m\u001b[0m | time: 0.003s\n",
            "| Adam | epoch: 669 | loss: 0.78963 - acc: 0.9086 -- iter: 08/36\n",
            "Training Step: 3342  | total loss: \u001b[1m\u001b[32m0.73007\u001b[0m\u001b[0m | time: 0.009s\n",
            "| Adam | epoch: 669 | loss: 0.73007 - acc: 0.9260 -- iter: 16/36\n",
            "Training Step: 3770  | total loss: \u001b[1m\u001b[32m0.36290\u001b[0m\u001b[0m | time: 0.029s\n",
            "| Adam | epoch: 754 | loss: 0.36290 - acc: 0.9765 -- iter: 36/36\n",
            "--\n",
            "Training Step: 3771  | total loss: \u001b[1m\u001b[32m0.34059\u001b[0m\u001b[0m | time: 0.006s\n",
            "| Adam | epoch: 755 | loss: 0.34059 - acc: 0.9789 -- iter: 08/36\n",
            "Training Step: 3772  | total loss: \u001b[1m\u001b[32m0.31427\u001b[0m\u001b[0m | time: 0.012s\n",
            "| Adam | epoch: 755 | loss: 0.31427 - acc: 0.9810 -- iter: 16/36\n",
            "Training Step: 3773  | total loss: \u001b[1m\u001b[32m0.29204\u001b[0m\u001b[0m | time: 0.019s\n",
            "| Adam | epoch: 755 | loss: 0.29204 - acc: 0.9829 -- iter: 24/36\n",
            "Training Step: 3774  | total loss: \u001b[1m\u001b[32m0.27273\u001b[0m\u001b[0m | time: 0.022s\n",
            "| Adam | epoch: 755 | loss: 0.27273 - acc: 0.9846 -- iter: 32/36\n",
            "Training Step: 3775  | total loss: \u001b[1m\u001b[32m0.25529\u001b[0m\u001b[0m | time: 0.028s\n",
            "| Adam | epoch: 755 | loss: 0.25529 - acc: 0.9861 -- iter: 36/36\n",
            "--\n",
            "Training Step: 3776  | total loss: \u001b[1m\u001b[32m0.23968\u001b[0m\u001b[0m | time: 0.007s\n",
            "| Adam | epoch: 756 | loss: 0.23968 - acc: 0.9875 -- iter: 08/36\n",
            "Training Step: 3777  | total loss: \u001b[1m\u001b[32m0.22523\u001b[0m\u001b[0m | time: 0.011s\n",
            "| Adam | epoch: 756 | loss: 0.22523 - acc: 0.9888 -- iter: 16/36\n",
            "Training Step: 3778  | total loss: \u001b[1m\u001b[32m0.21249\u001b[0m\u001b[0m | time: 0.015s\n",
            "| Adam | epoch: 756 | loss: 0.21249 - acc: 0.9899 -- iter: 24/36\n",
            "Training Step: 3779  | total loss: \u001b[1m\u001b[32m0.20112\u001b[0m\u001b[0m | time: 0.018s\n",
            "| Adam | epoch: 756 | loss: 0.20112 - acc: 0.9909 -- iter: 32/36\n",
            "Training Step: 3780  | total loss: \u001b[1m\u001b[32m0.19298\u001b[0m\u001b[0m | time: 0.023s\n",
            "| Adam | epoch: 756 | loss: 0.19298 - acc: 0.9918 -- iter: 36/36\n",
            "--\n",
            "Training Step: 3781  | total loss: \u001b[1m\u001b[32m0.18562\u001b[0m\u001b[0m | time: 0.004s\n",
            "| Adam | epoch: 757 | loss: 0.18562 - acc: 0.9926 -- iter: 08/36\n",
            "Training Step: 3782  | total loss: \u001b[1m\u001b[32m0.17596\u001b[0m\u001b[0m | time: 0.008s\n",
            "| Adam | epoch: 757 | loss: 0.17596 - acc: 0.9934 -- iter: 16/36\n",
            "Training Step: 3783  | total loss: \u001b[1m\u001b[32m0.16869\u001b[0m\u001b[0m | time: 0.013s\n",
            "| Adam | epoch: 757 | loss: 0.16869 - acc: 0.9940 -- iter: 24/36\n",
            "Training Step: 3784  | total loss: \u001b[1m\u001b[32m0.15952\u001b[0m\u001b[0m | time: 0.016s\n",
            "| Adam | epoch: 757 | loss: 0.15952 - acc: 0.9940 -- iter: 32/36\n",
            "Training Step: 3785  | total loss: \u001b[1m\u001b[32m0.15399\u001b[0m\u001b[0m | time: 0.021s\n",
            "| Adam | epoch: 757 | loss: 0.15399 - acc: 0.9946 -- iter: 36/36\n",
            "--\n",
            "Training Step: 3786  | total loss: \u001b[1m\u001b[32m0.14445\u001b[0m\u001b[0m | time: 0.004s\n",
            "| Adam | epoch: 758 | loss: 0.14445 - acc: 0.9952 -- iter: 08/36\n",
            "Training Step: 3787  | total loss: \u001b[1m\u001b[32m0.13583\u001b[0m\u001b[0m | time: 0.008s\n",
            "| Adam | epoch: 758 | loss: 0.13583 - acc: 0.9956 -- iter: 16/36\n",
            "Training Step: 3788  | total loss: \u001b[1m\u001b[32m0.13221\u001b[0m\u001b[0m | time: 0.013s\n",
            "| Adam | epoch: 758 | loss: 0.13221 - acc: 0.9961 -- iter: 24/36\n",
            "Training Step: 3789  | total loss: \u001b[1m\u001b[32m0.12953\u001b[0m\u001b[0m | time: 0.018s\n",
            "| Adam | epoch: 758 | loss: 0.12953 - acc: 0.9965 -- iter: 32/36\n",
            "Training Step: 3790  | total loss: \u001b[1m\u001b[32m0.12498\u001b[0m\u001b[0m | time: 0.022s\n",
            "| Adam | epoch: 758 | loss: 0.12498 - acc: 0.9968 -- iter: 36/36\n",
            "--\n",
            "Training Step: 3791  | total loss: \u001b[1m\u001b[32m0.12260\u001b[0m\u001b[0m | time: 0.004s\n",
            "| Adam | epoch: 759 | loss: 0.12260 - acc: 0.9971 -- iter: 08/36\n",
            "Training Step: 3792  | total loss: \u001b[1m\u001b[32m0.12105\u001b[0m\u001b[0m | time: 0.007s\n",
            "| Adam | epoch: 759 | loss: 0.12105 - acc: 0.9974 -- iter: 16/36\n",
            "Training Step: 3793  | total loss: \u001b[1m\u001b[32m0.11961\u001b[0m\u001b[0m | time: 0.011s\n",
            "| Adam | epoch: 759 | loss: 0.11961 - acc: 0.9977 -- iter: 24/36\n",
            "Training Step: 3794  | total loss: \u001b[1m\u001b[32m0.11320\u001b[0m\u001b[0m | time: 0.016s\n",
            "| Adam | epoch: 759 | loss: 0.11320 - acc: 0.9979 -- iter: 32/36\n",
            "Training Step: 3795  | total loss: \u001b[1m\u001b[32m0.11081\u001b[0m\u001b[0m | time: 0.020s\n",
            "| Adam | epoch: 759 | loss: 0.11081 - acc: 0.9981 -- iter: 36/36\n",
            "--\n",
            "Training Step: 3796  | total loss: \u001b[1m\u001b[32m0.11141\u001b[0m\u001b[0m | time: 0.004s\n",
            "| Adam | epoch: 760 | loss: 0.11141 - acc: 0.9983 -- iter: 08/36\n",
            "Training Step: 3797  | total loss: \u001b[1m\u001b[32m0.10852\u001b[0m\u001b[0m | time: 0.007s\n",
            "| Adam | epoch: 760 | loss: 0.10852 - acc: 0.9985 -- iter: 16/36\n",
            "Training Step: 3798  | total loss: \u001b[1m\u001b[32m0.10249\u001b[0m\u001b[0m | time: 0.015s\n",
            "| Adam | epoch: 760 | loss: 0.10249 - acc: 0.9986 -- iter: 24/36\n",
            "Training Step: 3799  | total loss: \u001b[1m\u001b[32m0.10249\u001b[0m\u001b[0m | time: 0.022s\n",
            "| Adam | epoch: 760 | loss: 0.10249 - acc: 0.9988 -- iter: 32/36\n",
            "Training Step: 3800  | total loss: \u001b[1m\u001b[32m0.10385\u001b[0m\u001b[0m | time: 0.028s\n",
            "| Adam | epoch: 760 | loss: 0.10385 - acc: 0.9990 -- iter: 36/36\n",
            "--\n",
            "Training Step: 3801  | total loss: \u001b[1m\u001b[32m0.09898\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 761 | loss: 0.09898 - acc: 0.8991 -- iter: 08/36\n",
            "Training Step: 3802  | total loss: \u001b[1m\u001b[32m0.84359\u001b[0m\u001b[0m | time: 0.009s\n",
            "| Adam | epoch: 761 | loss: 0.84359 - acc: 0.9092 -- iter: 16/36\n",
            "Training Step: 3803  | total loss: \u001b[1m\u001b[32m0.77078\u001b[0m\u001b[0m | time: 0.014s\n",
            "| Adam | epoch: 761 | loss: 0.77078 - acc: 0.9183 -- iter: 24/36\n",
            "Training Step: 3804  | total loss: \u001b[1m\u001b[32m0.70129\u001b[0m\u001b[0m | time: 0.020s\n",
            "| Adam | epoch: 761 | loss: 0.70129 - acc: 0.9264 -- iter: 32/36\n",
            "Training Step: 3805  | total loss: \u001b[1m\u001b[32m0.63869\u001b[0m\u001b[0m | time: 0.024s\n",
            "| Adam | epoch: 761 | loss: 0.63869 - acc: 0.9338 -- iter: 36/36\n",
            "--\n",
            "Training Step: 3806  | total loss: \u001b[1m\u001b[32m0.58307\u001b[0m\u001b[0m | time: 0.003s\n",
            "| Adam | epoch: 762 | loss: 0.58307 - acc: 0.9404 -- iter: 08/36\n",
            "Training Step: 3807  | total loss: \u001b[1m\u001b[32m0.53393\u001b[0m\u001b[0m | time: 0.010s\n",
            "| Adam | epoch: 762 | loss: 0.53393 - acc: 0.9464 -- iter: 16/36\n",
            "Training Step: 3808  | total loss: \u001b[1m\u001b[32m0.48892\u001b[0m\u001b[0m | time: 0.014s\n",
            "| Adam | epoch: 762 | loss: 0.48892 - acc: 0.9517 -- iter: 24/36\n",
            "Training Step: 3809  | total loss: \u001b[1m\u001b[32m0.44886\u001b[0m\u001b[0m | time: 0.021s\n",
            "| Adam | epoch: 762 | loss: 0.44886 - acc: 0.9566 -- iter: 32/36\n",
            "Training Step: 3810  | total loss: \u001b[1m\u001b[32m0.41682\u001b[0m\u001b[0m | time: 0.026s\n",
            "| Adam | epoch: 762 | loss: 0.41682 - acc: 0.9609 -- iter: 36/36\n",
            "--\n",
            "Training Step: 3811  | total loss: \u001b[1m\u001b[32m0.38800\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 763 | loss: 0.38800 - acc: 0.9648 -- iter: 08/36\n",
            "Training Step: 3812  | total loss: \u001b[1m\u001b[32m0.35955\u001b[0m\u001b[0m | time: 0.009s\n",
            "| Adam | epoch: 763 | loss: 0.35955 - acc: 0.9683 -- iter: 16/36\n",
            "Training Step: 3813  | total loss: \u001b[1m\u001b[32m0.33217\u001b[0m\u001b[0m | time: 0.013s\n",
            "| Adam | epoch: 763 | loss: 0.33217 - acc: 0.9715 -- iter: 24/36\n",
            "Training Step: 3814  | total loss: \u001b[1m\u001b[32m0.30602\u001b[0m\u001b[0m | time: 0.016s\n",
            "| Adam | epoch: 763 | loss: 0.30602 - acc: 0.9744 -- iter: 32/36\n",
            "Training Step: 3815  | total loss: \u001b[1m\u001b[32m0.28339\u001b[0m\u001b[0m | time: 0.021s\n",
            "| Adam | epoch: 763 | loss: 0.28339 - acc: 0.9769 -- iter: 36/36\n",
            "--\n",
            "Training Step: 3816  | total loss: \u001b[1m\u001b[32m0.26753\u001b[0m\u001b[0m | time: 0.004s\n",
            "| Adam | epoch: 764 | loss: 0.26753 - acc: 0.9792 -- iter: 08/36\n",
            "Training Step: 3817  | total loss: \u001b[1m\u001b[32m0.25324\u001b[0m\u001b[0m | time: 0.009s\n",
            "| Adam | epoch: 764 | loss: 0.25324 - acc: 0.9813 -- iter: 16/36\n",
            "Training Step: 3818  | total loss: \u001b[1m\u001b[32m0.23743\u001b[0m\u001b[0m | time: 0.012s\n",
            "| Adam | epoch: 764 | loss: 0.23743 - acc: 0.9832 -- iter: 24/36\n",
            "Training Step: 3819  | total loss: \u001b[1m\u001b[32m0.22336\u001b[0m\u001b[0m | time: 0.018s\n",
            "| Adam | epoch: 764 | loss: 0.22336 - acc: 0.9849 -- iter: 32/36\n",
            "Training Step: 3820  | total loss: \u001b[1m\u001b[32m0.20874\u001b[0m\u001b[0m | time: 0.024s\n",
            "| Adam | epoch: 764 | loss: 0.20874 - acc: 0.9864 -- iter: 36/36\n",
            "--\n",
            "Training Step: 3821  | total loss: \u001b[1m\u001b[32m0.19623\u001b[0m\u001b[0m | time: 0.004s\n",
            "| Adam | epoch: 765 | loss: 0.19623 - acc: 0.9877 -- iter: 08/36\n",
            "Training Step: 3822  | total loss: \u001b[1m\u001b[32m0.18496\u001b[0m\u001b[0m | time: 0.007s\n",
            "| Adam | epoch: 765 | loss: 0.18496 - acc: 0.9890 -- iter: 16/36\n",
            "Training Step: 3823  | total loss: \u001b[1m\u001b[32m0.17477\u001b[0m\u001b[0m | time: 0.012s\n",
            "| Adam | epoch: 765 | loss: 0.17477 - acc: 0.9890 -- iter: 24/36\n",
            "Training Step: 3824  | total loss: \u001b[1m\u001b[32m0.16876\u001b[0m\u001b[0m | time: 0.015s\n",
            "| Adam | epoch: 765 | loss: 0.16876 - acc: 0.9901 -- iter: 32/36\n",
            "Training Step: 3825  | total loss: \u001b[1m\u001b[32m0.15951\u001b[0m\u001b[0m | time: 0.020s\n",
            "| Adam | epoch: 765 | loss: 0.15951 - acc: 0.9911 -- iter: 36/36\n",
            "--\n",
            "Training Step: 3826  | total loss: \u001b[1m\u001b[32m0.15267\u001b[0m\u001b[0m | time: 0.004s\n",
            "| Adam | epoch: 766 | loss: 0.15267 - acc: 0.9920 -- iter: 08/36\n",
            "Training Step: 3827  | total loss: \u001b[1m\u001b[32m0.14822\u001b[0m\u001b[0m | time: 0.009s\n",
            "| Adam | epoch: 766 | loss: 0.14822 - acc: 0.9928 -- iter: 16/36\n",
            "Training Step: 3828  | total loss: \u001b[1m\u001b[32m0.14191\u001b[0m\u001b[0m | time: 0.014s\n",
            "| Adam | epoch: 766 | loss: 0.14191 - acc: 0.9935 -- iter: 24/36\n",
            "Training Step: 3829  | total loss: \u001b[1m\u001b[32m0.13621\u001b[0m\u001b[0m | time: 0.018s\n",
            "| Adam | epoch: 766 | loss: 0.13621 - acc: 0.9941 -- iter: 32/36\n",
            "Training Step: 3830  | total loss: \u001b[1m\u001b[32m0.13208\u001b[0m\u001b[0m | time: 0.022s\n",
            "| Adam | epoch: 766 | loss: 0.13208 - acc: 0.9947 -- iter: 36/36\n",
            "--\n",
            "Training Step: 3831  | total loss: \u001b[1m\u001b[32m0.12823\u001b[0m\u001b[0m | time: 0.004s\n",
            "| Adam | epoch: 767 | loss: 0.12823 - acc: 0.9952 -- iter: 08/36\n",
            "Training Step: 3832  | total loss: \u001b[1m\u001b[32m1.21323\u001b[0m\u001b[0m | time: 0.008s\n",
            "| Adam | epoch: 767 | loss: 1.21323 - acc: 0.9062 -- iter: 16/36\n",
            "Training Step: 3833  | total loss: \u001b[1m\u001b[32m1.09965\u001b[0m\u001b[0m | time: 0.012s\n",
            "| Adam | epoch: 767 | loss: 1.09965 - acc: 0.9062 -- iter: 24/36\n",
            "Training Step: 3834  | total loss: \u001b[1m\u001b[32m0.99883\u001b[0m\u001b[0m | time: 0.016s\n",
            "| Adam | epoch: 767 | loss: 0.99883 - acc: 0.9155 -- iter: 32/36\n",
            "Training Step: 3835  | total loss: \u001b[1m\u001b[32m0.82399\u001b[0m\u001b[0m | time: 0.021s\n",
            "| Adam | epoch: 767 | loss: 0.82399 - acc: 0.9240 -- iter: 36/36\n",
            "--\n",
            "Training Step: 3836  | total loss: \u001b[1m\u001b[32m0.75321\u001b[0m\u001b[0m | time: 0.004s\n",
            "| Adam | epoch: 768 | loss: 0.75321 - acc: 0.9316 -- iter: 08/36\n",
            "Training Step: 3837  | total loss: \u001b[1m\u001b[32m0.68753\u001b[0m\u001b[0m | time: 0.008s\n",
            "| Adam | epoch: 768 | loss: 0.68753 - acc: 0.9384 -- iter: 16/36\n",
            "Training Step: 3838  | total loss: \u001b[1m\u001b[32m0.62470\u001b[0m\u001b[0m | time: 0.012s\n",
            "| Adam | epoch: 768 | loss: 0.62470 - acc: 0.9446 -- iter: 24/36\n",
            "Training Step: 3839  | total loss: \u001b[1m\u001b[32m0.57114\u001b[0m\u001b[0m | time: 0.016s\n",
            "| Adam | epoch: 768 | loss: 0.57114 - acc: 0.9501 -- iter: 32/36\n",
            "Training Step: 3840  | total loss: \u001b[1m\u001b[32m0.52293\u001b[0m\u001b[0m | time: 0.021s\n",
            "| Adam | epoch: 768 | loss: 0.52293 - acc: 0.9551 -- iter: 36/36\n",
            "--\n",
            "Training Step: 3841  | total loss: \u001b[1m\u001b[32m0.47989\u001b[0m\u001b[0m | time: 0.004s\n",
            "| Adam | epoch: 769 | loss: 0.47989 - acc: 0.9636 -- iter: 08/36\n",
            "Training Step: 3842  | total loss: \u001b[1m\u001b[32m0.44079\u001b[0m\u001b[0m | time: 0.008s\n",
            "| Adam | epoch: 769 | loss: 0.44079 - acc: 0.9673 -- iter: 16/36\n",
            "Training Step: 3843  | total loss: \u001b[1m\u001b[32m0.40869\u001b[0m\u001b[0m | time: 0.013s\n",
            "| Adam | epoch: 769 | loss: 0.40869 - acc: 0.9673 -- iter: 24/36\n",
            "Training Step: 3844  | total loss: \u001b[1m\u001b[32m0.37798\u001b[0m\u001b[0m | time: 0.017s\n",
            "| Adam | epoch: 769 | loss: 0.37798 - acc: 0.9705 -- iter: 32/36\n",
            "Training Step: 3845  | total loss: \u001b[1m\u001b[32m0.34893\u001b[0m\u001b[0m | time: 0.022s\n",
            "| Adam | epoch: 769 | loss: 0.34893 - acc: 0.9735 -- iter: 36/36\n",
            "--\n",
            "Training Step: 3846  | total loss: \u001b[1m\u001b[32m0.32275\u001b[0m\u001b[0m | time: 0.004s\n",
            "| Adam | epoch: 770 | loss: 0.32275 - acc: 0.9761 -- iter: 08/36\n",
            "Training Step: 3847  | total loss: \u001b[1m\u001b[32m0.30098\u001b[0m\u001b[0m | time: 0.008s\n",
            "| Adam | epoch: 770 | loss: 0.30098 - acc: 0.9785 -- iter: 16/36\n",
            "Training Step: 3848  | total loss: \u001b[1m\u001b[32m0.27896\u001b[0m\u001b[0m | time: 0.012s\n",
            "| Adam | epoch: 770 | loss: 0.27896 - acc: 0.9807 -- iter: 24/36\n",
            "Training Step: 3849  | total loss: \u001b[1m\u001b[32m0.25848\u001b[0m\u001b[0m | time: 0.021s\n",
            "| Adam | epoch: 770 | loss: 0.25848 - acc: 0.9826 -- iter: 32/36\n",
            "Training Step: 3850  | total loss: \u001b[1m\u001b[32m0.23955\u001b[0m\u001b[0m | time: 0.025s\n",
            "| Adam | epoch: 770 | loss: 0.23955 - acc: 0.9843 -- iter: 36/36\n",
            "--\n",
            "Training Step: 3851  | total loss: \u001b[1m\u001b[32m0.22775\u001b[0m\u001b[0m | time: 0.003s\n",
            "| Adam | epoch: 771 | loss: 0.22775 - acc: 0.9859 -- iter: 08/36\n",
            "Training Step: 3852  | total loss: \u001b[1m\u001b[32m0.21710\u001b[0m\u001b[0m | time: 0.008s\n",
            "| Adam | epoch: 771 | loss: 0.21710 - acc: 0.9873 -- iter: 16/36\n",
            "Training Step: 3853  | total loss: \u001b[1m\u001b[32m0.20440\u001b[0m\u001b[0m | time: 0.012s\n",
            "| Adam | epoch: 771 | loss: 0.20440 - acc: 0.9897 -- iter: 24/36\n",
            "Training Step: 3854  | total loss: \u001b[1m\u001b[32m0.19382\u001b[0m\u001b[0m | time: 0.017s\n",
            "| Adam | epoch: 771 | loss: 0.19382 - acc: 0.9908 -- iter: 32/36\n",
            "Training Step: 3855  | total loss: \u001b[1m\u001b[32m0.18262\u001b[0m\u001b[0m | time: 0.021s\n",
            "| Adam | epoch: 771 | loss: 0.18262 - acc: 0.9917 -- iter: 36/36\n",
            "--\n",
            "Training Step: 3856  | total loss: \u001b[1m\u001b[32m0.17225\u001b[0m\u001b[0m | time: 0.003s\n",
            "| Adam | epoch: 772 | loss: 0.17225 - acc: 0.9925 -- iter: 08/36\n",
            "Training Step: 3857  | total loss: \u001b[1m\u001b[32m0.16906\u001b[0m\u001b[0m | time: 0.008s\n",
            "| Adam | epoch: 772 | loss: 0.16906 - acc: 0.9933 -- iter: 16/36\n",
            "Training Step: 3858  | total loss: \u001b[1m\u001b[32m0.16610\u001b[0m\u001b[0m | time: 0.014s\n",
            "| Adam | epoch: 772 | loss: 0.16610 - acc: 0.9939 -- iter: 24/36\n",
            "Training Step: 3859  | total loss: \u001b[1m\u001b[32m0.15829\u001b[0m\u001b[0m | time: 0.018s\n",
            "| Adam | epoch: 772 | loss: 0.15829 - acc: 0.9945 -- iter: 32/36\n",
            "Training Step: 3860  | total loss: \u001b[1m\u001b[32m0.15087\u001b[0m\u001b[0m | time: 0.022s\n",
            "| Adam | epoch: 772 | loss: 0.15087 - acc: 0.9951 -- iter: 36/36\n",
            "--\n",
            "Training Step: 3861  | total loss: \u001b[1m\u001b[32m0.14320\u001b[0m\u001b[0m | time: 0.004s\n",
            "| Adam | epoch: 773 | loss: 0.14320 - acc: 0.9956 -- iter: 08/36\n",
            "Training Step: 3862  | total loss: \u001b[1m\u001b[32m0.13701\u001b[0m\u001b[0m | time: 0.008s\n",
            "| Adam | epoch: 773 | loss: 0.13701 - acc: 0.9960 -- iter: 16/36\n",
            "Training Step: 3863  | total loss: \u001b[1m\u001b[32m0.13048\u001b[0m\u001b[0m | time: 0.012s\n",
            "| Adam | epoch: 773 | loss: 0.13048 - acc: 0.9964 -- iter: 24/36\n",
            "Training Step: 3864  | total loss: \u001b[1m\u001b[32m0.12457\u001b[0m\u001b[0m | time: 0.015s\n",
            "| Adam | epoch: 773 | loss: 0.12457 - acc: 0.9968 -- iter: 32/36\n",
            "Training Step: 3865  | total loss: \u001b[1m\u001b[32m0.12124\u001b[0m\u001b[0m | time: 0.019s\n",
            "| Adam | epoch: 773 | loss: 0.12124 - acc: 0.9971 -- iter: 36/36\n",
            "--\n",
            "Training Step: 3866  | total loss: \u001b[1m\u001b[32m0.11954\u001b[0m\u001b[0m | time: 0.003s\n",
            "| Adam | epoch: 774 | loss: 0.11954 - acc: 0.9974 -- iter: 08/36\n",
            "Training Step: 3867  | total loss: \u001b[1m\u001b[32m0.11523\u001b[0m\u001b[0m | time: 0.008s\n",
            "| Adam | epoch: 774 | loss: 0.11523 - acc: 0.9977 -- iter: 16/36\n",
            "Training Step: 3868  | total loss: \u001b[1m\u001b[32m0.11523\u001b[0m\u001b[0m | time: 0.011s\n",
            "| Adam | epoch: 774 | loss: 0.11523 - acc: 0.9977 -- iter: 24/36\n",
            "Training Step: 3869  | total loss: \u001b[1m\u001b[32m0.11282\u001b[0m\u001b[0m | time: 0.018s\n",
            "| Adam | epoch: 774 | loss: 0.11282 - acc: 0.9979 -- iter: 32/36\n",
            "Training Step: 3870  | total loss: \u001b[1m\u001b[32m0.11259\u001b[0m\u001b[0m | time: 0.025s\n",
            "| Adam | epoch: 774 | loss: 0.11259 - acc: 0.9981 -- iter: 36/36\n",
            "--\n",
            "Training Step: 3871  | total loss: \u001b[1m\u001b[32m0.10872\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 775 | loss: 0.10872 - acc: 0.9985 -- iter: 08/36\n",
            "Training Step: 3872  | total loss: \u001b[1m\u001b[32m0.10722\u001b[0m\u001b[0m | time: 0.010s\n",
            "| Adam | epoch: 775 | loss: 0.10722 - acc: 0.9986 -- iter: 16/36\n",
            "Training Step: 3873  | total loss: \u001b[1m\u001b[32m0.10315\u001b[0m\u001b[0m | time: 0.014s\n",
            "| Adam | epoch: 775 | loss: 0.10315 - acc: 0.9988 -- iter: 24/36\n",
            "Training Step: 3874  | total loss: \u001b[1m\u001b[32m0.10053\u001b[0m\u001b[0m | time: 0.019s\n",
            "| Adam | epoch: 775 | loss: 0.10053 - acc: 0.9989 -- iter: 32/36\n",
            "Training Step: 3875  | total loss: \u001b[1m\u001b[32m0.09705\u001b[0m\u001b[0m | time: 0.022s\n",
            "| Adam | epoch: 775 | loss: 0.09705 - acc: 0.9990 -- iter: 36/36\n",
            "--\n",
            "Training Step: 3876  | total loss: \u001b[1m\u001b[32m0.09390\u001b[0m\u001b[0m | time: 0.003s\n",
            "| Adam | epoch: 776 | loss: 0.09390 - acc: 0.9991 -- iter: 08/36\n",
            "Training Step: 3877  | total loss: \u001b[1m\u001b[32m0.09148\u001b[0m\u001b[0m | time: 0.007s\n",
            "| Adam | epoch: 776 | loss: 0.09148 - acc: 0.9992 -- iter: 16/36\n",
            "Training Step: 3878  | total loss: \u001b[1m\u001b[32m0.09170\u001b[0m\u001b[0m | time: 0.011s\n",
            "| Adam | epoch: 776 | loss: 0.09170 - acc: 0.9993 -- iter: 24/36\n",
            "Training Step: 3879  | total loss: \u001b[1m\u001b[32m0.09283\u001b[0m\u001b[0m | time: 0.014s\n",
            "| Adam | epoch: 776 | loss: 0.09283 - acc: 0.9993 -- iter: 32/36\n",
            "Training Step: 3880  | total loss: \u001b[1m\u001b[32m0.09436\u001b[0m\u001b[0m | time: 0.017s\n",
            "| Adam | epoch: 776 | loss: 0.09436 - acc: 0.9994 -- iter: 36/36\n",
            "--\n",
            "Training Step: 3881  | total loss: \u001b[1m\u001b[32m0.09378\u001b[0m\u001b[0m | time: 0.004s\n",
            "| Adam | epoch: 777 | loss: 0.09378 - acc: 0.9995 -- iter: 08/36\n",
            "Training Step: 3882  | total loss: \u001b[1m\u001b[32m0.09321\u001b[0m\u001b[0m | time: 0.007s\n",
            "| Adam | epoch: 777 | loss: 0.09321 - acc: 0.9995 -- iter: 16/36\n",
            "Training Step: 3883  | total loss: \u001b[1m\u001b[32m0.09097\u001b[0m\u001b[0m | time: 0.011s\n",
            "| Adam | epoch: 777 | loss: 0.09097 - acc: 0.9996 -- iter: 24/36\n",
            "Training Step: 3884  | total loss: \u001b[1m\u001b[32m0.09102\u001b[0m\u001b[0m | time: 0.016s\n",
            "| Adam | epoch: 777 | loss: 0.09102 - acc: 0.9996 -- iter: 32/36\n",
            "Training Step: 3885  | total loss: \u001b[1m\u001b[32m0.08758\u001b[0m\u001b[0m | time: 0.022s\n",
            "| Adam | epoch: 777 | loss: 0.08758 - acc: 0.9996 -- iter: 36/36\n",
            "--\n",
            "Training Step: 3886  | total loss: \u001b[1m\u001b[32m0.08722\u001b[0m\u001b[0m | time: 0.004s\n",
            "| Adam | epoch: 778 | loss: 0.08722 - acc: 0.9997 -- iter: 08/36\n",
            "Training Step: 3887  | total loss: \u001b[1m\u001b[32m0.08744\u001b[0m\u001b[0m | time: 0.011s\n",
            "| Adam | epoch: 778 | loss: 0.08744 - acc: 0.9997 -- iter: 16/36\n",
            "Training Step: 3888  | total loss: \u001b[1m\u001b[32m0.08744\u001b[0m\u001b[0m | time: 0.017s\n",
            "| Adam | epoch: 778 | loss: 0.08744 - acc: 0.9997 -- iter: 24/36\n",
            "Training Step: 3889  | total loss: \u001b[1m\u001b[32m0.08612\u001b[0m\u001b[0m | time: 0.024s\n",
            "| Adam | epoch: 778 | loss: 0.08612 - acc: 0.9997 -- iter: 32/36\n",
            "Training Step: 3890  | total loss: \u001b[1m\u001b[32m0.08612\u001b[0m\u001b[0m | time: 0.028s\n",
            "| Adam | epoch: 778 | loss: 0.08612 - acc: 0.9998 -- iter: 36/36\n",
            "--\n",
            "Training Step: 3891  | total loss: \u001b[1m\u001b[32m0.08823\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 779 | loss: 0.08823 - acc: 0.9998 -- iter: 08/36\n",
            "Training Step: 3892  | total loss: \u001b[1m\u001b[32m0.08505\u001b[0m\u001b[0m | time: 0.010s\n",
            "| Adam | epoch: 779 | loss: 0.08505 - acc: 0.9998 -- iter: 16/36\n",
            "Training Step: 3893  | total loss: \u001b[1m\u001b[32m0.08448\u001b[0m\u001b[0m | time: 0.015s\n",
            "| Adam | epoch: 779 | loss: 0.08448 - acc: 0.9998 -- iter: 24/36\n",
            "Training Step: 3894  | total loss: \u001b[1m\u001b[32m0.08128\u001b[0m\u001b[0m | time: 0.019s\n",
            "| Adam | epoch: 779 | loss: 0.08128 - acc: 0.9998 -- iter: 32/36\n",
            "Training Step: 3895  | total loss: \u001b[1m\u001b[32m0.07835\u001b[0m\u001b[0m | time: 0.024s\n",
            "| Adam | epoch: 779 | loss: 0.07835 - acc: 0.9999 -- iter: 36/36\n",
            "--\n",
            "Training Step: 3896  | total loss: \u001b[1m\u001b[32m0.08057\u001b[0m\u001b[0m | time: 0.007s\n",
            "| Adam | epoch: 780 | loss: 0.08057 - acc: 0.9999 -- iter: 08/36\n",
            "Training Step: 3897  | total loss: \u001b[1m\u001b[32m0.07981\u001b[0m\u001b[0m | time: 0.016s\n",
            "| Adam | epoch: 780 | loss: 0.07981 - acc: 0.9999 -- iter: 16/36\n",
            "Training Step: 3898  | total loss: \u001b[1m\u001b[32m0.07982\u001b[0m\u001b[0m | time: 0.025s\n",
            "| Adam | epoch: 780 | loss: 0.07982 - acc: 0.9999 -- iter: 24/36\n",
            "Training Step: 3899  | total loss: \u001b[1m\u001b[32m0.08110\u001b[0m\u001b[0m | time: 0.029s\n",
            "| Adam | epoch: 780 | loss: 0.08110 - acc: 0.9999 -- iter: 32/36\n",
            "Training Step: 3900  | total loss: \u001b[1m\u001b[32m0.08220\u001b[0m\u001b[0m | time: 0.034s\n",
            "| Adam | epoch: 780 | loss: 0.08220 - acc: 0.9999 -- iter: 36/36\n",
            "--\n",
            "Training Step: 3901  | total loss: \u001b[1m\u001b[32m0.08311\u001b[0m\u001b[0m | time: 0.006s\n",
            "| Adam | epoch: 781 | loss: 0.08311 - acc: 0.9999 -- iter: 08/36\n",
            "Training Step: 3902  | total loss: \u001b[1m\u001b[32m0.08232\u001b[0m\u001b[0m | time: 0.011s\n",
            "| Adam | epoch: 781 | loss: 0.08232 - acc: 0.9999 -- iter: 16/36\n",
            "Training Step: 3903  | total loss: \u001b[1m\u001b[32m0.08022\u001b[0m\u001b[0m | time: 0.017s\n",
            "| Adam | epoch: 781 | loss: 0.08022 - acc: 0.9999 -- iter: 24/36\n",
            "Training Step: 3904  | total loss: \u001b[1m\u001b[32m0.07848\u001b[0m\u001b[0m | time: 0.023s\n",
            "| Adam | epoch: 781 | loss: 0.07848 - acc: 0.9999 -- iter: 32/36\n",
            "Training Step: 3905  | total loss: \u001b[1m\u001b[32m0.07815\u001b[0m\u001b[0m | time: 0.028s\n",
            "| Adam | epoch: 781 | loss: 0.07815 - acc: 1.0000 -- iter: 36/36\n",
            "--\n",
            "Training Step: 3906  | total loss: \u001b[1m\u001b[32m0.07782\u001b[0m\u001b[0m | time: 0.004s\n",
            "| Adam | epoch: 782 | loss: 0.07782 - acc: 1.0000 -- iter: 08/36\n",
            "Training Step: 3907  | total loss: \u001b[1m\u001b[32m0.07679\u001b[0m\u001b[0m | time: 0.008s\n",
            "| Adam | epoch: 782 | loss: 0.07679 - acc: 1.0000 -- iter: 16/36\n",
            "Training Step: 3908  | total loss: \u001b[1m\u001b[32m0.07881\u001b[0m\u001b[0m | time: 0.013s\n",
            "| Adam | epoch: 782 | loss: 0.07881 - acc: 1.0000 -- iter: 24/36\n",
            "Training Step: 3909  | total loss: \u001b[1m\u001b[32m0.07927\u001b[0m\u001b[0m | time: 0.019s\n",
            "| Adam | epoch: 782 | loss: 0.07927 - acc: 1.0000 -- iter: 32/36\n",
            "Training Step: 3910  | total loss: \u001b[1m\u001b[32m0.07657\u001b[0m\u001b[0m | time: 0.024s\n",
            "| Adam | epoch: 782 | loss: 0.07657 - acc: 1.0000 -- iter: 36/36\n",
            "--\n",
            "Training Step: 3911  | total loss: \u001b[1m\u001b[32m0.07834\u001b[0m\u001b[0m | time: 0.004s\n",
            "| Adam | epoch: 783 | loss: 0.07834 - acc: 1.0000 -- iter: 08/36\n",
            "Training Step: 3912  | total loss: \u001b[1m\u001b[32m0.07986\u001b[0m\u001b[0m | time: 0.009s\n",
            "| Adam | epoch: 783 | loss: 0.07986 - acc: 1.0000 -- iter: 16/36\n",
            "Training Step: 3913  | total loss: \u001b[1m\u001b[32m0.08057\u001b[0m\u001b[0m | time: 0.013s\n",
            "| Adam | epoch: 783 | loss: 0.08057 - acc: 1.0000 -- iter: 24/36\n",
            "Training Step: 3914  | total loss: \u001b[1m\u001b[32m0.08114\u001b[0m\u001b[0m | time: 0.018s\n",
            "| Adam | epoch: 783 | loss: 0.08114 - acc: 1.0000 -- iter: 32/36\n",
            "Training Step: 3915  | total loss: \u001b[1m\u001b[32m0.08014\u001b[0m\u001b[0m | time: 0.022s\n",
            "| Adam | epoch: 783 | loss: 0.08014 - acc: 1.0000 -- iter: 36/36\n",
            "--\n",
            "Training Step: 3916  | total loss: \u001b[1m\u001b[32m0.07888\u001b[0m\u001b[0m | time: 0.004s\n",
            "| Adam | epoch: 784 | loss: 0.07888 - acc: 1.0000 -- iter: 08/36\n",
            "Training Step: 3917  | total loss: \u001b[1m\u001b[32m0.08163\u001b[0m\u001b[0m | time: 0.009s\n",
            "| Adam | epoch: 784 | loss: 0.08163 - acc: 1.0000 -- iter: 16/36\n",
            "Training Step: 3918  | total loss: \u001b[1m\u001b[32m0.08405\u001b[0m\u001b[0m | time: 0.013s\n",
            "| Adam | epoch: 784 | loss: 0.08405 - acc: 1.0000 -- iter: 24/36\n",
            "Training Step: 3919  | total loss: \u001b[1m\u001b[32m0.08405\u001b[0m\u001b[0m | time: 0.018s\n",
            "| Adam | epoch: 784 | loss: 0.08405 - acc: 1.0000 -- iter: 32/36\n",
            "Training Step: 3920  | total loss: \u001b[1m\u001b[32m0.08318\u001b[0m\u001b[0m | time: 0.022s\n",
            "| Adam | epoch: 784 | loss: 0.08318 - acc: 1.0000 -- iter: 36/36\n",
            "--\n",
            "Training Step: 3921  | total loss: \u001b[1m\u001b[32m0.08098\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 785 | loss: 0.08098 - acc: 1.0000 -- iter: 08/36\n",
            "Training Step: 3922  | total loss: \u001b[1m\u001b[32m0.08099\u001b[0m\u001b[0m | time: 0.009s\n",
            "| Adam | epoch: 785 | loss: 0.08099 - acc: 1.0000 -- iter: 16/36\n",
            "Training Step: 3923  | total loss: \u001b[1m\u001b[32m0.07907\u001b[0m\u001b[0m | time: 0.013s\n",
            "| Adam | epoch: 785 | loss: 0.07907 - acc: 1.0000 -- iter: 24/36\n",
            "Training Step: 3924  | total loss: \u001b[1m\u001b[32m0.08241\u001b[0m\u001b[0m | time: 0.019s\n",
            "| Adam | epoch: 785 | loss: 0.08241 - acc: 1.0000 -- iter: 32/36\n",
            "Training Step: 3925  | total loss: \u001b[1m\u001b[32m0.08533\u001b[0m\u001b[0m | time: 0.024s\n",
            "| Adam | epoch: 785 | loss: 0.08533 - acc: 1.0000 -- iter: 36/36\n",
            "--\n",
            "Training Step: 3926  | total loss: \u001b[1m\u001b[32m0.08249\u001b[0m\u001b[0m | time: 0.004s\n",
            "| Adam | epoch: 786 | loss: 0.08249 - acc: 1.0000 -- iter: 08/36\n",
            "Training Step: 3927  | total loss: \u001b[1m\u001b[32m0.08158\u001b[0m\u001b[0m | time: 0.010s\n",
            "| Adam | epoch: 786 | loss: 0.08158 - acc: 1.0000 -- iter: 16/36\n",
            "Training Step: 3928  | total loss: \u001b[1m\u001b[32m0.08201\u001b[0m\u001b[0m | time: 0.015s\n",
            "| Adam | epoch: 786 | loss: 0.08201 - acc: 1.0000 -- iter: 24/36\n",
            "Training Step: 3929  | total loss: \u001b[1m\u001b[32m0.08248\u001b[0m\u001b[0m | time: 0.018s\n",
            "| Adam | epoch: 786 | loss: 0.08248 - acc: 1.0000 -- iter: 32/36\n",
            "Training Step: 3930  | total loss: \u001b[1m\u001b[32m0.07961\u001b[0m\u001b[0m | time: 0.022s\n",
            "| Adam | epoch: 786 | loss: 0.07961 - acc: 1.0000 -- iter: 36/36\n",
            "--\n",
            "Training Step: 3931  | total loss: \u001b[1m\u001b[32m0.07701\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 787 | loss: 0.07701 - acc: 1.0000 -- iter: 08/36\n",
            "Training Step: 3932  | total loss: \u001b[1m\u001b[32m0.07558\u001b[0m\u001b[0m | time: 0.011s\n",
            "| Adam | epoch: 787 | loss: 0.07558 - acc: 1.0000 -- iter: 16/36\n",
            "Training Step: 3933  | total loss: \u001b[1m\u001b[32m0.07465\u001b[0m\u001b[0m | time: 0.014s\n",
            "| Adam | epoch: 787 | loss: 0.07465 - acc: 1.0000 -- iter: 24/36\n",
            "Training Step: 3934  | total loss: \u001b[1m\u001b[32m0.07575\u001b[0m\u001b[0m | time: 0.017s\n",
            "| Adam | epoch: 787 | loss: 0.07575 - acc: 1.0000 -- iter: 32/36\n",
            "Training Step: 3935  | total loss: \u001b[1m\u001b[32m0.07449\u001b[0m\u001b[0m | time: 0.021s\n",
            "| Adam | epoch: 787 | loss: 0.07449 - acc: 1.0000 -- iter: 36/36\n",
            "--\n",
            "Training Step: 3936  | total loss: \u001b[1m\u001b[32m0.07956\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 788 | loss: 0.07956 - acc: 1.0000 -- iter: 08/36\n",
            "Training Step: 3937  | total loss: \u001b[1m\u001b[32m0.08406\u001b[0m\u001b[0m | time: 0.009s\n",
            "| Adam | epoch: 788 | loss: 0.08406 - acc: 1.0000 -- iter: 16/36\n",
            "Training Step: 3938  | total loss: \u001b[1m\u001b[32m0.08236\u001b[0m\u001b[0m | time: 0.013s\n",
            "| Adam | epoch: 788 | loss: 0.08236 - acc: 1.0000 -- iter: 24/36\n",
            "Training Step: 3939  | total loss: \u001b[1m\u001b[32m0.08111\u001b[0m\u001b[0m | time: 0.016s\n",
            "| Adam | epoch: 788 | loss: 0.08111 - acc: 1.0000 -- iter: 32/36\n",
            "Training Step: 3940  | total loss: \u001b[1m\u001b[32m0.07915\u001b[0m\u001b[0m | time: 0.019s\n",
            "| Adam | epoch: 788 | loss: 0.07915 - acc: 1.0000 -- iter: 36/36\n",
            "--\n",
            "Training Step: 3941  | total loss: \u001b[1m\u001b[32m0.07890\u001b[0m\u001b[0m | time: 0.003s\n",
            "| Adam | epoch: 789 | loss: 0.07890 - acc: 1.0000 -- iter: 08/36\n",
            "Training Step: 3942  | total loss: \u001b[1m\u001b[32m0.08041\u001b[0m\u001b[0m | time: 0.006s\n",
            "| Adam | epoch: 789 | loss: 0.08041 - acc: 1.0000 -- iter: 16/36\n",
            "Training Step: 3943  | total loss: \u001b[1m\u001b[32m0.08173\u001b[0m\u001b[0m | time: 0.013s\n",
            "| Adam | epoch: 789 | loss: 0.08173 - acc: 1.0000 -- iter: 24/36\n",
            "Training Step: 3944  | total loss: \u001b[1m\u001b[32m0.07983\u001b[0m\u001b[0m | time: 0.021s\n",
            "| Adam | epoch: 789 | loss: 0.07983 - acc: 1.0000 -- iter: 32/36\n",
            "Training Step: 3945  | total loss: \u001b[1m\u001b[32m0.07706\u001b[0m\u001b[0m | time: 0.025s\n",
            "| Adam | epoch: 789 | loss: 0.07706 - acc: 1.0000 -- iter: 36/36\n",
            "--\n",
            "Training Step: 3946  | total loss: \u001b[1m\u001b[32m0.07741\u001b[0m\u001b[0m | time: 0.003s\n",
            "| Adam | epoch: 790 | loss: 0.07741 - acc: 1.0000 -- iter: 08/36\n",
            "Training Step: 3947  | total loss: \u001b[1m\u001b[32m0.07673\u001b[0m\u001b[0m | time: 0.006s\n",
            "| Adam | epoch: 790 | loss: 0.07673 - acc: 1.0000 -- iter: 16/36\n",
            "Training Step: 3948  | total loss: \u001b[1m\u001b[32m0.07578\u001b[0m\u001b[0m | time: 0.009s\n",
            "| Adam | epoch: 790 | loss: 0.07578 - acc: 1.0000 -- iter: 24/36\n",
            "Training Step: 3949  | total loss: \u001b[1m\u001b[32m0.07489\u001b[0m\u001b[0m | time: 0.017s\n",
            "| Adam | epoch: 790 | loss: 0.07489 - acc: 1.0000 -- iter: 32/36\n",
            "Training Step: 3950  | total loss: \u001b[1m\u001b[32m0.07319\u001b[0m\u001b[0m | time: 0.037s\n",
            "| Adam | epoch: 790 | loss: 0.07319 - acc: 1.0000 -- iter: 36/36\n",
            "--\n",
            "Training Step: 3951  | total loss: \u001b[1m\u001b[32m0.07359\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 791 | loss: 0.07359 - acc: 1.0000 -- iter: 08/36\n",
            "Training Step: 3952  | total loss: \u001b[1m\u001b[32m0.07368\u001b[0m\u001b[0m | time: 0.010s\n",
            "| Adam | epoch: 791 | loss: 0.07368 - acc: 1.0000 -- iter: 16/36\n",
            "Training Step: 3953  | total loss: \u001b[1m\u001b[32m0.07188\u001b[0m\u001b[0m | time: 0.015s\n",
            "| Adam | epoch: 791 | loss: 0.07188 - acc: 1.0000 -- iter: 24/36\n",
            "Training Step: 3954  | total loss: \u001b[1m\u001b[32m0.07385\u001b[0m\u001b[0m | time: 0.019s\n",
            "| Adam | epoch: 791 | loss: 0.07385 - acc: 1.0000 -- iter: 32/36\n",
            "Training Step: 3955  | total loss: \u001b[1m\u001b[32m0.07559\u001b[0m\u001b[0m | time: 0.025s\n",
            "| Adam | epoch: 791 | loss: 0.07559 - acc: 1.0000 -- iter: 36/36\n",
            "--\n",
            "Training Step: 3956  | total loss: \u001b[1m\u001b[32m0.07572\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 792 | loss: 0.07572 - acc: 1.0000 -- iter: 08/36\n",
            "Training Step: 3957  | total loss: \u001b[1m\u001b[32m0.07475\u001b[0m\u001b[0m | time: 0.009s\n",
            "| Adam | epoch: 792 | loss: 0.07475 - acc: 1.0000 -- iter: 16/36\n",
            "Training Step: 3958  | total loss: \u001b[1m\u001b[32m0.07379\u001b[0m\u001b[0m | time: 0.014s\n",
            "| Adam | epoch: 792 | loss: 0.07379 - acc: 1.0000 -- iter: 24/36\n",
            "Training Step: 3959  | total loss: \u001b[1m\u001b[32m0.07455\u001b[0m\u001b[0m | time: 0.020s\n",
            "| Adam | epoch: 792 | loss: 0.07455 - acc: 1.0000 -- iter: 32/36\n",
            "Training Step: 3960  | total loss: \u001b[1m\u001b[32m0.07206\u001b[0m\u001b[0m | time: 0.025s\n",
            "| Adam | epoch: 792 | loss: 0.07206 - acc: 1.0000 -- iter: 36/36\n",
            "--\n",
            "Training Step: 3961  | total loss: \u001b[1m\u001b[32m0.07097\u001b[0m\u001b[0m | time: 0.008s\n",
            "| Adam | epoch: 793 | loss: 0.07097 - acc: 1.0000 -- iter: 08/36\n",
            "Training Step: 3962  | total loss: \u001b[1m\u001b[32m0.06997\u001b[0m\u001b[0m | time: 0.012s\n",
            "| Adam | epoch: 793 | loss: 0.06997 - acc: 1.0000 -- iter: 16/36\n",
            "Training Step: 3963  | total loss: \u001b[1m\u001b[32m0.93851\u001b[0m\u001b[0m | time: 0.017s\n",
            "| Adam | epoch: 793 | loss: 0.93851 - acc: 0.9250 -- iter: 24/36\n",
            "Training Step: 3964  | total loss: \u001b[1m\u001b[32m0.85081\u001b[0m\u001b[0m | time: 0.024s\n",
            "| Adam | epoch: 793 | loss: 0.85081 - acc: 0.9325 -- iter: 32/36\n",
            "Training Step: 3965  | total loss: \u001b[1m\u001b[32m0.77359\u001b[0m\u001b[0m | time: 0.029s\n",
            "| Adam | epoch: 793 | loss: 0.77359 - acc: 0.9325 -- iter: 36/36\n",
            "--\n",
            "Training Step: 3966  | total loss: \u001b[1m\u001b[32m0.70413\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 794 | loss: 0.70413 - acc: 0.9392 -- iter: 08/36\n",
            "Training Step: 3967  | total loss: \u001b[1m\u001b[32m0.64030\u001b[0m\u001b[0m | time: 0.010s\n",
            "| Adam | epoch: 794 | loss: 0.64030 - acc: 0.9453 -- iter: 16/36\n",
            "Training Step: 3968  | total loss: \u001b[1m\u001b[32m0.58276\u001b[0m\u001b[0m | time: 0.013s\n",
            "| Adam | epoch: 794 | loss: 0.58276 - acc: 0.9508 -- iter: 24/36\n",
            "Training Step: 3969  | total loss: \u001b[1m\u001b[32m0.53202\u001b[0m\u001b[0m | time: 0.017s\n",
            "| Adam | epoch: 794 | loss: 0.53202 - acc: 0.9557 -- iter: 32/36\n",
            "Training Step: 3970  | total loss: \u001b[1m\u001b[32m0.48606\u001b[0m\u001b[0m | time: 0.023s\n",
            "| Adam | epoch: 794 | loss: 0.48606 - acc: 0.9601 -- iter: 36/36\n",
            "--\n",
            "Training Step: 3971  | total loss: \u001b[1m\u001b[32m0.44338\u001b[0m\u001b[0m | time: 0.008s\n",
            "| Adam | epoch: 795 | loss: 0.44338 - acc: 0.9641 -- iter: 08/36\n",
            "Training Step: 3972  | total loss: \u001b[1m\u001b[32m0.40496\u001b[0m\u001b[0m | time: 0.015s\n",
            "| Adam | epoch: 795 | loss: 0.40496 - acc: 0.9677 -- iter: 16/36\n",
            "Training Step: 3973  | total loss: \u001b[1m\u001b[32m0.37085\u001b[0m\u001b[0m | time: 0.020s\n",
            "| Adam | epoch: 795 | loss: 0.37085 - acc: 0.9709 -- iter: 24/36\n",
            "Training Step: 3974  | total loss: \u001b[1m\u001b[32m0.34006\u001b[0m\u001b[0m | time: 0.026s\n",
            "| Adam | epoch: 795 | loss: 0.34006 - acc: 0.9738 -- iter: 32/36\n",
            "Training Step: 3975  | total loss: \u001b[1m\u001b[32m0.31187\u001b[0m\u001b[0m | time: 0.031s\n",
            "| Adam | epoch: 795 | loss: 0.31187 - acc: 0.9765 -- iter: 36/36\n",
            "--\n",
            "Training Step: 3976  | total loss: \u001b[1m\u001b[32m0.28813\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 796 | loss: 0.28813 - acc: 0.9788 -- iter: 08/36\n",
            "Training Step: 3977  | total loss: \u001b[1m\u001b[32m0.26378\u001b[0m\u001b[0m | time: 0.009s\n",
            "| Adam | epoch: 796 | loss: 0.26378 - acc: 0.9809 -- iter: 16/36\n",
            "Training Step: 3978  | total loss: \u001b[1m\u001b[32m0.24185\u001b[0m\u001b[0m | time: 0.014s\n",
            "| Adam | epoch: 796 | loss: 0.24185 - acc: 0.9846 -- iter: 24/36\n",
            "Training Step: 3979  | total loss: \u001b[1m\u001b[32m0.22629\u001b[0m\u001b[0m | time: 0.019s\n",
            "| Adam | epoch: 796 | loss: 0.22629 - acc: 0.9861 -- iter: 32/36\n",
            "Training Step: 3980  | total loss: \u001b[1m\u001b[32m0.21057\u001b[0m\u001b[0m | time: 0.024s\n",
            "| Adam | epoch: 796 | loss: 0.21057 - acc: 0.9875 -- iter: 36/36\n",
            "--\n",
            "Training Step: 3981  | total loss: \u001b[1m\u001b[32m0.19719\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 797 | loss: 0.19719 - acc: 0.9887 -- iter: 08/36\n",
            "Training Step: 3982  | total loss: \u001b[1m\u001b[32m0.18526\u001b[0m\u001b[0m | time: 0.009s\n",
            "| Adam | epoch: 797 | loss: 0.18526 - acc: 0.9899 -- iter: 16/36\n",
            "Training Step: 3983  | total loss: \u001b[1m\u001b[32m0.17328\u001b[0m\u001b[0m | time: 0.014s\n",
            "| Adam | epoch: 797 | loss: 0.17328 - acc: 0.9909 -- iter: 24/36\n",
            "Training Step: 3984  | total loss: \u001b[1m\u001b[32m0.16247\u001b[0m\u001b[0m | time: 0.018s\n",
            "| Adam | epoch: 797 | loss: 0.16247 - acc: 0.9918 -- iter: 32/36\n",
            "Training Step: 3985  | total loss: \u001b[1m\u001b[32m0.15248\u001b[0m\u001b[0m | time: 0.022s\n",
            "| Adam | epoch: 797 | loss: 0.15248 - acc: 0.9926 -- iter: 36/36\n",
            "--\n",
            "Training Step: 3986  | total loss: \u001b[1m\u001b[32m0.14305\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 798 | loss: 0.14305 - acc: 0.9926 -- iter: 08/36\n",
            "Training Step: 3987  | total loss: \u001b[1m\u001b[32m0.13639\u001b[0m\u001b[0m | time: 0.011s\n",
            "| Adam | epoch: 798 | loss: 0.13639 - acc: 0.9934 -- iter: 16/36\n",
            "Training Step: 3988  | total loss: \u001b[1m\u001b[32m0.13639\u001b[0m\u001b[0m | time: 0.019s\n",
            "| Adam | epoch: 798 | loss: 0.13639 - acc: 0.9940 -- iter: 24/36\n",
            "Training Step: 3989  | total loss: \u001b[1m\u001b[32m0.13024\u001b[0m\u001b[0m | time: 0.025s\n",
            "| Adam | epoch: 798 | loss: 0.13024 - acc: 0.9946 -- iter: 32/36\n",
            "Training Step: 3990  | total loss: \u001b[1m\u001b[32m0.12409\u001b[0m\u001b[0m | time: 0.032s\n",
            "| Adam | epoch: 798 | loss: 0.12409 - acc: 0.9952 -- iter: 36/36\n",
            "--\n",
            "Training Step: 3991  | total loss: \u001b[1m\u001b[32m0.11853\u001b[0m\u001b[0m | time: 0.006s\n",
            "| Adam | epoch: 799 | loss: 0.11853 - acc: 0.9956 -- iter: 08/36\n",
            "Training Step: 3992  | total loss: \u001b[1m\u001b[32m0.11370\u001b[0m\u001b[0m | time: 0.013s\n",
            "| Adam | epoch: 799 | loss: 0.11370 - acc: 0.9961 -- iter: 16/36\n",
            "Training Step: 3993  | total loss: \u001b[1m\u001b[32m0.10937\u001b[0m\u001b[0m | time: 0.021s\n",
            "| Adam | epoch: 799 | loss: 0.10937 - acc: 0.9965 -- iter: 24/36\n",
            "Training Step: 3994  | total loss: \u001b[1m\u001b[32m0.10387\u001b[0m\u001b[0m | time: 0.027s\n",
            "| Adam | epoch: 799 | loss: 0.10387 - acc: 0.9968 -- iter: 32/36\n",
            "Training Step: 3995  | total loss: \u001b[1m\u001b[32m0.09945\u001b[0m\u001b[0m | time: 0.034s\n",
            "| Adam | epoch: 799 | loss: 0.09945 - acc: 0.9971 -- iter: 36/36\n",
            "--\n",
            "Training Step: 3996  | total loss: \u001b[1m\u001b[32m0.09739\u001b[0m\u001b[0m | time: 0.006s\n",
            "| Adam | epoch: 800 | loss: 0.09739 - acc: 0.9974 -- iter: 08/36\n",
            "Training Step: 3997  | total loss: \u001b[1m\u001b[32m0.09185\u001b[0m\u001b[0m | time: 0.013s\n",
            "| Adam | epoch: 800 | loss: 0.09185 - acc: 0.9977 -- iter: 16/36\n",
            "Training Step: 3998  | total loss: \u001b[1m\u001b[32m0.09185\u001b[0m\u001b[0m | time: 0.020s\n",
            "| Adam | epoch: 800 | loss: 0.09185 - acc: 0.9979 -- iter: 24/36\n",
            "Training Step: 3999  | total loss: \u001b[1m\u001b[32m0.08783\u001b[0m\u001b[0m | time: 0.025s\n",
            "| Adam | epoch: 800 | loss: 0.08783 - acc: 0.9981 -- iter: 32/36\n",
            "Training Step: 4000  | total loss: \u001b[1m\u001b[32m0.08618\u001b[0m\u001b[0m | time: 0.031s\n",
            "| Adam | epoch: 800 | loss: 0.08618 - acc: 0.9983 -- iter: 36/36\n",
            "--\n",
            "Training Step: 4001  | total loss: \u001b[1m\u001b[32m0.08295\u001b[0m\u001b[0m | time: 0.004s\n",
            "| Adam | epoch: 801 | loss: 0.08295 - acc: 0.9985 -- iter: 08/36\n",
            "Training Step: 4002  | total loss: \u001b[1m\u001b[32m0.08001\u001b[0m\u001b[0m | time: 0.007s\n",
            "| Adam | epoch: 801 | loss: 0.08001 - acc: 0.9986 -- iter: 16/36\n",
            "Training Step: 4003  | total loss: \u001b[1m\u001b[32m0.07848\u001b[0m\u001b[0m | time: 0.012s\n",
            "| Adam | epoch: 801 | loss: 0.07848 - acc: 0.9988 -- iter: 24/36\n",
            "Training Step: 4004  | total loss: \u001b[1m\u001b[32m0.07853\u001b[0m\u001b[0m | time: 0.017s\n",
            "| Adam | epoch: 801 | loss: 0.07853 - acc: 0.9989 -- iter: 32/36\n",
            "Training Step: 4005  | total loss: \u001b[1m\u001b[32m0.07618\u001b[0m\u001b[0m | time: 0.025s\n",
            "| Adam | epoch: 801 | loss: 0.07618 - acc: 0.9990 -- iter: 36/36\n",
            "--\n",
            "Training Step: 4006  | total loss: \u001b[1m\u001b[32m0.07507\u001b[0m\u001b[0m | time: 0.004s\n",
            "| Adam | epoch: 802 | loss: 0.07507 - acc: 0.9991 -- iter: 08/36\n",
            "Training Step: 4007  | total loss: \u001b[1m\u001b[32m0.07309\u001b[0m\u001b[0m | time: 0.010s\n",
            "| Adam | epoch: 802 | loss: 0.07309 - acc: 0.9992 -- iter: 16/36\n",
            "Training Step: 4008  | total loss: \u001b[1m\u001b[32m0.07130\u001b[0m\u001b[0m | time: 0.015s\n",
            "| Adam | epoch: 802 | loss: 0.07130 - acc: 0.9993 -- iter: 24/36\n",
            "Training Step: 4009  | total loss: \u001b[1m\u001b[32m0.07055\u001b[0m\u001b[0m | time: 0.019s\n",
            "| Adam | epoch: 802 | loss: 0.07055 - acc: 0.9993 -- iter: 32/36\n",
            "Training Step: 4010  | total loss: \u001b[1m\u001b[32m0.07055\u001b[0m\u001b[0m | time: 0.025s\n",
            "| Adam | epoch: 802 | loss: 0.07055 - acc: 0.9994 -- iter: 36/36\n",
            "--\n",
            "Training Step: 4011  | total loss: \u001b[1m\u001b[32m0.07004\u001b[0m\u001b[0m | time: 0.004s\n",
            "| Adam | epoch: 803 | loss: 0.07004 - acc: 0.9995 -- iter: 08/36\n",
            "Training Step: 4012  | total loss: \u001b[1m\u001b[32m0.06889\u001b[0m\u001b[0m | time: 0.008s\n",
            "| Adam | epoch: 803 | loss: 0.06889 - acc: 0.9995 -- iter: 16/36\n",
            "Training Step: 4013  | total loss: \u001b[1m\u001b[32m0.06919\u001b[0m\u001b[0m | time: 0.013s\n",
            "| Adam | epoch: 803 | loss: 0.06919 - acc: 0.9996 -- iter: 24/36\n",
            "Training Step: 4014  | total loss: \u001b[1m\u001b[32m0.06943\u001b[0m\u001b[0m | time: 0.018s\n",
            "| Adam | epoch: 803 | loss: 0.06943 - acc: 0.9996 -- iter: 32/36\n",
            "Training Step: 4015  | total loss: \u001b[1m\u001b[32m0.07023\u001b[0m\u001b[0m | time: 0.022s\n",
            "| Adam | epoch: 803 | loss: 0.07023 - acc: 0.9997 -- iter: 36/36\n",
            "--\n",
            "Training Step: 4016  | total loss: \u001b[1m\u001b[32m0.06827\u001b[0m\u001b[0m | time: 0.004s\n",
            "| Adam | epoch: 804 | loss: 0.06827 - acc: 0.9997 -- iter: 08/36\n",
            "Training Step: 4017  | total loss: \u001b[1m\u001b[32m0.06812\u001b[0m\u001b[0m | time: 0.009s\n",
            "| Adam | epoch: 804 | loss: 0.06812 - acc: 0.9997 -- iter: 16/36\n",
            "Training Step: 4018  | total loss: \u001b[1m\u001b[32m0.06698\u001b[0m\u001b[0m | time: 0.014s\n",
            "| Adam | epoch: 804 | loss: 0.06698 - acc: 0.9997 -- iter: 24/36\n",
            "Training Step: 4019  | total loss: \u001b[1m\u001b[32m0.06698\u001b[0m\u001b[0m | time: 0.018s\n",
            "| Adam | epoch: 804 | loss: 0.06698 - acc: 0.9998 -- iter: 32/36\n",
            "Training Step: 4020  | total loss: \u001b[1m\u001b[32m0.06560\u001b[0m\u001b[0m | time: 0.024s\n",
            "| Adam | epoch: 804 | loss: 0.06560 - acc: 0.9998 -- iter: 36/36\n",
            "--\n",
            "Training Step: 4021  | total loss: \u001b[1m\u001b[32m0.06471\u001b[0m\u001b[0m | time: 0.004s\n",
            "| Adam | epoch: 805 | loss: 0.06471 - acc: 0.9998 -- iter: 08/36\n",
            "Training Step: 4022  | total loss: \u001b[1m\u001b[32m0.06488\u001b[0m\u001b[0m | time: 0.008s\n",
            "| Adam | epoch: 805 | loss: 0.06488 - acc: 0.9998 -- iter: 16/36\n",
            "Training Step: 4023  | total loss: \u001b[1m\u001b[32m0.06594\u001b[0m\u001b[0m | time: 0.014s\n",
            "| Adam | epoch: 805 | loss: 0.06594 - acc: 0.9999 -- iter: 24/36\n",
            "Training Step: 4024  | total loss: \u001b[1m\u001b[32m0.06714\u001b[0m\u001b[0m | time: 0.019s\n",
            "| Adam | epoch: 805 | loss: 0.06714 - acc: 0.9999 -- iter: 32/36\n",
            "Training Step: 4025  | total loss: \u001b[1m\u001b[32m0.06547\u001b[0m\u001b[0m | time: 0.023s\n",
            "| Adam | epoch: 805 | loss: 0.06547 - acc: 0.9999 -- iter: 36/36\n",
            "--\n",
            "Training Step: 4026  | total loss: \u001b[1m\u001b[32m0.06394\u001b[0m\u001b[0m | time: 0.004s\n",
            "| Adam | epoch: 806 | loss: 0.06394 - acc: 0.9999 -- iter: 08/36\n",
            "Training Step: 4027  | total loss: \u001b[1m\u001b[32m0.06261\u001b[0m\u001b[0m | time: 0.008s\n",
            "| Adam | epoch: 806 | loss: 0.06261 - acc: 0.9999 -- iter: 16/36\n",
            "Training Step: 4028  | total loss: \u001b[1m\u001b[32m0.06199\u001b[0m\u001b[0m | time: 0.012s\n",
            "| Adam | epoch: 806 | loss: 0.06199 - acc: 0.9999 -- iter: 24/36\n",
            "Training Step: 4029  | total loss: \u001b[1m\u001b[32m0.06293\u001b[0m\u001b[0m | time: 0.017s\n",
            "| Adam | epoch: 806 | loss: 0.06293 - acc: 0.9999 -- iter: 32/36\n",
            "Training Step: 4030  | total loss: \u001b[1m\u001b[32m0.06410\u001b[0m\u001b[0m | time: 0.022s\n",
            "| Adam | epoch: 806 | loss: 0.06410 - acc: 0.9999 -- iter: 36/36\n",
            "--\n",
            "Training Step: 4031  | total loss: \u001b[1m\u001b[32m0.06543\u001b[0m\u001b[0m | time: 0.004s\n",
            "| Adam | epoch: 807 | loss: 0.06543 - acc: 0.9999 -- iter: 08/36\n",
            "Training Step: 4032  | total loss: \u001b[1m\u001b[32m0.06660\u001b[0m\u001b[0m | time: 0.009s\n",
            "| Adam | epoch: 807 | loss: 0.06660 - acc: 0.9999 -- iter: 16/36\n",
            "Training Step: 4033  | total loss: \u001b[1m\u001b[32m0.06554\u001b[0m\u001b[0m | time: 0.014s\n",
            "| Adam | epoch: 807 | loss: 0.06554 - acc: 0.9999 -- iter: 24/36\n",
            "Training Step: 4034  | total loss: \u001b[1m\u001b[32m0.06493\u001b[0m\u001b[0m | time: 0.018s\n",
            "| Adam | epoch: 807 | loss: 0.06493 - acc: 1.0000 -- iter: 32/36\n",
            "Training Step: 4035  | total loss: \u001b[1m\u001b[32m0.06339\u001b[0m\u001b[0m | time: 0.024s\n",
            "| Adam | epoch: 807 | loss: 0.06339 - acc: 1.0000 -- iter: 36/36\n",
            "--\n",
            "Training Step: 4036  | total loss: \u001b[1m\u001b[32m0.06426\u001b[0m\u001b[0m | time: 0.007s\n",
            "| Adam | epoch: 808 | loss: 0.06426 - acc: 1.0000 -- iter: 08/36\n",
            "Training Step: 4037  | total loss: \u001b[1m\u001b[32m0.06304\u001b[0m\u001b[0m | time: 0.012s\n",
            "| Adam | epoch: 808 | loss: 0.06304 - acc: 1.0000 -- iter: 16/36\n",
            "Training Step: 4038  | total loss: \u001b[1m\u001b[32m0.06304\u001b[0m\u001b[0m | time: 0.019s\n",
            "| Adam | epoch: 808 | loss: 0.06304 - acc: 1.0000 -- iter: 24/36\n",
            "Training Step: 4039  | total loss: \u001b[1m\u001b[32m0.06138\u001b[0m\u001b[0m | time: 0.023s\n",
            "| Adam | epoch: 808 | loss: 0.06138 - acc: 1.0000 -- iter: 32/36\n",
            "Training Step: 4040  | total loss: \u001b[1m\u001b[32m0.06146\u001b[0m\u001b[0m | time: 0.030s\n",
            "| Adam | epoch: 808 | loss: 0.06146 - acc: 1.0000 -- iter: 36/36\n",
            "--\n",
            "Training Step: 4041  | total loss: \u001b[1m\u001b[32m0.06106\u001b[0m\u001b[0m | time: 0.008s\n",
            "| Adam | epoch: 809 | loss: 0.06106 - acc: 1.0000 -- iter: 08/36\n",
            "Training Step: 4042  | total loss: \u001b[1m\u001b[32m0.06245\u001b[0m\u001b[0m | time: 0.015s\n",
            "| Adam | epoch: 809 | loss: 0.06245 - acc: 1.0000 -- iter: 16/36\n",
            "Training Step: 4043  | total loss: \u001b[1m\u001b[32m0.06171\u001b[0m\u001b[0m | time: 0.023s\n",
            "| Adam | epoch: 809 | loss: 0.06171 - acc: 1.0000 -- iter: 24/36\n",
            "Training Step: 4044  | total loss: \u001b[1m\u001b[32m0.06171\u001b[0m\u001b[0m | time: 0.034s\n",
            "| Adam | epoch: 809 | loss: 0.06171 - acc: 1.0000 -- iter: 32/36\n",
            "Training Step: 4045  | total loss: \u001b[1m\u001b[32m0.06082\u001b[0m\u001b[0m | time: 0.040s\n",
            "| Adam | epoch: 809 | loss: 0.06082 - acc: 1.0000 -- iter: 36/36\n",
            "--\n",
            "Training Step: 4046  | total loss: \u001b[1m\u001b[32m0.06082\u001b[0m\u001b[0m | time: 0.007s\n",
            "| Adam | epoch: 810 | loss: 0.06082 - acc: 1.0000 -- iter: 08/36\n",
            "Training Step: 4047  | total loss: \u001b[1m\u001b[32m0.05960\u001b[0m\u001b[0m | time: 0.014s\n",
            "| Adam | epoch: 810 | loss: 0.05960 - acc: 1.0000 -- iter: 16/36\n",
            "Training Step: 4232  | total loss: \u001b[1m\u001b[32m0.04348\u001b[0m\u001b[0m | time: 0.011s\n",
            "| Adam | epoch: 847 | loss: 0.04348 - acc: 1.0000 -- iter: 16/36\n",
            "Training Step: 4233  | total loss: \u001b[1m\u001b[32m0.04395\u001b[0m\u001b[0m | time: 0.016s\n",
            "| Adam | epoch: 847 | loss: 0.04395 - acc: 1.0000 -- iter: 24/36\n",
            "Training Step: 4234  | total loss: \u001b[1m\u001b[32m0.04464\u001b[0m\u001b[0m | time: 0.021s\n",
            "| Adam | epoch: 847 | loss: 0.04464 - acc: 1.0000 -- iter: 32/36\n",
            "Training Step: 4235  | total loss: \u001b[1m\u001b[32m0.04369\u001b[0m\u001b[0m | time: 0.025s\n",
            "| Adam | epoch: 847 | loss: 0.04369 - acc: 1.0000 -- iter: 36/36\n",
            "--\n",
            "Training Step: 4236  | total loss: \u001b[1m\u001b[32m0.04282\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 848 | loss: 0.04282 - acc: 1.0000 -- iter: 08/36\n",
            "Training Step: 4237  | total loss: \u001b[1m\u001b[32m0.04282\u001b[0m\u001b[0m | time: 0.009s\n",
            "| Adam | epoch: 848 | loss: 0.04282 - acc: 1.0000 -- iter: 16/36\n",
            "Training Step: 4238  | total loss: \u001b[1m\u001b[32m0.04311\u001b[0m\u001b[0m | time: 0.015s\n",
            "| Adam | epoch: 848 | loss: 0.04311 - acc: 1.0000 -- iter: 24/36\n",
            "Training Step: 4239  | total loss: \u001b[1m\u001b[32m0.04248\u001b[0m\u001b[0m | time: 0.020s\n",
            "| Adam | epoch: 848 | loss: 0.04248 - acc: 1.0000 -- iter: 32/36\n",
            "Training Step: 4240  | total loss: \u001b[1m\u001b[32m0.04264\u001b[0m\u001b[0m | time: 0.023s\n",
            "| Adam | epoch: 848 | loss: 0.04264 - acc: 1.0000 -- iter: 36/36\n",
            "--\n",
            "Training Step: 4241  | total loss: \u001b[1m\u001b[32m0.03999\u001b[0m\u001b[0m | time: 0.003s\n",
            "| Adam | epoch: 849 | loss: 0.03999 - acc: 1.0000 -- iter: 08/36\n",
            "Training Step: 4242  | total loss: \u001b[1m\u001b[32m0.03999\u001b[0m\u001b[0m | time: 0.006s\n",
            "| Adam | epoch: 849 | loss: 0.03999 - acc: 1.0000 -- iter: 16/36\n",
            "Training Step: 4243  | total loss: \u001b[1m\u001b[32m0.03922\u001b[0m\u001b[0m | time: 0.010s\n",
            "| Adam | epoch: 849 | loss: 0.03922 - acc: 1.0000 -- iter: 24/36\n",
            "Training Step: 4244  | total loss: \u001b[1m\u001b[32m0.03924\u001b[0m\u001b[0m | time: 0.015s\n",
            "| Adam | epoch: 849 | loss: 0.03924 - acc: 1.0000 -- iter: 32/36\n",
            "Training Step: 4245  | total loss: \u001b[1m\u001b[32m0.04002\u001b[0m\u001b[0m | time: 0.021s\n",
            "| Adam | epoch: 849 | loss: 0.04002 - acc: 1.0000 -- iter: 36/36\n",
            "--\n",
            "Training Step: 4246  | total loss: \u001b[1m\u001b[32m0.03935\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 850 | loss: 0.03935 - acc: 1.0000 -- iter: 08/36\n",
            "Training Step: 4247  | total loss: \u001b[1m\u001b[32m0.03921\u001b[0m\u001b[0m | time: 0.010s\n",
            "| Adam | epoch: 850 | loss: 0.03921 - acc: 1.0000 -- iter: 16/36\n",
            "Training Step: 4248  | total loss: \u001b[1m\u001b[32m0.03906\u001b[0m\u001b[0m | time: 0.015s\n",
            "| Adam | epoch: 850 | loss: 0.03906 - acc: 1.0000 -- iter: 24/36\n",
            "Training Step: 4249  | total loss: \u001b[1m\u001b[32m0.03860\u001b[0m\u001b[0m | time: 0.020s\n",
            "| Adam | epoch: 850 | loss: 0.03860 - acc: 1.0000 -- iter: 32/36\n",
            "Training Step: 4250  | total loss: \u001b[1m\u001b[32m0.03991\u001b[0m\u001b[0m | time: 0.024s\n",
            "| Adam | epoch: 850 | loss: 0.03991 - acc: 1.0000 -- iter: 36/36\n",
            "--\n",
            "Training Step: 4251  | total loss: \u001b[1m\u001b[32m0.04112\u001b[0m\u001b[0m | time: 0.006s\n",
            "| Adam | epoch: 851 | loss: 0.04112 - acc: 1.0000 -- iter: 08/36\n",
            "Training Step: 4252  | total loss: \u001b[1m\u001b[32m0.04011\u001b[0m\u001b[0m | time: 0.010s\n",
            "| Adam | epoch: 851 | loss: 0.04011 - acc: 1.0000 -- iter: 16/36\n",
            "Training Step: 4253  | total loss: \u001b[1m\u001b[32m0.04177\u001b[0m\u001b[0m | time: 0.015s\n",
            "| Adam | epoch: 851 | loss: 0.04177 - acc: 1.0000 -- iter: 24/36\n",
            "Training Step: 4254  | total loss: \u001b[1m\u001b[32m0.04325\u001b[0m\u001b[0m | time: 0.019s\n",
            "| Adam | epoch: 851 | loss: 0.04325 - acc: 1.0000 -- iter: 32/36\n",
            "Training Step: 4255  | total loss: \u001b[1m\u001b[32m0.04385\u001b[0m\u001b[0m | time: 0.024s\n",
            "| Adam | epoch: 851 | loss: 0.04385 - acc: 1.0000 -- iter: 36/36\n",
            "--\n",
            "Training Step: 4256  | total loss: \u001b[1m\u001b[32m0.04278\u001b[0m\u001b[0m | time: 0.004s\n",
            "| Adam | epoch: 852 | loss: 0.04278 - acc: 1.0000 -- iter: 08/36\n",
            "Training Step: 4257  | total loss: \u001b[1m\u001b[32m0.04317\u001b[0m\u001b[0m | time: 0.007s\n",
            "| Adam | epoch: 852 | loss: 0.04317 - acc: 1.0000 -- iter: 16/36\n",
            "Training Step: 4258  | total loss: \u001b[1m\u001b[32m0.04326\u001b[0m\u001b[0m | time: 0.013s\n",
            "| Adam | epoch: 852 | loss: 0.04326 - acc: 1.0000 -- iter: 24/36\n",
            "Training Step: 4259  | total loss: \u001b[1m\u001b[32m0.04332\u001b[0m\u001b[0m | time: 0.017s\n",
            "| Adam | epoch: 852 | loss: 0.04332 - acc: 1.0000 -- iter: 32/36\n",
            "Training Step: 4260  | total loss: \u001b[1m\u001b[32m0.04337\u001b[0m\u001b[0m | time: 0.022s\n",
            "| Adam | epoch: 852 | loss: 0.04337 - acc: 1.0000 -- iter: 36/36\n",
            "--\n",
            "Training Step: 4261  | total loss: \u001b[1m\u001b[32m0.04307\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 853 | loss: 0.04307 - acc: 1.0000 -- iter: 08/36\n",
            "Training Step: 4262  | total loss: \u001b[1m\u001b[32m0.04259\u001b[0m\u001b[0m | time: 0.010s\n",
            "| Adam | epoch: 853 | loss: 0.04259 - acc: 1.0000 -- iter: 16/36\n",
            "Training Step: 4263  | total loss: \u001b[1m\u001b[32m0.04250\u001b[0m\u001b[0m | time: 0.014s\n",
            "| Adam | epoch: 853 | loss: 0.04250 - acc: 1.0000 -- iter: 24/36\n",
            "Training Step: 4264  | total loss: \u001b[1m\u001b[32m0.04208\u001b[0m\u001b[0m | time: 0.018s\n",
            "| Adam | epoch: 853 | loss: 0.04208 - acc: 1.0000 -- iter: 32/36\n",
            "Training Step: 4265  | total loss: \u001b[1m\u001b[32m0.04337\u001b[0m\u001b[0m | time: 0.023s\n",
            "| Adam | epoch: 853 | loss: 0.04337 - acc: 1.0000 -- iter: 36/36\n",
            "--\n",
            "Training Step: 4266  | total loss: \u001b[1m\u001b[32m0.04450\u001b[0m\u001b[0m | time: 0.004s\n",
            "| Adam | epoch: 854 | loss: 0.04450 - acc: 1.0000 -- iter: 08/36\n",
            "Training Step: 4267  | total loss: \u001b[1m\u001b[32m0.04336\u001b[0m\u001b[0m | time: 0.007s\n",
            "| Adam | epoch: 854 | loss: 0.04336 - acc: 1.0000 -- iter: 16/36\n",
            "Training Step: 4268  | total loss: \u001b[1m\u001b[32m0.04351\u001b[0m\u001b[0m | time: 0.011s\n",
            "| Adam | epoch: 854 | loss: 0.04351 - acc: 1.0000 -- iter: 24/36\n",
            "Training Step: 4269  | total loss: \u001b[1m\u001b[32m0.04321\u001b[0m\u001b[0m | time: 0.017s\n",
            "| Adam | epoch: 854 | loss: 0.04321 - acc: 1.0000 -- iter: 32/36\n",
            "Training Step: 4270  | total loss: \u001b[1m\u001b[32m0.04264\u001b[0m\u001b[0m | time: 0.021s\n",
            "| Adam | epoch: 854 | loss: 0.04264 - acc: 1.0000 -- iter: 36/36\n",
            "--\n",
            "Training Step: 4271  | total loss: \u001b[1m\u001b[32m0.04258\u001b[0m\u001b[0m | time: 0.006s\n",
            "| Adam | epoch: 855 | loss: 0.04258 - acc: 1.0000 -- iter: 08/36\n",
            "Training Step: 4272  | total loss: \u001b[1m\u001b[32m0.04252\u001b[0m\u001b[0m | time: 0.010s\n",
            "| Adam | epoch: 855 | loss: 0.04252 - acc: 1.0000 -- iter: 16/36\n",
            "Training Step: 4273  | total loss: \u001b[1m\u001b[32m0.04178\u001b[0m\u001b[0m | time: 0.014s\n",
            "| Adam | epoch: 855 | loss: 0.04178 - acc: 1.0000 -- iter: 24/36\n",
            "Training Step: 4274  | total loss: \u001b[1m\u001b[32m0.04281\u001b[0m\u001b[0m | time: 0.018s\n",
            "| Adam | epoch: 855 | loss: 0.04281 - acc: 1.0000 -- iter: 32/36\n",
            "Training Step: 4275  | total loss: \u001b[1m\u001b[32m0.04215\u001b[0m\u001b[0m | time: 0.022s\n",
            "| Adam | epoch: 855 | loss: 0.04215 - acc: 1.0000 -- iter: 36/36\n",
            "--\n",
            "Training Step: 4276  | total loss: \u001b[1m\u001b[32m0.04215\u001b[0m\u001b[0m | time: 0.004s\n",
            "| Adam | epoch: 856 | loss: 0.04215 - acc: 1.0000 -- iter: 08/36\n",
            "Training Step: 4277  | total loss: \u001b[1m\u001b[32m0.04009\u001b[0m\u001b[0m | time: 0.008s\n",
            "| Adam | epoch: 856 | loss: 0.04009 - acc: 1.0000 -- iter: 16/36\n",
            "Training Step: 4278  | total loss: \u001b[1m\u001b[32m0.03874\u001b[0m\u001b[0m | time: 0.014s\n",
            "| Adam | epoch: 856 | loss: 0.03874 - acc: 1.0000 -- iter: 24/36\n",
            "Training Step: 4279  | total loss: \u001b[1m\u001b[32m0.03839\u001b[0m\u001b[0m | time: 0.020s\n",
            "| Adam | epoch: 856 | loss: 0.03839 - acc: 1.0000 -- iter: 32/36\n",
            "Training Step: 4280  | total loss: \u001b[1m\u001b[32m0.03862\u001b[0m\u001b[0m | time: 0.025s\n",
            "| Adam | epoch: 856 | loss: 0.03862 - acc: 1.0000 -- iter: 36/36\n",
            "--\n",
            "Training Step: 4281  | total loss: \u001b[1m\u001b[32m0.04017\u001b[0m\u001b[0m | time: 0.004s\n",
            "| Adam | epoch: 857 | loss: 0.04017 - acc: 1.0000 -- iter: 08/36\n",
            "Training Step: 4282  | total loss: \u001b[1m\u001b[32m0.04036\u001b[0m\u001b[0m | time: 0.008s\n",
            "| Adam | epoch: 857 | loss: 0.04036 - acc: 1.0000 -- iter: 16/36\n",
            "Training Step: 4283  | total loss: \u001b[1m\u001b[32m0.03962\u001b[0m\u001b[0m | time: 0.013s\n",
            "| Adam | epoch: 857 | loss: 0.03962 - acc: 1.0000 -- iter: 24/36\n",
            "Training Step: 4284  | total loss: \u001b[1m\u001b[32m0.03894\u001b[0m\u001b[0m | time: 0.016s\n",
            "| Adam | epoch: 857 | loss: 0.03894 - acc: 1.0000 -- iter: 32/36\n",
            "Training Step: 4285  | total loss: \u001b[1m\u001b[32m0.03899\u001b[0m\u001b[0m | time: 0.021s\n",
            "| Adam | epoch: 857 | loss: 0.03899 - acc: 1.0000 -- iter: 36/36\n",
            "--\n",
            "Training Step: 4286  | total loss: \u001b[1m\u001b[32m0.03892\u001b[0m\u001b[0m | time: 0.004s\n",
            "| Adam | epoch: 858 | loss: 0.03892 - acc: 1.0000 -- iter: 08/36\n",
            "Training Step: 4287  | total loss: \u001b[1m\u001b[32m0.03922\u001b[0m\u001b[0m | time: 0.009s\n",
            "| Adam | epoch: 858 | loss: 0.03922 - acc: 1.0000 -- iter: 16/36\n",
            "Training Step: 4288  | total loss: \u001b[1m\u001b[32m0.03757\u001b[0m\u001b[0m | time: 0.013s\n",
            "| Adam | epoch: 858 | loss: 0.03757 - acc: 1.0000 -- iter: 24/36\n",
            "Training Step: 4289  | total loss: \u001b[1m\u001b[32m0.03722\u001b[0m\u001b[0m | time: 0.018s\n",
            "| Adam | epoch: 858 | loss: 0.03722 - acc: 1.0000 -- iter: 32/36\n",
            "Training Step: 4290  | total loss: \u001b[1m\u001b[32m0.03722\u001b[0m\u001b[0m | time: 0.022s\n",
            "| Adam | epoch: 858 | loss: 0.03722 - acc: 1.0000 -- iter: 36/36\n",
            "--\n",
            "Training Step: 4291  | total loss: \u001b[1m\u001b[32m0.03690\u001b[0m\u001b[0m | time: 0.006s\n",
            "| Adam | epoch: 859 | loss: 0.03690 - acc: 1.0000 -- iter: 08/36\n",
            "Training Step: 4292  | total loss: \u001b[1m\u001b[32m0.03860\u001b[0m\u001b[0m | time: 0.010s\n",
            "| Adam | epoch: 859 | loss: 0.03860 - acc: 1.0000 -- iter: 16/36\n",
            "Training Step: 4293  | total loss: \u001b[1m\u001b[32m0.03919\u001b[0m\u001b[0m | time: 0.014s\n",
            "| Adam | epoch: 859 | loss: 0.03919 - acc: 1.0000 -- iter: 24/36\n",
            "Training Step: 4294  | total loss: \u001b[1m\u001b[32m0.03912\u001b[0m\u001b[0m | time: 0.019s\n",
            "| Adam | epoch: 859 | loss: 0.03912 - acc: 1.0000 -- iter: 32/36\n",
            "Training Step: 4295  | total loss: \u001b[1m\u001b[32m0.04066\u001b[0m\u001b[0m | time: 0.022s\n",
            "| Adam | epoch: 859 | loss: 0.04066 - acc: 1.0000 -- iter: 36/36\n",
            "--\n",
            "Training Step: 4296  | total loss: \u001b[1m\u001b[32m0.04203\u001b[0m\u001b[0m | time: 0.004s\n",
            "| Adam | epoch: 860 | loss: 0.04203 - acc: 1.0000 -- iter: 08/36\n",
            "Training Step: 4297  | total loss: \u001b[1m\u001b[32m0.04116\u001b[0m\u001b[0m | time: 0.009s\n",
            "| Adam | epoch: 860 | loss: 0.04116 - acc: 1.0000 -- iter: 16/36\n",
            "Training Step: 4298  | total loss: \u001b[1m\u001b[32m0.03981\u001b[0m\u001b[0m | time: 0.013s\n",
            "| Adam | epoch: 860 | loss: 0.03981 - acc: 1.0000 -- iter: 24/36\n",
            "Training Step: 4299  | total loss: \u001b[1m\u001b[32m0.04059\u001b[0m\u001b[0m | time: 0.018s\n",
            "| Adam | epoch: 860 | loss: 0.04059 - acc: 1.0000 -- iter: 32/36\n",
            "Training Step: 4300  | total loss: \u001b[1m\u001b[32m0.03946\u001b[0m\u001b[0m | time: 0.022s\n",
            "| Adam | epoch: 860 | loss: 0.03946 - acc: 1.0000 -- iter: 36/36\n",
            "--\n",
            "Training Step: 4301  | total loss: \u001b[1m\u001b[32m0.03857\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 861 | loss: 0.03857 - acc: 1.0000 -- iter: 08/36\n",
            "Training Step: 4302  | total loss: \u001b[1m\u001b[32m0.03776\u001b[0m\u001b[0m | time: 0.009s\n",
            "| Adam | epoch: 861 | loss: 0.03776 - acc: 1.0000 -- iter: 16/36\n",
            "Training Step: 4303  | total loss: \u001b[1m\u001b[32m0.03777\u001b[0m\u001b[0m | time: 0.016s\n",
            "| Adam | epoch: 861 | loss: 0.03777 - acc: 1.0000 -- iter: 24/36\n",
            "Training Step: 4304  | total loss: \u001b[1m\u001b[32m0.03773\u001b[0m\u001b[0m | time: 0.020s\n",
            "| Adam | epoch: 861 | loss: 0.03773 - acc: 1.0000 -- iter: 32/36\n",
            "Training Step: 4305  | total loss: \u001b[1m\u001b[32m0.03919\u001b[0m\u001b[0m | time: 0.024s\n",
            "| Adam | epoch: 861 | loss: 0.03919 - acc: 1.0000 -- iter: 36/36\n",
            "--\n",
            "Training Step: 4306  | total loss: \u001b[1m\u001b[32m0.03886\u001b[0m\u001b[0m | time: 0.004s\n",
            "| Adam | epoch: 862 | loss: 0.03886 - acc: 1.0000 -- iter: 08/36\n",
            "Training Step: 4307  | total loss: \u001b[1m\u001b[32m0.03742\u001b[0m\u001b[0m | time: 0.008s\n",
            "| Adam | epoch: 862 | loss: 0.03742 - acc: 1.0000 -- iter: 16/36\n",
            "Training Step: 4308  | total loss: \u001b[1m\u001b[32m0.03611\u001b[0m\u001b[0m | time: 0.013s\n",
            "| Adam | epoch: 862 | loss: 0.03611 - acc: 1.0000 -- iter: 24/36\n",
            "Training Step: 4309  | total loss: \u001b[1m\u001b[32m0.03654\u001b[0m\u001b[0m | time: 0.018s\n",
            "| Adam | epoch: 862 | loss: 0.03654 - acc: 1.0000 -- iter: 32/36\n",
            "Training Step: 4310  | total loss: \u001b[1m\u001b[32m0.03687\u001b[0m\u001b[0m | time: 0.023s\n",
            "| Adam | epoch: 862 | loss: 0.03687 - acc: 1.0000 -- iter: 36/36\n",
            "--\n",
            "Training Step: 4311  | total loss: \u001b[1m\u001b[32m0.03738\u001b[0m\u001b[0m | time: 0.008s\n",
            "| Adam | epoch: 863 | loss: 0.03738 - acc: 1.0000 -- iter: 08/36\n",
            "Training Step: 4312  | total loss: \u001b[1m\u001b[32m0.03738\u001b[0m\u001b[0m | time: 0.013s\n",
            "| Adam | epoch: 863 | loss: 0.03738 - acc: 1.0000 -- iter: 16/36\n",
            "Training Step: 4313  | total loss: \u001b[1m\u001b[32m0.03635\u001b[0m\u001b[0m | time: 0.017s\n",
            "| Adam | epoch: 863 | loss: 0.03635 - acc: 1.0000 -- iter: 24/36\n",
            "Training Step: 4314  | total loss: \u001b[1m\u001b[32m0.03642\u001b[0m\u001b[0m | time: 0.022s\n",
            "| Adam | epoch: 863 | loss: 0.03642 - acc: 1.0000 -- iter: 32/36\n",
            "Training Step: 4315  | total loss: \u001b[1m\u001b[32m0.03648\u001b[0m\u001b[0m | time: 0.026s\n",
            "| Adam | epoch: 863 | loss: 0.03648 - acc: 1.0000 -- iter: 36/36\n",
            "--\n",
            "Training Step: 4316  | total loss: \u001b[1m\u001b[32m0.03722\u001b[0m\u001b[0m | time: 0.006s\n",
            "| Adam | epoch: 864 | loss: 0.03722 - acc: 1.0000 -- iter: 08/36\n",
            "Training Step: 4317  | total loss: \u001b[1m\u001b[32m0.03762\u001b[0m\u001b[0m | time: 0.010s\n",
            "| Adam | epoch: 864 | loss: 0.03762 - acc: 1.0000 -- iter: 16/36\n",
            "Training Step: 4318  | total loss: \u001b[1m\u001b[32m0.03824\u001b[0m\u001b[0m | time: 0.015s\n",
            "| Adam | epoch: 864 | loss: 0.03824 - acc: 1.0000 -- iter: 24/36\n",
            "Training Step: 4319  | total loss: \u001b[1m\u001b[32m0.04023\u001b[0m\u001b[0m | time: 0.022s\n",
            "| Adam | epoch: 864 | loss: 0.04023 - acc: 1.0000 -- iter: 32/36\n",
            "Training Step: 4320  | total loss: \u001b[1m\u001b[32m0.04200\u001b[0m\u001b[0m | time: 0.027s\n",
            "| Adam | epoch: 864 | loss: 0.04200 - acc: 1.0000 -- iter: 36/36\n",
            "--\n",
            "Training Step: 4321  | total loss: \u001b[1m\u001b[32m0.04120\u001b[0m\u001b[0m | time: 0.006s\n",
            "| Adam | epoch: 865 | loss: 0.04120 - acc: 1.0000 -- iter: 08/36\n",
            "Training Step: 4322  | total loss: \u001b[1m\u001b[32m0.04015\u001b[0m\u001b[0m | time: 0.011s\n",
            "| Adam | epoch: 865 | loss: 0.04015 - acc: 1.0000 -- iter: 16/36\n",
            "Training Step: 4323  | total loss: \u001b[1m\u001b[32m0.03907\u001b[0m\u001b[0m | time: 0.017s\n",
            "| Adam | epoch: 865 | loss: 0.03907 - acc: 1.0000 -- iter: 24/36\n",
            "Training Step: 4324  | total loss: \u001b[1m\u001b[32m0.04013\u001b[0m\u001b[0m | time: 0.021s\n",
            "| Adam | epoch: 865 | loss: 0.04013 - acc: 1.0000 -- iter: 32/36\n",
            "Training Step: 4325  | total loss: \u001b[1m\u001b[32m0.04128\u001b[0m\u001b[0m | time: 0.026s\n",
            "| Adam | epoch: 865 | loss: 0.04128 - acc: 1.0000 -- iter: 36/36\n",
            "--\n",
            "Training Step: 4326  | total loss: \u001b[1m\u001b[32m0.04229\u001b[0m\u001b[0m | time: 0.006s\n",
            "| Adam | epoch: 866 | loss: 0.04229 - acc: 1.0000 -- iter: 08/36\n",
            "Training Step: 4327  | total loss: \u001b[1m\u001b[32m0.04020\u001b[0m\u001b[0m | time: 0.011s\n",
            "| Adam | epoch: 866 | loss: 0.04020 - acc: 1.0000 -- iter: 16/36\n",
            "Training Step: 4328  | total loss: \u001b[1m\u001b[32m0.03979\u001b[0m\u001b[0m | time: 0.016s\n",
            "| Adam | epoch: 866 | loss: 0.03979 - acc: 1.0000 -- iter: 24/36\n",
            "Training Step: 4329  | total loss: \u001b[1m\u001b[32m0.03902\u001b[0m\u001b[0m | time: 0.021s\n",
            "| Adam | epoch: 866 | loss: 0.03902 - acc: 1.0000 -- iter: 32/36\n",
            "Training Step: 4330  | total loss: \u001b[1m\u001b[32m0.03830\u001b[0m\u001b[0m | time: 0.027s\n",
            "| Adam | epoch: 866 | loss: 0.03830 - acc: 1.0000 -- iter: 36/36\n",
            "--\n",
            "Training Step: 4331  | total loss: \u001b[1m\u001b[32m0.03726\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 867 | loss: 0.03726 - acc: 1.0000 -- iter: 08/36\n",
            "Training Step: 4332  | total loss: \u001b[1m\u001b[32m0.03632\u001b[0m\u001b[0m | time: 0.010s\n",
            "| Adam | epoch: 867 | loss: 0.03632 - acc: 1.0000 -- iter: 16/36\n",
            "Training Step: 4333  | total loss: \u001b[1m\u001b[32m0.03619\u001b[0m\u001b[0m | time: 0.015s\n",
            "| Adam | epoch: 867 | loss: 0.03619 - acc: 1.0000 -- iter: 24/36\n",
            "Training Step: 4334  | total loss: \u001b[1m\u001b[32m0.03689\u001b[0m\u001b[0m | time: 0.022s\n",
            "| Adam | epoch: 867 | loss: 0.03689 - acc: 1.0000 -- iter: 32/36\n",
            "Training Step: 4335  | total loss: \u001b[1m\u001b[32m0.03709\u001b[0m\u001b[0m | time: 0.030s\n",
            "| Adam | epoch: 867 | loss: 0.03709 - acc: 1.0000 -- iter: 36/36\n",
            "--\n",
            "Training Step: 4336  | total loss: \u001b[1m\u001b[32m0.03703\u001b[0m\u001b[0m | time: 0.008s\n",
            "| Adam | epoch: 868 | loss: 0.03703 - acc: 1.0000 -- iter: 08/36\n",
            "Training Step: 4337  | total loss: \u001b[1m\u001b[32m0.03668\u001b[0m\u001b[0m | time: 0.014s\n",
            "| Adam | epoch: 868 | loss: 0.03668 - acc: 1.0000 -- iter: 16/36\n",
            "Training Step: 4338  | total loss: \u001b[1m\u001b[32m0.03636\u001b[0m\u001b[0m | time: 0.020s\n",
            "| Adam | epoch: 868 | loss: 0.03636 - acc: 1.0000 -- iter: 24/36\n",
            "Training Step: 4339  | total loss: \u001b[1m\u001b[32m0.03690\u001b[0m\u001b[0m | time: 0.025s\n",
            "| Adam | epoch: 868 | loss: 0.03690 - acc: 1.0000 -- iter: 32/36\n",
            "Training Step: 4340  | total loss: \u001b[1m\u001b[32m0.03681\u001b[0m\u001b[0m | time: 0.031s\n",
            "| Adam | epoch: 868 | loss: 0.03681 - acc: 1.0000 -- iter: 36/36\n",
            "--\n",
            "Training Step: 4341  | total loss: \u001b[1m\u001b[32m0.03615\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 869 | loss: 0.03615 - acc: 1.0000 -- iter: 08/36\n",
            "Training Step: 4342  | total loss: \u001b[1m\u001b[32m0.03615\u001b[0m\u001b[0m | time: 0.012s\n",
            "| Adam | epoch: 869 | loss: 0.03615 - acc: 1.0000 -- iter: 16/36\n",
            "Training Step: 4343  | total loss: \u001b[1m\u001b[32m0.03532\u001b[0m\u001b[0m | time: 0.019s\n",
            "| Adam | epoch: 869 | loss: 0.03532 - acc: 1.0000 -- iter: 24/36\n",
            "Training Step: 4344  | total loss: \u001b[1m\u001b[32m0.03444\u001b[0m\u001b[0m | time: 0.026s\n",
            "| Adam | epoch: 869 | loss: 0.03444 - acc: 1.0000 -- iter: 32/36\n",
            "Training Step: 4345  | total loss: \u001b[1m\u001b[32m0.03363\u001b[0m\u001b[0m | time: 0.033s\n",
            "| Adam | epoch: 869 | loss: 0.03363 - acc: 1.0000 -- iter: 36/36\n",
            "--\n",
            "Training Step: 4346  | total loss: \u001b[1m\u001b[32m0.03460\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 870 | loss: 0.03460 - acc: 1.0000 -- iter: 08/36\n",
            "Training Step: 4347  | total loss: \u001b[1m\u001b[32m0.03518\u001b[0m\u001b[0m | time: 0.015s\n",
            "| Adam | epoch: 870 | loss: 0.03518 - acc: 1.0000 -- iter: 16/36\n",
            "Training Step: 4348  | total loss: \u001b[1m\u001b[32m0.03574\u001b[0m\u001b[0m | time: 0.022s\n",
            "| Adam | epoch: 870 | loss: 0.03574 - acc: 1.0000 -- iter: 24/36\n",
            "Training Step: 4349  | total loss: \u001b[1m\u001b[32m0.03681\u001b[0m\u001b[0m | time: 0.027s\n",
            "| Adam | epoch: 870 | loss: 0.03681 - acc: 1.0000 -- iter: 32/36\n",
            "Training Step: 4350  | total loss: \u001b[1m\u001b[32m0.03776\u001b[0m\u001b[0m | time: 0.034s\n",
            "| Adam | epoch: 870 | loss: 0.03776 - acc: 1.0000 -- iter: 36/36\n",
            "--\n",
            "Training Step: 4351  | total loss: \u001b[1m\u001b[32m0.03684\u001b[0m\u001b[0m | time: 0.004s\n",
            "| Adam | epoch: 871 | loss: 0.03684 - acc: 1.0000 -- iter: 08/36\n",
            "Training Step: 4352  | total loss: \u001b[1m\u001b[32m0.03661\u001b[0m\u001b[0m | time: 0.011s\n",
            "| Adam | epoch: 871 | loss: 0.03661 - acc: 1.0000 -- iter: 16/36\n",
            "Training Step: 4353  | total loss: \u001b[1m\u001b[32m0.03604\u001b[0m\u001b[0m | time: 0.015s\n",
            "| Adam | epoch: 871 | loss: 0.03604 - acc: 1.0000 -- iter: 24/36\n",
            "Training Step: 4354  | total loss: \u001b[1m\u001b[32m0.03625\u001b[0m\u001b[0m | time: 0.022s\n",
            "| Adam | epoch: 871 | loss: 0.03625 - acc: 1.0000 -- iter: 32/36\n",
            "Training Step: 4355  | total loss: \u001b[1m\u001b[32m0.03646\u001b[0m\u001b[0m | time: 0.027s\n",
            "| Adam | epoch: 871 | loss: 0.03646 - acc: 1.0000 -- iter: 36/36\n",
            "--\n",
            "Training Step: 4356  | total loss: \u001b[1m\u001b[32m0.03664\u001b[0m\u001b[0m | time: 0.006s\n",
            "| Adam | epoch: 872 | loss: 0.03664 - acc: 1.0000 -- iter: 08/36\n",
            "Training Step: 4357  | total loss: \u001b[1m\u001b[32m0.03582\u001b[0m\u001b[0m | time: 0.012s\n",
            "| Adam | epoch: 872 | loss: 0.03582 - acc: 1.0000 -- iter: 16/36\n",
            "Training Step: 4358  | total loss: \u001b[1m\u001b[32m0.03544\u001b[0m\u001b[0m | time: 0.018s\n",
            "| Adam | epoch: 872 | loss: 0.03544 - acc: 1.0000 -- iter: 24/36\n",
            "Training Step: 4359  | total loss: \u001b[1m\u001b[32m0.03573\u001b[0m\u001b[0m | time: 0.024s\n",
            "| Adam | epoch: 872 | loss: 0.03573 - acc: 1.0000 -- iter: 32/36\n",
            "Training Step: 4360  | total loss: \u001b[1m\u001b[32m0.03616\u001b[0m\u001b[0m | time: 0.029s\n",
            "| Adam | epoch: 872 | loss: 0.03616 - acc: 1.0000 -- iter: 36/36\n",
            "--\n",
            "Training Step: 4361  | total loss: \u001b[1m\u001b[32m0.03590\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 873 | loss: 0.03590 - acc: 1.0000 -- iter: 08/36\n",
            "Training Step: 4362  | total loss: \u001b[1m\u001b[32m0.03567\u001b[0m\u001b[0m | time: 0.012s\n",
            "| Adam | epoch: 873 | loss: 0.03567 - acc: 1.0000 -- iter: 16/36\n",
            "Training Step: 4363  | total loss: \u001b[1m\u001b[32m0.03497\u001b[0m\u001b[0m | time: 0.017s\n",
            "| Adam | epoch: 873 | loss: 0.03497 - acc: 1.0000 -- iter: 24/36\n",
            "Training Step: 4364  | total loss: \u001b[1m\u001b[32m0.03424\u001b[0m\u001b[0m | time: 0.022s\n",
            "| Adam | epoch: 873 | loss: 0.03424 - acc: 1.0000 -- iter: 32/36\n",
            "Training Step: 4365  | total loss: \u001b[1m\u001b[32m0.85353\u001b[0m\u001b[0m | time: 0.027s\n",
            "| Adam | epoch: 873 | loss: 0.85353 - acc: 1.0000 -- iter: 36/36\n",
            "--\n",
            "Training Step: 4366  | total loss: \u001b[1m\u001b[32m0.77061\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 874 | loss: 0.77061 - acc: 0.9250 -- iter: 08/36\n",
            "Training Step: 4367  | total loss: \u001b[1m\u001b[32m0.69852\u001b[0m\u001b[0m | time: 0.010s\n",
            "| Adam | epoch: 874 | loss: 0.69852 - acc: 0.9325 -- iter: 16/36\n",
            "Training Step: 4368  | total loss: \u001b[1m\u001b[32m0.63369\u001b[0m\u001b[0m | time: 0.015s\n",
            "| Adam | epoch: 874 | loss: 0.63369 - acc: 0.9453 -- iter: 24/36\n",
            "Training Step: 4369  | total loss: \u001b[1m\u001b[32m0.63369\u001b[0m\u001b[0m | time: 0.020s\n",
            "| Adam | epoch: 874 | loss: 0.63369 - acc: 0.9453 -- iter: 32/36\n",
            "Training Step: 4370  | total loss: \u001b[1m\u001b[32m0.51964\u001b[0m\u001b[0m | time: 0.027s\n",
            "| Adam | epoch: 874 | loss: 0.51964 - acc: 0.9557 -- iter: 36/36\n",
            "--\n",
            "Training Step: 4371  | total loss: \u001b[1m\u001b[32m1.35940\u001b[0m\u001b[0m | time: 0.004s\n",
            "| Adam | epoch: 875 | loss: 1.35940 - acc: 0.8601 -- iter: 08/36\n",
            "Training Step: 4372  | total loss: \u001b[1m\u001b[32m1.22638\u001b[0m\u001b[0m | time: 0.009s\n",
            "| Adam | epoch: 875 | loss: 1.22638 - acc: 0.8741 -- iter: 16/36\n",
            "Training Step: 4373  | total loss: \u001b[1m\u001b[32m1.10617\u001b[0m\u001b[0m | time: 0.014s\n",
            "| Adam | epoch: 875 | loss: 1.10617 - acc: 0.8867 -- iter: 24/36\n",
            "Training Step: 4374  | total loss: \u001b[1m\u001b[32m0.99800\u001b[0m\u001b[0m | time: 0.020s\n",
            "| Adam | epoch: 875 | loss: 0.99800 - acc: 0.8980 -- iter: 32/36\n",
            "Training Step: 4375  | total loss: \u001b[1m\u001b[32m0.90289\u001b[0m\u001b[0m | time: 0.028s\n",
            "| Adam | epoch: 875 | loss: 0.90289 - acc: 0.9082 -- iter: 36/36\n",
            "--\n",
            "Training Step: 4376  | total loss: \u001b[1m\u001b[32m0.81698\u001b[0m\u001b[0m | time: 0.004s\n",
            "| Adam | epoch: 876 | loss: 0.81698 - acc: 0.9174 -- iter: 08/36\n",
            "Training Step: 4377  | total loss: \u001b[1m\u001b[32m0.81698\u001b[0m\u001b[0m | time: 0.009s\n",
            "| Adam | epoch: 876 | loss: 0.81698 - acc: 0.9174 -- iter: 16/36\n",
            "Training Step: 4378  | total loss: \u001b[1m\u001b[32m1.79650\u001b[0m\u001b[0m | time: 0.014s\n",
            "| Adam | epoch: 876 | loss: 1.79650 - acc: 0.8257 -- iter: 24/36\n",
            "Training Step: 4379  | total loss: \u001b[1m\u001b[32m1.46142\u001b[0m\u001b[0m | time: 0.020s\n",
            "| Adam | epoch: 876 | loss: 1.46142 - acc: 0.8588 -- iter: 32/36\n",
            "Training Step: 4380  | total loss: \u001b[1m\u001b[32m1.31897\u001b[0m\u001b[0m | time: 0.025s\n",
            "| Adam | epoch: 876 | loss: 1.31897 - acc: 0.8729 -- iter: 36/36\n",
            "--\n",
            "Training Step: 4381  | total loss: \u001b[1m\u001b[32m1.19228\u001b[0m\u001b[0m | time: 0.004s\n",
            "| Adam | epoch: 877 | loss: 1.19228 - acc: 0.8856 -- iter: 08/36\n",
            "Training Step: 4382  | total loss: \u001b[1m\u001b[32m1.19228\u001b[0m\u001b[0m | time: 0.009s\n",
            "| Adam | epoch: 877 | loss: 1.19228 - acc: 0.8856 -- iter: 16/36\n",
            "Training Step: 4383  | total loss: \u001b[1m\u001b[32m1.07715\u001b[0m\u001b[0m | time: 0.016s\n",
            "| Adam | epoch: 877 | loss: 1.07715 - acc: 0.8971 -- iter: 24/36\n",
            "Training Step: 4384  | total loss: \u001b[1m\u001b[32m1.56869\u001b[0m\u001b[0m | time: 0.022s\n",
            "| Adam | epoch: 877 | loss: 1.56869 - acc: 0.8324 -- iter: 32/36\n",
            "Training Step: 4385  | total loss: \u001b[1m\u001b[32m1.41554\u001b[0m\u001b[0m | time: 0.028s\n",
            "| Adam | epoch: 877 | loss: 1.41554 - acc: 0.8491 -- iter: 36/36\n",
            "--\n",
            "Training Step: 4386  | total loss: \u001b[1m\u001b[32m1.27640\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 878 | loss: 1.27640 - acc: 0.8642 -- iter: 08/36\n",
            "Training Step: 4387  | total loss: \u001b[1m\u001b[32m1.15121\u001b[0m\u001b[0m | time: 0.010s\n",
            "| Adam | epoch: 878 | loss: 1.15121 - acc: 0.8778 -- iter: 16/36\n",
            "Training Step: 4388  | total loss: \u001b[1m\u001b[32m0.94171\u001b[0m\u001b[0m | time: 0.017s\n",
            "| Adam | epoch: 878 | loss: 0.94171 - acc: 0.9010 -- iter: 24/36\n",
            "Training Step: 4389  | total loss: \u001b[1m\u001b[32m0.85157\u001b[0m\u001b[0m | time: 0.023s\n",
            "| Adam | epoch: 878 | loss: 0.85157 - acc: 0.9109 -- iter: 32/36\n",
            "Training Step: 4390  | total loss: \u001b[1m\u001b[32m0.77060\u001b[0m\u001b[0m | time: 0.028s\n",
            "| Adam | epoch: 878 | loss: 0.77060 - acc: 0.9198 -- iter: 36/36\n",
            "--\n",
            "Training Step: 4391  | total loss: \u001b[1m\u001b[32m0.69871\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 879 | loss: 0.69871 - acc: 0.9278 -- iter: 08/36\n",
            "Training Step: 4392  | total loss: \u001b[1m\u001b[32m0.63402\u001b[0m\u001b[0m | time: 0.011s\n",
            "| Adam | epoch: 879 | loss: 0.63402 - acc: 0.9351 -- iter: 16/36\n",
            "Training Step: 4393  | total loss: \u001b[1m\u001b[32m0.63402\u001b[0m\u001b[0m | time: 0.016s\n",
            "| Adam | epoch: 879 | loss: 0.63402 - acc: 0.9351 -- iter: 24/36\n",
            "Training Step: 4394  | total loss: \u001b[1m\u001b[32m0.57379\u001b[0m\u001b[0m | time: 0.021s\n",
            "| Adam | epoch: 879 | loss: 0.57379 - acc: 0.9415 -- iter: 32/36\n",
            "Training Step: 4395  | total loss: \u001b[1m\u001b[32m0.52125\u001b[0m\u001b[0m | time: 0.026s\n",
            "| Adam | epoch: 879 | loss: 0.52125 - acc: 0.9474 -- iter: 36/36\n",
            "--\n",
            "Training Step: 4396  | total loss: \u001b[1m\u001b[32m0.47375\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 880 | loss: 0.47375 - acc: 0.9527 -- iter: 08/36\n",
            "Training Step: 4397  | total loss: \u001b[1m\u001b[32m0.39305\u001b[0m\u001b[0m | time: 0.011s\n",
            "| Adam | epoch: 880 | loss: 0.39305 - acc: 0.9616 -- iter: 16/36\n",
            "Training Step: 4398  | total loss: \u001b[1m\u001b[32m0.35890\u001b[0m\u001b[0m | time: 0.018s\n",
            "| Adam | epoch: 880 | loss: 0.35890 - acc: 0.9655 -- iter: 24/36\n",
            "Training Step: 4399  | total loss: \u001b[1m\u001b[32m0.32702\u001b[0m\u001b[0m | time: 0.024s\n",
            "| Adam | epoch: 880 | loss: 0.32702 - acc: 0.9689 -- iter: 32/36\n",
            "Training Step: 4400  | total loss: \u001b[1m\u001b[32m0.29822\u001b[0m\u001b[0m | time: 0.030s\n",
            "| Adam | epoch: 880 | loss: 0.29822 - acc: 0.9720 -- iter: 36/36\n",
            "--\n",
            "Training Step: 4401  | total loss: \u001b[1m\u001b[32m0.89851\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 881 | loss: 0.89851 - acc: 0.8998 -- iter: 08/36\n",
            "Training Step: 4402  | total loss: \u001b[1m\u001b[32m0.81271\u001b[0m\u001b[0m | time: 0.010s\n",
            "| Adam | epoch: 881 | loss: 0.81271 - acc: 0.9099 -- iter: 16/36\n",
            "Training Step: 4403  | total loss: \u001b[1m\u001b[32m0.73550\u001b[0m\u001b[0m | time: 0.016s\n",
            "| Adam | epoch: 881 | loss: 0.73550 - acc: 0.9189 -- iter: 24/36\n",
            "Training Step: 4404  | total loss: \u001b[1m\u001b[32m0.73550\u001b[0m\u001b[0m | time: 0.022s\n",
            "| Adam | epoch: 881 | loss: 0.73550 - acc: 0.9189 -- iter: 32/36\n",
            "Training Step: 4405  | total loss: \u001b[1m\u001b[32m0.66601\u001b[0m\u001b[0m | time: 0.027s\n",
            "| Adam | epoch: 881 | loss: 0.66601 - acc: 0.9270 -- iter: 36/36\n",
            "--\n",
            "Training Step: 4406  | total loss: \u001b[1m\u001b[32m0.60405\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 882 | loss: 0.60405 - acc: 0.9343 -- iter: 08/36\n",
            "Training Step: 4407  | total loss: \u001b[1m\u001b[32m0.54789\u001b[0m\u001b[0m | time: 0.010s\n",
            "| Adam | epoch: 882 | loss: 0.54789 - acc: 0.9409 -- iter: 16/36\n",
            "Training Step: 4408  | total loss: \u001b[1m\u001b[32m1.24507\u001b[0m\u001b[0m | time: 0.019s\n",
            "| Adam | epoch: 882 | loss: 1.24507 - acc: 0.8843 -- iter: 24/36\n",
            "Training Step: 4409  | total loss: \u001b[1m\u001b[32m1.12488\u001b[0m\u001b[0m | time: 0.024s\n",
            "| Adam | epoch: 882 | loss: 1.12488 - acc: 0.8958 -- iter: 32/36\n",
            "Training Step: 4410  | total loss: \u001b[1m\u001b[32m1.01782\u001b[0m\u001b[0m | time: 0.030s\n",
            "| Adam | epoch: 882 | loss: 1.01782 - acc: 0.9063 -- iter: 36/36\n",
            "--\n",
            "Training Step: 4411  | total loss: \u001b[1m\u001b[32m0.92149\u001b[0m\u001b[0m | time: 0.008s\n",
            "| Adam | epoch: 883 | loss: 0.92149 - acc: 0.9156 -- iter: 08/36\n",
            "Training Step: 4412  | total loss: \u001b[1m\u001b[32m0.75664\u001b[0m\u001b[0m | time: 0.013s\n",
            "| Adam | epoch: 883 | loss: 0.75664 - acc: 0.9241 -- iter: 16/36\n",
            "Training Step: 4413  | total loss: \u001b[1m\u001b[32m0.75664\u001b[0m\u001b[0m | time: 0.019s\n",
            "| Adam | epoch: 883 | loss: 0.75664 - acc: 0.9317 -- iter: 24/36\n",
            "Training Step: 4414  | total loss: \u001b[1m\u001b[32m1.33575\u001b[0m\u001b[0m | time: 0.023s\n",
            "| Adam | epoch: 883 | loss: 1.33575 - acc: 0.8635 -- iter: 32/36\n",
            "Training Step: 4415  | total loss: \u001b[1m\u001b[32m1.20642\u001b[0m\u001b[0m | time: 0.030s\n",
            "| Adam | epoch: 883 | loss: 1.20642 - acc: 0.8771 -- iter: 36/36\n",
            "--\n",
            "Training Step: 4416  | total loss: \u001b[1m\u001b[32m1.09050\u001b[0m\u001b[0m | time: 0.007s\n",
            "| Adam | epoch: 884 | loss: 1.09050 - acc: 0.8894 -- iter: 08/36\n",
            "Training Step: 4417  | total loss: \u001b[1m\u001b[32m0.98630\u001b[0m\u001b[0m | time: 0.014s\n",
            "| Adam | epoch: 884 | loss: 0.98630 - acc: 0.9005 -- iter: 16/36\n",
            "Training Step: 4418  | total loss: \u001b[1m\u001b[32m0.89246\u001b[0m\u001b[0m | time: 0.019s\n",
            "| Adam | epoch: 884 | loss: 0.89246 - acc: 0.9104 -- iter: 24/36\n",
            "Training Step: 4419  | total loss: \u001b[1m\u001b[32m1.07301\u001b[0m\u001b[0m | time: 0.024s\n",
            "| Adam | epoch: 884 | loss: 1.07301 - acc: 0.9194 -- iter: 32/36\n",
            "Training Step: 4420  | total loss: \u001b[1m\u001b[32m0.97133\u001b[0m\u001b[0m | time: 0.030s\n",
            "| Adam | epoch: 884 | loss: 0.97133 - acc: 0.8775 -- iter: 36/36\n",
            "--\n",
            "Training Step: 4421  | total loss: \u001b[1m\u001b[32m0.88079\u001b[0m\u001b[0m | time: 0.004s\n",
            "| Adam | epoch: 885 | loss: 0.88079 - acc: 0.8897 -- iter: 08/36\n",
            "Training Step: 4422  | total loss: \u001b[1m\u001b[32m0.79936\u001b[0m\u001b[0m | time: 0.011s\n",
            "| Adam | epoch: 885 | loss: 0.79936 - acc: 0.9007 -- iter: 16/36\n",
            "Training Step: 4423  | total loss: \u001b[1m\u001b[32m0.72300\u001b[0m\u001b[0m | time: 0.015s\n",
            "| Adam | epoch: 885 | loss: 0.72300 - acc: 0.9107 -- iter: 24/36\n",
            "Training Step: 4424  | total loss: \u001b[1m\u001b[32m0.65528\u001b[0m\u001b[0m | time: 0.023s\n",
            "| Adam | epoch: 885 | loss: 0.65528 - acc: 0.9196 -- iter: 32/36\n",
            "Training Step: 4425  | total loss: \u001b[1m\u001b[32m1.28506\u001b[0m\u001b[0m | time: 0.028s\n",
            "| Adam | epoch: 885 | loss: 1.28506 - acc: 0.9276 -- iter: 36/36\n",
            "--\n",
            "Training Step: 4426  | total loss: \u001b[1m\u001b[32m1.16178\u001b[0m\u001b[0m | time: 0.004s\n",
            "| Adam | epoch: 886 | loss: 1.16178 - acc: 0.8474 -- iter: 08/36\n",
            "Training Step: 4427  | total loss: \u001b[1m\u001b[32m1.04924\u001b[0m\u001b[0m | time: 0.011s\n",
            "| Adam | epoch: 886 | loss: 1.04924 - acc: 0.8626 -- iter: 16/36\n",
            "Training Step: 4428  | total loss: \u001b[1m\u001b[32m0.94795\u001b[0m\u001b[0m | time: 0.016s\n",
            "| Adam | epoch: 886 | loss: 0.94795 - acc: 0.8764 -- iter: 24/36\n",
            "Training Step: 4429  | total loss: \u001b[1m\u001b[32m0.94795\u001b[0m\u001b[0m | time: 0.020s\n",
            "| Adam | epoch: 886 | loss: 0.94795 - acc: 0.8887 -- iter: 32/36\n",
            "Training Step: 4430  | total loss: \u001b[1m\u001b[32m0.85959\u001b[0m\u001b[0m | time: 0.026s\n",
            "| Adam | epoch: 886 | loss: 0.85959 - acc: 0.8999 -- iter: 36/36\n",
            "--\n",
            "Training Step: 4431  | total loss: \u001b[1m\u001b[32m0.77922\u001b[0m\u001b[0m | time: 0.004s\n",
            "| Adam | epoch: 887 | loss: 0.77922 - acc: 0.9099 -- iter: 08/36\n",
            "Training Step: 4432  | total loss: \u001b[1m\u001b[32m2.02066\u001b[0m\u001b[0m | time: 0.009s\n",
            "| Adam | epoch: 887 | loss: 2.02066 - acc: 0.8189 -- iter: 16/36\n",
            "Training Step: 4433  | total loss: \u001b[1m\u001b[32m1.64840\u001b[0m\u001b[0m | time: 0.014s\n",
            "| Adam | epoch: 887 | loss: 1.64840 - acc: 0.8370 -- iter: 24/36\n",
            "Training Step: 4434  | total loss: \u001b[1m\u001b[32m1.49075\u001b[0m\u001b[0m | time: 0.020s\n",
            "| Adam | epoch: 887 | loss: 1.49075 - acc: 0.8533 -- iter: 32/36\n",
            "Training Step: 4435  | total loss: \u001b[1m\u001b[32m1.34680\u001b[0m\u001b[0m | time: 0.025s\n",
            "| Adam | epoch: 887 | loss: 1.34680 - acc: 0.8680 -- iter: 36/36\n",
            "--\n",
            "Training Step: 4436  | total loss: \u001b[1m\u001b[32m1.21718\u001b[0m\u001b[0m | time: 0.004s\n",
            "| Adam | epoch: 888 | loss: 1.21718 - acc: 0.8812 -- iter: 08/36\n",
            "Training Step: 4437  | total loss: \u001b[1m\u001b[32m2.00427\u001b[0m\u001b[0m | time: 0.009s\n",
            "| Adam | epoch: 888 | loss: 2.00427 - acc: 0.8931 -- iter: 16/36\n",
            "Training Step: 4438  | total loss: \u001b[1m\u001b[32m1.80996\u001b[0m\u001b[0m | time: 0.015s\n",
            "| Adam | epoch: 888 | loss: 1.80996 - acc: 0.8038 -- iter: 24/36\n",
            "Training Step: 4439  | total loss: \u001b[1m\u001b[32m1.63600\u001b[0m\u001b[0m | time: 0.020s\n",
            "| Adam | epoch: 888 | loss: 1.63600 - acc: 0.8234 -- iter: 32/36\n",
            "Training Step: 4440  | total loss: \u001b[1m\u001b[32m1.47952\u001b[0m\u001b[0m | time: 0.025s\n",
            "| Adam | epoch: 888 | loss: 1.47952 - acc: 0.8410 -- iter: 36/36\n",
            "--\n",
            "Training Step: 4441  | total loss: \u001b[1m\u001b[32m1.33812\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 889 | loss: 1.33812 - acc: 0.8569 -- iter: 08/36\n",
            "Training Step: 4442  | total loss: \u001b[1m\u001b[32m1.21132\u001b[0m\u001b[0m | time: 0.010s\n",
            "| Adam | epoch: 889 | loss: 1.21132 - acc: 0.8712 -- iter: 16/36\n",
            "Training Step: 4443  | total loss: \u001b[1m\u001b[32m1.92190\u001b[0m\u001b[0m | time: 0.015s\n",
            "| Adam | epoch: 889 | loss: 1.92190 - acc: 0.8841 -- iter: 24/36\n",
            "Training Step: 4444  | total loss: \u001b[1m\u001b[32m1.73535\u001b[0m\u001b[0m | time: 0.025s\n",
            "| Adam | epoch: 889 | loss: 1.73535 - acc: 0.7957 -- iter: 32/36\n",
            "Training Step: 4445  | total loss: \u001b[1m\u001b[32m1.56759\u001b[0m\u001b[0m | time: 0.032s\n",
            "| Adam | epoch: 889 | loss: 1.56759 - acc: 0.8161 -- iter: 36/36\n",
            "--\n",
            "Training Step: 4446  | total loss: \u001b[1m\u001b[32m1.56759\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 890 | loss: 1.56759 - acc: 0.8345 -- iter: 08/36\n",
            "Training Step: 4447  | total loss: \u001b[1m\u001b[32m1.41663\u001b[0m\u001b[0m | time: 0.009s\n",
            "| Adam | epoch: 890 | loss: 1.41663 - acc: 0.8511 -- iter: 16/36\n",
            "Training Step: 4448  | total loss: \u001b[1m\u001b[32m1.16150\u001b[0m\u001b[0m | time: 0.013s\n",
            "| Adam | epoch: 890 | loss: 1.16150 - acc: 0.8660 -- iter: 24/36\n",
            "Training Step: 4449  | total loss: \u001b[1m\u001b[32m1.78780\u001b[0m\u001b[0m | time: 0.020s\n",
            "| Adam | epoch: 890 | loss: 1.78780 - acc: 0.8794 -- iter: 32/36\n",
            "Training Step: 4450  | total loss: \u001b[1m\u001b[32m1.61711\u001b[0m\u001b[0m | time: 0.025s\n",
            "| Adam | epoch: 890 | loss: 1.61711 - acc: 0.8039 -- iter: 36/36\n",
            "--\n",
            "Training Step: 4451  | total loss: \u001b[1m\u001b[32m1.46000\u001b[0m\u001b[0m | time: 0.006s\n",
            "| Adam | epoch: 891 | loss: 1.46000 - acc: 0.8235 -- iter: 08/36\n",
            "Training Step: 4452  | total loss: \u001b[1m\u001b[32m1.31858\u001b[0m\u001b[0m | time: 0.011s\n",
            "| Adam | epoch: 891 | loss: 1.31858 - acc: 0.8412 -- iter: 16/36\n",
            "Training Step: 4453  | total loss: \u001b[1m\u001b[32m1.19349\u001b[0m\u001b[0m | time: 0.015s\n",
            "| Adam | epoch: 891 | loss: 1.19349 - acc: 0.8571 -- iter: 24/36\n",
            "Training Step: 4454  | total loss: \u001b[1m\u001b[32m1.07934\u001b[0m\u001b[0m | time: 0.021s\n",
            "| Adam | epoch: 891 | loss: 1.07934 - acc: 0.8714 -- iter: 32/36\n",
            "Training Step: 4455  | total loss: \u001b[1m\u001b[32m0.98050\u001b[0m\u001b[0m | time: 0.027s\n",
            "| Adam | epoch: 891 | loss: 0.98050 - acc: 0.8842 -- iter: 36/36\n",
            "--\n",
            "Training Step: 4456  | total loss: \u001b[1m\u001b[32m0.88890\u001b[0m\u001b[0m | time: 0.006s\n",
            "| Adam | epoch: 892 | loss: 0.88890 - acc: 0.8958 -- iter: 08/36\n",
            "Training Step: 4457  | total loss: \u001b[1m\u001b[32m0.80893\u001b[0m\u001b[0m | time: 0.011s\n",
            "| Adam | epoch: 892 | loss: 0.80893 - acc: 0.9062 -- iter: 16/36\n",
            "Training Step: 4458  | total loss: \u001b[1m\u001b[32m0.73687\u001b[0m\u001b[0m | time: 0.017s\n",
            "| Adam | epoch: 892 | loss: 0.73687 - acc: 0.9156 -- iter: 24/36\n",
            "Training Step: 4459  | total loss: \u001b[1m\u001b[32m0.67015\u001b[0m\u001b[0m | time: 0.024s\n",
            "| Adam | epoch: 892 | loss: 0.67015 - acc: 0.9240 -- iter: 32/36\n",
            "Training Step: 4460  | total loss: \u001b[1m\u001b[32m0.60977\u001b[0m\u001b[0m | time: 0.030s\n",
            "| Adam | epoch: 892 | loss: 0.60977 - acc: 0.9316 -- iter: 36/36\n",
            "--\n",
            "Training Step: 4461  | total loss: \u001b[1m\u001b[32m1.15038\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 893 | loss: 1.15038 - acc: 0.9385 -- iter: 08/36\n",
            "Training Step: 4462  | total loss: \u001b[1m\u001b[32m1.04236\u001b[0m\u001b[0m | time: 0.011s\n",
            "| Adam | epoch: 893 | loss: 1.04236 - acc: 0.8696 -- iter: 16/36\n",
            "Training Step: 4463  | total loss: \u001b[1m\u001b[32m0.94526\u001b[0m\u001b[0m | time: 0.016s\n",
            "| Adam | epoch: 893 | loss: 0.94526 - acc: 0.8827 -- iter: 24/36\n",
            "Training Step: 4464  | total loss: \u001b[1m\u001b[32m0.85783\u001b[0m\u001b[0m | time: 0.023s\n",
            "| Adam | epoch: 893 | loss: 0.85783 - acc: 0.8944 -- iter: 32/36\n",
            "Training Step: 4465  | total loss: \u001b[1m\u001b[32m0.77853\u001b[0m\u001b[0m | time: 0.029s\n",
            "| Adam | epoch: 893 | loss: 0.77853 - acc: 0.9050 -- iter: 36/36\n",
            "--\n",
            "Training Step: 4466  | total loss: \u001b[1m\u001b[32m0.70775\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 894 | loss: 0.70775 - acc: 0.9145 -- iter: 08/36\n",
            "Training Step: 4467  | total loss: \u001b[1m\u001b[32m1.52061\u001b[0m\u001b[0m | time: 0.011s\n",
            "| Adam | epoch: 894 | loss: 1.52061 - acc: 0.9230 -- iter: 16/36\n",
            "Training Step: 4468  | total loss: \u001b[1m\u001b[32m1.37772\u001b[0m\u001b[0m | time: 0.017s\n",
            "| Adam | epoch: 894 | loss: 1.37772 - acc: 0.8432 -- iter: 24/36\n",
            "Training Step: 4469  | total loss: \u001b[1m\u001b[32m1.24830\u001b[0m\u001b[0m | time: 0.021s\n",
            "| Adam | epoch: 894 | loss: 1.24830 - acc: 0.8589 -- iter: 32/36\n",
            "Training Step: 4470  | total loss: \u001b[1m\u001b[32m1.13184\u001b[0m\u001b[0m | time: 0.028s\n",
            "| Adam | epoch: 894 | loss: 1.13184 - acc: 0.8730 -- iter: 36/36\n",
            "--\n",
            "Training Step: 4471  | total loss: \u001b[1m\u001b[32m1.02517\u001b[0m\u001b[0m | time: 0.006s\n",
            "| Adam | epoch: 895 | loss: 1.02517 - acc: 0.8857 -- iter: 08/36\n",
            "Training Step: 4472  | total loss: \u001b[1m\u001b[32m0.92876\u001b[0m\u001b[0m | time: 0.011s\n",
            "| Adam | epoch: 895 | loss: 0.92876 - acc: 0.8971 -- iter: 16/36\n",
            "Training Step: 4473  | total loss: \u001b[1m\u001b[32m1.37986\u001b[0m\u001b[0m | time: 0.018s\n",
            "| Adam | epoch: 895 | loss: 1.37986 - acc: 0.9074 -- iter: 24/36\n",
            "Training Step: 4474  | total loss: \u001b[1m\u001b[32m1.24886\u001b[0m\u001b[0m | time: 0.025s\n",
            "| Adam | epoch: 895 | loss: 1.24886 - acc: 0.8542 -- iter: 32/36\n",
            "Training Step: 4475  | total loss: \u001b[1m\u001b[32m1.13404\u001b[0m\u001b[0m | time: 0.031s\n",
            "| Adam | epoch: 895 | loss: 1.13404 - acc: 0.8688 -- iter: 36/36\n",
            "--\n",
            "Training Step: 4476  | total loss: \u001b[1m\u001b[32m1.03073\u001b[0m\u001b[0m | time: 0.006s\n",
            "| Adam | epoch: 896 | loss: 1.03073 - acc: 0.8819 -- iter: 08/36\n",
            "Training Step: 4477  | total loss: \u001b[1m\u001b[32m0.93543\u001b[0m\u001b[0m | time: 0.012s\n",
            "| Adam | epoch: 896 | loss: 0.93543 - acc: 0.8937 -- iter: 16/36\n",
            "Training Step: 4478  | total loss: \u001b[1m\u001b[32m0.84910\u001b[0m\u001b[0m | time: 0.017s\n",
            "| Adam | epoch: 896 | loss: 0.84910 - acc: 0.9043 -- iter: 24/36\n",
            "Training Step: 4479  | total loss: \u001b[1m\u001b[32m1.76472\u001b[0m\u001b[0m | time: 0.022s\n",
            "| Adam | epoch: 896 | loss: 1.76472 - acc: 0.8350 -- iter: 32/36\n",
            "Training Step: 4480  | total loss: \u001b[1m\u001b[32m1.59536\u001b[0m\u001b[0m | time: 0.028s\n",
            "| Adam | epoch: 896 | loss: 1.59536 - acc: 0.8515 -- iter: 36/36\n",
            "--\n",
            "Training Step: 4481  | total loss: \u001b[1m\u001b[32m1.44576\u001b[0m\u001b[0m | time: 0.004s\n",
            "| Adam | epoch: 897 | loss: 1.44576 - acc: 0.8664 -- iter: 08/36\n",
            "Training Step: 4482  | total loss: \u001b[1m\u001b[32m1.31113\u001b[0m\u001b[0m | time: 0.009s\n",
            "| Adam | epoch: 897 | loss: 1.31113 - acc: 0.8797 -- iter: 16/36\n",
            "Training Step: 4483  | total loss: \u001b[1m\u001b[32m1.18681\u001b[0m\u001b[0m | time: 0.014s\n",
            "| Adam | epoch: 897 | loss: 1.18681 - acc: 0.8917 -- iter: 24/36\n",
            "Training Step: 4484  | total loss: \u001b[1m\u001b[32m1.18681\u001b[0m\u001b[0m | time: 0.021s\n",
            "| Adam | epoch: 897 | loss: 1.18681 - acc: 0.8917 -- iter: 32/36\n",
            "Training Step: 4485  | total loss: \u001b[1m\u001b[32m1.35792\u001b[0m\u001b[0m | time: 0.025s\n",
            "| Adam | epoch: 897 | loss: 1.35792 - acc: 0.8373 -- iter: 36/36\n",
            "--\n",
            "Training Step: 4486  | total loss: \u001b[1m\u001b[32m1.23041\u001b[0m\u001b[0m | time: 0.006s\n",
            "| Adam | epoch: 898 | loss: 1.23041 - acc: 0.8536 -- iter: 08/36\n",
            "Training Step: 4487  | total loss: \u001b[1m\u001b[32m1.11558\u001b[0m\u001b[0m | time: 0.010s\n",
            "| Adam | epoch: 898 | loss: 1.11558 - acc: 0.8682 -- iter: 16/36\n",
            "Training Step: 4488  | total loss: \u001b[1m\u001b[32m1.01224\u001b[0m\u001b[0m | time: 0.015s\n",
            "| Adam | epoch: 898 | loss: 1.01224 - acc: 0.8814 -- iter: 24/36\n",
            "Training Step: 4489  | total loss: \u001b[1m\u001b[32m0.91978\u001b[0m\u001b[0m | time: 0.020s\n",
            "| Adam | epoch: 898 | loss: 0.91978 - acc: 0.8933 -- iter: 32/36\n",
            "Training Step: 4490  | total loss: \u001b[1m\u001b[32m0.83524\u001b[0m\u001b[0m | time: 0.027s\n",
            "| Adam | epoch: 898 | loss: 0.83524 - acc: 0.9039 -- iter: 36/36\n",
            "--\n",
            "Training Step: 4491  | total loss: \u001b[1m\u001b[32m1.35365\u001b[0m\u001b[0m | time: 0.004s\n",
            "| Adam | epoch: 899 | loss: 1.35365 - acc: 0.8260 -- iter: 08/36\n",
            "Training Step: 4492  | total loss: \u001b[1m\u001b[32m1.22599\u001b[0m\u001b[0m | time: 0.012s\n",
            "| Adam | epoch: 899 | loss: 1.22599 - acc: 0.8434 -- iter: 16/36\n",
            "Training Step: 4493  | total loss: \u001b[1m\u001b[32m1.11057\u001b[0m\u001b[0m | time: 0.018s\n",
            "| Adam | epoch: 899 | loss: 1.11057 - acc: 0.8591 -- iter: 24/36\n",
            "Training Step: 4494  | total loss: \u001b[1m\u001b[32m1.00675\u001b[0m\u001b[0m | time: 0.024s\n",
            "| Adam | epoch: 899 | loss: 1.00675 - acc: 0.8732 -- iter: 32/36\n",
            "Training Step: 4495  | total loss: \u001b[1m\u001b[32m0.91333\u001b[0m\u001b[0m | time: 0.030s\n",
            "| Adam | epoch: 899 | loss: 0.91333 - acc: 0.8859 -- iter: 36/36\n",
            "--\n",
            "Training Step: 4496  | total loss: \u001b[1m\u001b[32m0.83128\u001b[0m\u001b[0m | time: 0.004s\n",
            "| Adam | epoch: 900 | loss: 0.83128 - acc: 0.8973 -- iter: 08/36\n",
            "Training Step: 4497  | total loss: \u001b[1m\u001b[32m1.35654\u001b[0m\u001b[0m | time: 0.011s\n",
            "| Adam | epoch: 900 | loss: 1.35654 - acc: 0.8201 -- iter: 16/36\n",
            "Training Step: 4498  | total loss: \u001b[1m\u001b[32m1.22938\u001b[0m\u001b[0m | time: 0.015s\n",
            "| Adam | epoch: 900 | loss: 1.22938 - acc: 0.8380 -- iter: 24/36\n",
            "Training Step: 4499  | total loss: \u001b[1m\u001b[32m1.11553\u001b[0m\u001b[0m | time: 0.019s\n",
            "| Adam | epoch: 900 | loss: 1.11553 - acc: 0.8542 -- iter: 32/36\n",
            "Training Step: 4500  | total loss: \u001b[1m\u001b[32m1.01306\u001b[0m\u001b[0m | time: 0.023s\n",
            "| Adam | epoch: 900 | loss: 1.01306 - acc: 0.8688 -- iter: 36/36\n",
            "--\n",
            "Training Step: 4501  | total loss: \u001b[1m\u001b[32m0.92003\u001b[0m\u001b[0m | time: 0.004s\n",
            "| Adam | epoch: 901 | loss: 0.92003 - acc: 0.8819 -- iter: 08/36\n",
            "Training Step: 4502  | total loss: \u001b[1m\u001b[32m0.83784\u001b[0m\u001b[0m | time: 0.012s\n",
            "| Adam | epoch: 901 | loss: 0.83784 - acc: 0.8937 -- iter: 16/36\n",
            "Training Step: 4503  | total loss: \u001b[1m\u001b[32m1.23537\u001b[0m\u001b[0m | time: 0.019s\n",
            "| Adam | epoch: 901 | loss: 1.23537 - acc: 0.8294 -- iter: 24/36\n",
            "Training Step: 4504  | total loss: \u001b[1m\u001b[32m1.12098\u001b[0m\u001b[0m | time: 0.024s\n",
            "| Adam | epoch: 901 | loss: 1.12098 - acc: 0.8464 -- iter: 32/36\n",
            "Training Step: 4505  | total loss: \u001b[1m\u001b[32m1.01643\u001b[0m\u001b[0m | time: 0.031s\n",
            "| Adam | epoch: 901 | loss: 1.01643 - acc: 0.8618 -- iter: 36/36\n",
            "--\n",
            "Training Step: 4506  | total loss: \u001b[1m\u001b[32m0.92233\u001b[0m\u001b[0m | time: 0.007s\n",
            "| Adam | epoch: 902 | loss: 0.92233 - acc: 0.8618 -- iter: 08/36\n",
            "Training Step: 4507  | total loss: \u001b[1m\u001b[32m0.83858\u001b[0m\u001b[0m | time: 0.013s\n",
            "| Adam | epoch: 902 | loss: 0.83858 - acc: 0.8756 -- iter: 16/36\n",
            "Training Step: 4508  | total loss: \u001b[1m\u001b[32m0.76461\u001b[0m\u001b[0m | time: 0.018s\n",
            "| Adam | epoch: 902 | loss: 0.76461 - acc: 0.8880 -- iter: 24/36\n",
            "Training Step: 4641  | total loss: \u001b[1m\u001b[32m0.08201\u001b[0m\u001b[0m | time: 0.003s\n",
            "| Adam | epoch: 929 | loss: 0.08201 - acc: 0.9999 -- iter: 08/36\n",
            "Training Step: 4642  | total loss: \u001b[1m\u001b[32m0.08060\u001b[0m\u001b[0m | time: 0.009s\n",
            "| Adam | epoch: 929 | loss: 0.08060 - acc: 0.9999 -- iter: 16/36\n",
            "Training Step: 4643  | total loss: \u001b[1m\u001b[32m0.08169\u001b[0m\u001b[0m | time: 0.014s\n",
            "| Adam | epoch: 929 | loss: 0.08169 - acc: 0.9999 -- iter: 24/36\n",
            "Training Step: 4644  | total loss: \u001b[1m\u001b[32m0.08169\u001b[0m\u001b[0m | time: 0.018s\n",
            "| Adam | epoch: 929 | loss: 0.08169 - acc: 1.0000 -- iter: 32/36\n",
            "Training Step: 4645  | total loss: \u001b[1m\u001b[32m0.08265\u001b[0m\u001b[0m | time: 0.022s\n",
            "| Adam | epoch: 929 | loss: 0.08265 - acc: 1.0000 -- iter: 36/36\n",
            "--\n",
            "Training Step: 4646  | total loss: \u001b[1m\u001b[32m0.08170\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 930 | loss: 0.08170 - acc: 1.0000 -- iter: 08/36\n",
            "Training Step: 4647  | total loss: \u001b[1m\u001b[32m0.08092\u001b[0m\u001b[0m | time: 0.008s\n",
            "| Adam | epoch: 930 | loss: 0.08092 - acc: 1.0000 -- iter: 16/36\n",
            "Training Step: 4648  | total loss: \u001b[1m\u001b[32m0.08065\u001b[0m\u001b[0m | time: 0.013s\n",
            "| Adam | epoch: 930 | loss: 0.08065 - acc: 1.0000 -- iter: 24/36\n",
            "Training Step: 4649  | total loss: \u001b[1m\u001b[32m0.07799\u001b[0m\u001b[0m | time: 0.016s\n",
            "| Adam | epoch: 930 | loss: 0.07799 - acc: 1.0000 -- iter: 32/36\n",
            "Training Step: 4650  | total loss: \u001b[1m\u001b[32m0.07689\u001b[0m\u001b[0m | time: 0.020s\n",
            "| Adam | epoch: 930 | loss: 0.07689 - acc: 1.0000 -- iter: 36/36\n",
            "--\n",
            "Training Step: 4651  | total loss: \u001b[1m\u001b[32m0.07587\u001b[0m\u001b[0m | time: 0.003s\n",
            "| Adam | epoch: 931 | loss: 0.07587 - acc: 1.0000 -- iter: 08/36\n",
            "Training Step: 4652  | total loss: \u001b[1m\u001b[32m0.07646\u001b[0m\u001b[0m | time: 0.011s\n",
            "| Adam | epoch: 931 | loss: 0.07646 - acc: 1.0000 -- iter: 16/36\n",
            "Training Step: 4653  | total loss: \u001b[1m\u001b[32m0.07575\u001b[0m\u001b[0m | time: 0.015s\n",
            "| Adam | epoch: 931 | loss: 0.07575 - acc: 1.0000 -- iter: 24/36\n",
            "Training Step: 4654  | total loss: \u001b[1m\u001b[32m0.07768\u001b[0m\u001b[0m | time: 0.019s\n",
            "| Adam | epoch: 931 | loss: 0.07768 - acc: 1.0000 -- iter: 32/36\n",
            "Training Step: 4655  | total loss: \u001b[1m\u001b[32m0.07714\u001b[0m\u001b[0m | time: 0.022s\n",
            "| Adam | epoch: 931 | loss: 0.07714 - acc: 1.0000 -- iter: 36/36\n",
            "--\n",
            "Training Step: 4656  | total loss: \u001b[1m\u001b[32m0.07560\u001b[0m\u001b[0m | time: 0.004s\n",
            "| Adam | epoch: 932 | loss: 0.07560 - acc: 1.0000 -- iter: 08/36\n",
            "Training Step: 4657  | total loss: \u001b[1m\u001b[32m0.07419\u001b[0m\u001b[0m | time: 0.007s\n",
            "| Adam | epoch: 932 | loss: 0.07419 - acc: 1.0000 -- iter: 16/36\n",
            "Training Step: 4658  | total loss: \u001b[1m\u001b[32m0.07374\u001b[0m\u001b[0m | time: 0.011s\n",
            "| Adam | epoch: 932 | loss: 0.07374 - acc: 1.0000 -- iter: 24/36\n",
            "Training Step: 4659  | total loss: \u001b[1m\u001b[32m0.07543\u001b[0m\u001b[0m | time: 0.017s\n",
            "| Adam | epoch: 932 | loss: 0.07543 - acc: 1.0000 -- iter: 32/36\n",
            "Training Step: 4660  | total loss: \u001b[1m\u001b[32m0.07433\u001b[0m\u001b[0m | time: 0.022s\n",
            "| Adam | epoch: 932 | loss: 0.07433 - acc: 1.0000 -- iter: 36/36\n",
            "--\n",
            "Training Step: 4661  | total loss: \u001b[1m\u001b[32m0.07523\u001b[0m\u001b[0m | time: 0.004s\n",
            "| Adam | epoch: 933 | loss: 0.07523 - acc: 1.0000 -- iter: 08/36\n",
            "Training Step: 4662  | total loss: \u001b[1m\u001b[32m0.07599\u001b[0m\u001b[0m | time: 0.010s\n",
            "| Adam | epoch: 933 | loss: 0.07599 - acc: 1.0000 -- iter: 16/36\n",
            "Training Step: 4663  | total loss: \u001b[1m\u001b[32m0.07664\u001b[0m\u001b[0m | time: 0.015s\n",
            "| Adam | epoch: 933 | loss: 0.07664 - acc: 1.0000 -- iter: 24/36\n",
            "Training Step: 4664  | total loss: \u001b[1m\u001b[32m0.07626\u001b[0m\u001b[0m | time: 0.019s\n",
            "| Adam | epoch: 933 | loss: 0.07626 - acc: 1.0000 -- iter: 32/36\n",
            "Training Step: 4665  | total loss: \u001b[1m\u001b[32m0.07427\u001b[0m\u001b[0m | time: 0.023s\n",
            "| Adam | epoch: 933 | loss: 0.07427 - acc: 1.0000 -- iter: 36/36\n",
            "--\n",
            "Training Step: 4666  | total loss: \u001b[1m\u001b[32m0.77690\u001b[0m\u001b[0m | time: 0.003s\n",
            "| Adam | epoch: 934 | loss: 0.77690 - acc: 0.9000 -- iter: 08/36\n",
            "Training Step: 4667  | total loss: \u001b[1m\u001b[32m0.70511\u001b[0m\u001b[0m | time: 0.008s\n",
            "| Adam | epoch: 934 | loss: 0.70511 - acc: 0.9100 -- iter: 16/36\n",
            "Training Step: 4668  | total loss: \u001b[1m\u001b[32m0.64058\u001b[0m\u001b[0m | time: 0.010s\n",
            "| Adam | epoch: 934 | loss: 0.64058 - acc: 0.9190 -- iter: 24/36\n",
            "Training Step: 4669  | total loss: \u001b[1m\u001b[32m0.58252\u001b[0m\u001b[0m | time: 0.016s\n",
            "| Adam | epoch: 934 | loss: 0.58252 - acc: 0.9271 -- iter: 32/36\n",
            "Training Step: 4670  | total loss: \u001b[1m\u001b[32m0.53253\u001b[0m\u001b[0m | time: 0.021s\n",
            "| Adam | epoch: 934 | loss: 0.53253 - acc: 0.9344 -- iter: 36/36\n",
            "--\n",
            "Training Step: 4671  | total loss: \u001b[1m\u001b[32m0.48583\u001b[0m\u001b[0m | time: 0.004s\n",
            "| Adam | epoch: 935 | loss: 0.48583 - acc: 0.9409 -- iter: 08/36\n",
            "Training Step: 4672  | total loss: \u001b[1m\u001b[32m0.44593\u001b[0m\u001b[0m | time: 0.008s\n",
            "| Adam | epoch: 935 | loss: 0.44593 - acc: 0.9469 -- iter: 16/36\n",
            "Training Step: 4673  | total loss: \u001b[1m\u001b[32m0.40746\u001b[0m\u001b[0m | time: 0.016s\n",
            "| Adam | epoch: 935 | loss: 0.40746 - acc: 0.9522 -- iter: 24/36\n",
            "Training Step: 4674  | total loss: \u001b[1m\u001b[32m0.37504\u001b[0m\u001b[0m | time: 0.024s\n",
            "| Adam | epoch: 935 | loss: 0.37504 - acc: 0.9570 -- iter: 32/36\n",
            "Training Step: 4675  | total loss: \u001b[1m\u001b[32m0.34586\u001b[0m\u001b[0m | time: 0.029s\n",
            "| Adam | epoch: 935 | loss: 0.34586 - acc: 0.9613 -- iter: 36/36\n",
            "--\n",
            "Training Step: 4676  | total loss: \u001b[1m\u001b[32m0.31737\u001b[0m\u001b[0m | time: 0.004s\n",
            "| Adam | epoch: 936 | loss: 0.31737 - acc: 0.9651 -- iter: 08/36\n",
            "Training Step: 4677  | total loss: \u001b[1m\u001b[32m0.29351\u001b[0m\u001b[0m | time: 0.009s\n",
            "| Adam | epoch: 936 | loss: 0.29351 - acc: 0.9686 -- iter: 16/36\n",
            "Training Step: 4678  | total loss: \u001b[1m\u001b[32m0.27266\u001b[0m\u001b[0m | time: 0.013s\n",
            "| Adam | epoch: 936 | loss: 0.27266 - acc: 0.9718 -- iter: 24/36\n",
            "Training Step: 4679  | total loss: \u001b[1m\u001b[32m0.25193\u001b[0m\u001b[0m | time: 0.019s\n",
            "| Adam | epoch: 936 | loss: 0.25193 - acc: 0.9746 -- iter: 32/36\n",
            "Training Step: 4680  | total loss: \u001b[1m\u001b[32m0.23386\u001b[0m\u001b[0m | time: 0.026s\n",
            "| Adam | epoch: 936 | loss: 0.23386 - acc: 0.9771 -- iter: 36/36\n",
            "--\n",
            "Training Step: 4681  | total loss: \u001b[1m\u001b[32m0.21758\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 937 | loss: 0.21758 - acc: 0.9794 -- iter: 08/36\n",
            "Training Step: 4682  | total loss: \u001b[1m\u001b[32m0.20195\u001b[0m\u001b[0m | time: 0.013s\n",
            "| Adam | epoch: 937 | loss: 0.20195 - acc: 0.9815 -- iter: 16/36\n",
            "Training Step: 4683  | total loss: \u001b[1m\u001b[32m0.18969\u001b[0m\u001b[0m | time: 0.020s\n",
            "| Adam | epoch: 937 | loss: 0.18969 - acc: 0.9833 -- iter: 24/36\n",
            "Training Step: 4684  | total loss: \u001b[1m\u001b[32m0.17922\u001b[0m\u001b[0m | time: 0.024s\n",
            "| Adam | epoch: 937 | loss: 0.17922 - acc: 0.9850 -- iter: 32/36\n",
            "Training Step: 4685  | total loss: \u001b[1m\u001b[32m0.16729\u001b[0m\u001b[0m | time: 0.029s\n",
            "| Adam | epoch: 937 | loss: 0.16729 - acc: 0.9865 -- iter: 36/36\n",
            "--\n",
            "Training Step: 4686  | total loss: \u001b[1m\u001b[32m0.15974\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 938 | loss: 0.15974 - acc: 0.9878 -- iter: 08/36\n",
            "Training Step: 4687  | total loss: \u001b[1m\u001b[32m0.15290\u001b[0m\u001b[0m | time: 0.011s\n",
            "| Adam | epoch: 938 | loss: 0.15290 - acc: 0.9891 -- iter: 16/36\n",
            "Training Step: 4688  | total loss: \u001b[1m\u001b[32m0.14593\u001b[0m\u001b[0m | time: 0.014s\n",
            "| Adam | epoch: 938 | loss: 0.14593 - acc: 0.9902 -- iter: 24/36\n",
            "Training Step: 4689  | total loss: \u001b[1m\u001b[32m0.13848\u001b[0m\u001b[0m | time: 0.019s\n",
            "| Adam | epoch: 938 | loss: 0.13848 - acc: 0.9911 -- iter: 32/36\n",
            "Training Step: 4690  | total loss: \u001b[1m\u001b[32m0.13101\u001b[0m\u001b[0m | time: 0.026s\n",
            "| Adam | epoch: 938 | loss: 0.13101 - acc: 0.9920 -- iter: 36/36\n",
            "--\n",
            "Training Step: 4691  | total loss: \u001b[1m\u001b[32m0.12543\u001b[0m\u001b[0m | time: 0.004s\n",
            "| Adam | epoch: 939 | loss: 0.12543 - acc: 0.9928 -- iter: 08/36\n",
            "Training Step: 4692  | total loss: \u001b[1m\u001b[32m0.11991\u001b[0m\u001b[0m | time: 0.010s\n",
            "| Adam | epoch: 939 | loss: 0.11991 - acc: 0.9935 -- iter: 16/36\n",
            "Training Step: 4693  | total loss: \u001b[1m\u001b[32m0.11492\u001b[0m\u001b[0m | time: 0.014s\n",
            "| Adam | epoch: 939 | loss: 0.11492 - acc: 0.9942 -- iter: 24/36\n",
            "Training Step: 4694  | total loss: \u001b[1m\u001b[32m0.11039\u001b[0m\u001b[0m | time: 0.019s\n",
            "| Adam | epoch: 939 | loss: 0.11039 - acc: 0.9948 -- iter: 32/36\n",
            "Training Step: 4695  | total loss: \u001b[1m\u001b[32m0.10635\u001b[0m\u001b[0m | time: 0.023s\n",
            "| Adam | epoch: 939 | loss: 0.10635 - acc: 0.9953 -- iter: 36/36\n",
            "--\n",
            "Training Step: 4696  | total loss: \u001b[1m\u001b[32m0.10271\u001b[0m\u001b[0m | time: 0.004s\n",
            "| Adam | epoch: 940 | loss: 0.10271 - acc: 0.9958 -- iter: 08/36\n",
            "Training Step: 4697  | total loss: \u001b[1m\u001b[32m0.10015\u001b[0m\u001b[0m | time: 0.008s\n",
            "| Adam | epoch: 940 | loss: 0.10015 - acc: 0.9962 -- iter: 16/36\n",
            "Training Step: 4698  | total loss: \u001b[1m\u001b[32m0.09728\u001b[0m\u001b[0m | time: 0.013s\n",
            "| Adam | epoch: 940 | loss: 0.09728 - acc: 0.9966 -- iter: 24/36\n",
            "Training Step: 4699  | total loss: \u001b[1m\u001b[32m0.09467\u001b[0m\u001b[0m | time: 0.017s\n",
            "| Adam | epoch: 940 | loss: 0.09467 - acc: 0.9969 -- iter: 32/36\n",
            "Training Step: 4700  | total loss: \u001b[1m\u001b[32m0.09118\u001b[0m\u001b[0m | time: 0.021s\n",
            "| Adam | epoch: 940 | loss: 0.09118 - acc: 0.9972 -- iter: 36/36\n",
            "--\n",
            "Training Step: 4701  | total loss: \u001b[1m\u001b[32m0.08911\u001b[0m\u001b[0m | time: 0.004s\n",
            "| Adam | epoch: 941 | loss: 0.08911 - acc: 0.9975 -- iter: 08/36\n",
            "Training Step: 4702  | total loss: \u001b[1m\u001b[32m0.08737\u001b[0m\u001b[0m | time: 0.009s\n",
            "| Adam | epoch: 941 | loss: 0.08737 - acc: 0.9977 -- iter: 16/36\n",
            "Training Step: 4703  | total loss: \u001b[1m\u001b[32m0.08592\u001b[0m\u001b[0m | time: 0.014s\n",
            "| Adam | epoch: 941 | loss: 0.08592 - acc: 0.9980 -- iter: 24/36\n",
            "Training Step: 4704  | total loss: \u001b[1m\u001b[32m0.08226\u001b[0m\u001b[0m | time: 0.020s\n",
            "| Adam | epoch: 941 | loss: 0.08226 - acc: 0.9982 -- iter: 32/36\n",
            "Training Step: 4705  | total loss: \u001b[1m\u001b[32m0.07892\u001b[0m\u001b[0m | time: 0.026s\n",
            "| Adam | epoch: 941 | loss: 0.07892 - acc: 0.9984 -- iter: 36/36\n",
            "--\n",
            "Training Step: 4706  | total loss: \u001b[1m\u001b[32m0.07778\u001b[0m\u001b[0m | time: 0.006s\n",
            "| Adam | epoch: 942 | loss: 0.07778 - acc: 0.9985 -- iter: 08/36\n",
            "Training Step: 4707  | total loss: \u001b[1m\u001b[32m0.07721\u001b[0m\u001b[0m | time: 0.011s\n",
            "| Adam | epoch: 942 | loss: 0.07721 - acc: 0.9987 -- iter: 16/36\n",
            "Training Step: 4708  | total loss: \u001b[1m\u001b[32m0.07679\u001b[0m\u001b[0m | time: 0.017s\n",
            "| Adam | epoch: 942 | loss: 0.07679 - acc: 0.9988 -- iter: 24/36\n",
            "Training Step: 4709  | total loss: \u001b[1m\u001b[32m0.07669\u001b[0m\u001b[0m | time: 0.023s\n",
            "| Adam | epoch: 942 | loss: 0.07669 - acc: 0.9989 -- iter: 32/36\n",
            "Training Step: 4710  | total loss: \u001b[1m\u001b[32m0.07382\u001b[0m\u001b[0m | time: 0.029s\n",
            "| Adam | epoch: 942 | loss: 0.07382 - acc: 0.9990 -- iter: 36/36\n",
            "--\n",
            "Training Step: 4711  | total loss: \u001b[1m\u001b[32m0.07119\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 943 | loss: 0.07119 - acc: 0.9991 -- iter: 08/36\n",
            "Training Step: 4712  | total loss: \u001b[1m\u001b[32m0.07060\u001b[0m\u001b[0m | time: 0.009s\n",
            "| Adam | epoch: 943 | loss: 0.07060 - acc: 0.9992 -- iter: 16/36\n",
            "Training Step: 4713  | total loss: \u001b[1m\u001b[32m0.07155\u001b[0m\u001b[0m | time: 0.014s\n",
            "| Adam | epoch: 943 | loss: 0.07155 - acc: 0.9993 -- iter: 24/36\n",
            "Training Step: 4714  | total loss: \u001b[1m\u001b[32m0.07041\u001b[0m\u001b[0m | time: 0.018s\n",
            "| Adam | epoch: 943 | loss: 0.07041 - acc: 0.9994 -- iter: 32/36\n",
            "Training Step: 4715  | total loss: \u001b[1m\u001b[32m0.06891\u001b[0m\u001b[0m | time: 0.021s\n",
            "| Adam | epoch: 943 | loss: 0.06891 - acc: 0.9994 -- iter: 36/36\n",
            "--\n",
            "Training Step: 4716  | total loss: \u001b[1m\u001b[32m0.06999\u001b[0m\u001b[0m | time: 0.003s\n",
            "| Adam | epoch: 944 | loss: 0.06999 - acc: 0.9995 -- iter: 08/36\n",
            "Training Step: 4717  | total loss: \u001b[1m\u001b[32m0.07094\u001b[0m\u001b[0m | time: 0.008s\n",
            "| Adam | epoch: 944 | loss: 0.07094 - acc: 0.9995 -- iter: 16/36\n",
            "Training Step: 4718  | total loss: \u001b[1m\u001b[32m0.06962\u001b[0m\u001b[0m | time: 0.013s\n",
            "| Adam | epoch: 944 | loss: 0.06962 - acc: 0.9996 -- iter: 24/36\n",
            "Training Step: 4719  | total loss: \u001b[1m\u001b[32m0.06945\u001b[0m\u001b[0m | time: 0.018s\n",
            "| Adam | epoch: 944 | loss: 0.06945 - acc: 0.9996 -- iter: 32/36\n",
            "Training Step: 4720  | total loss: \u001b[1m\u001b[32m0.07042\u001b[0m\u001b[0m | time: 0.022s\n",
            "| Adam | epoch: 944 | loss: 0.07042 - acc: 0.9997 -- iter: 36/36\n",
            "--\n",
            "Training Step: 4721  | total loss: \u001b[1m\u001b[32m0.06961\u001b[0m\u001b[0m | time: 0.004s\n",
            "| Adam | epoch: 945 | loss: 0.06961 - acc: 0.9997 -- iter: 08/36\n",
            "Training Step: 4722  | total loss: \u001b[1m\u001b[32m0.06929\u001b[0m\u001b[0m | time: 0.008s\n",
            "| Adam | epoch: 945 | loss: 0.06929 - acc: 0.9997 -- iter: 16/36\n",
            "Training Step: 4723  | total loss: \u001b[1m\u001b[32m0.06897\u001b[0m\u001b[0m | time: 0.013s\n",
            "| Adam | epoch: 945 | loss: 0.06897 - acc: 0.9998 -- iter: 24/36\n",
            "Training Step: 4724  | total loss: \u001b[1m\u001b[32m0.06873\u001b[0m\u001b[0m | time: 0.017s\n",
            "| Adam | epoch: 945 | loss: 0.06873 - acc: 0.9998 -- iter: 32/36\n",
            "Training Step: 4725  | total loss: \u001b[1m\u001b[32m0.06915\u001b[0m\u001b[0m | time: 0.021s\n",
            "| Adam | epoch: 945 | loss: 0.06915 - acc: 0.9998 -- iter: 36/36\n",
            "--\n",
            "Training Step: 4726  | total loss: \u001b[1m\u001b[32m0.06830\u001b[0m\u001b[0m | time: 0.004s\n",
            "| Adam | epoch: 946 | loss: 0.06830 - acc: 0.9998 -- iter: 08/36\n",
            "Training Step: 4727  | total loss: \u001b[1m\u001b[32m0.06794\u001b[0m\u001b[0m | time: 0.008s\n",
            "| Adam | epoch: 946 | loss: 0.06794 - acc: 0.9999 -- iter: 16/36\n",
            "Training Step: 4728  | total loss: \u001b[1m\u001b[32m0.06753\u001b[0m\u001b[0m | time: 0.013s\n",
            "| Adam | epoch: 946 | loss: 0.06753 - acc: 0.9999 -- iter: 24/36\n",
            "Training Step: 4729  | total loss: \u001b[1m\u001b[32m0.06715\u001b[0m\u001b[0m | time: 0.019s\n",
            "| Adam | epoch: 946 | loss: 0.06715 - acc: 0.9999 -- iter: 32/36\n",
            "Training Step: 4730  | total loss: \u001b[1m\u001b[32m0.06637\u001b[0m\u001b[0m | time: 0.026s\n",
            "| Adam | epoch: 946 | loss: 0.06637 - acc: 0.9999 -- iter: 36/36\n",
            "--\n",
            "Training Step: 4731  | total loss: \u001b[1m\u001b[32m0.06640\u001b[0m\u001b[0m | time: 0.004s\n",
            "| Adam | epoch: 947 | loss: 0.06640 - acc: 0.9999 -- iter: 08/36\n",
            "Training Step: 4732  | total loss: \u001b[1m\u001b[32m0.06660\u001b[0m\u001b[0m | time: 0.010s\n",
            "| Adam | epoch: 947 | loss: 0.06660 - acc: 0.9999 -- iter: 16/36\n",
            "Training Step: 4733  | total loss: \u001b[1m\u001b[32m0.06638\u001b[0m\u001b[0m | time: 0.014s\n",
            "| Adam | epoch: 947 | loss: 0.06638 - acc: 0.9999 -- iter: 24/36\n",
            "Training Step: 4734  | total loss: \u001b[1m\u001b[32m0.07036\u001b[0m\u001b[0m | time: 0.020s\n",
            "| Adam | epoch: 947 | loss: 0.07036 - acc: 0.9999 -- iter: 32/36\n",
            "Training Step: 4735  | total loss: \u001b[1m\u001b[32m0.06947\u001b[0m\u001b[0m | time: 0.024s\n",
            "| Adam | epoch: 947 | loss: 0.06947 - acc: 0.9999 -- iter: 36/36\n",
            "--\n",
            "Training Step: 4736  | total loss: \u001b[1m\u001b[32m0.06791\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 948 | loss: 0.06791 - acc: 0.9999 -- iter: 08/36\n",
            "Training Step: 4737  | total loss: \u001b[1m\u001b[32m0.06748\u001b[0m\u001b[0m | time: 0.009s\n",
            "| Adam | epoch: 948 | loss: 0.06748 - acc: 0.9999 -- iter: 16/36\n",
            "Training Step: 4738  | total loss: \u001b[1m\u001b[32m0.06828\u001b[0m\u001b[0m | time: 0.013s\n",
            "| Adam | epoch: 948 | loss: 0.06828 - acc: 0.9999 -- iter: 24/36\n",
            "Training Step: 4739  | total loss: \u001b[1m\u001b[32m0.06543\u001b[0m\u001b[0m | time: 0.018s\n",
            "| Adam | epoch: 948 | loss: 0.06543 - acc: 1.0000 -- iter: 32/36\n",
            "Training Step: 4740  | total loss: \u001b[1m\u001b[32m0.06284\u001b[0m\u001b[0m | time: 0.022s\n",
            "| Adam | epoch: 948 | loss: 0.06284 - acc: 1.0000 -- iter: 36/36\n",
            "--\n",
            "Training Step: 4741  | total loss: \u001b[1m\u001b[32m0.06309\u001b[0m\u001b[0m | time: 0.004s\n",
            "| Adam | epoch: 949 | loss: 0.06309 - acc: 1.0000 -- iter: 08/36\n",
            "Training Step: 4742  | total loss: \u001b[1m\u001b[32m0.06316\u001b[0m\u001b[0m | time: 0.009s\n",
            "| Adam | epoch: 949 | loss: 0.06316 - acc: 1.0000 -- iter: 16/36\n",
            "Training Step: 4743  | total loss: \u001b[1m\u001b[32m0.68598\u001b[0m\u001b[0m | time: 0.014s\n",
            "| Adam | epoch: 949 | loss: 0.68598 - acc: 1.0000 -- iter: 24/36\n",
            "Training Step: 4744  | total loss: \u001b[1m\u001b[32m0.62336\u001b[0m\u001b[0m | time: 0.019s\n",
            "| Adam | epoch: 949 | loss: 0.62336 - acc: 0.9250 -- iter: 32/36\n",
            "Training Step: 4745  | total loss: \u001b[1m\u001b[32m0.56846\u001b[0m\u001b[0m | time: 0.024s\n",
            "| Adam | epoch: 949 | loss: 0.56846 - acc: 0.9325 -- iter: 36/36\n",
            "--\n",
            "Training Step: 4746  | total loss: \u001b[1m\u001b[32m0.51907\u001b[0m\u001b[0m | time: 0.004s\n",
            "| Adam | epoch: 950 | loss: 0.51907 - acc: 0.9453 -- iter: 08/36\n",
            "Training Step: 4747  | total loss: \u001b[1m\u001b[32m0.47404\u001b[0m\u001b[0m | time: 0.010s\n",
            "| Adam | epoch: 950 | loss: 0.47404 - acc: 0.9508 -- iter: 16/36\n",
            "Training Step: 4748  | total loss: \u001b[1m\u001b[32m0.43229\u001b[0m\u001b[0m | time: 0.014s\n",
            "| Adam | epoch: 950 | loss: 0.43229 - acc: 0.9508 -- iter: 24/36\n",
            "Training Step: 4749  | total loss: \u001b[1m\u001b[32m1.05119\u001b[0m\u001b[0m | time: 0.018s\n",
            "| Adam | epoch: 950 | loss: 1.05119 - acc: 0.9557 -- iter: 32/36\n",
            "Training Step: 4750  | total loss: \u001b[1m\u001b[32m0.95183\u001b[0m\u001b[0m | time: 0.022s\n",
            "| Adam | epoch: 950 | loss: 0.95183 - acc: 0.8851 -- iter: 36/36\n",
            "--\n",
            "Training Step: 4751  | total loss: \u001b[1m\u001b[32m0.86464\u001b[0m\u001b[0m | time: 0.004s\n",
            "| Adam | epoch: 951 | loss: 0.86464 - acc: 0.8966 -- iter: 08/36\n",
            "Training Step: 4752  | total loss: \u001b[1m\u001b[32m0.78619\u001b[0m\u001b[0m | time: 0.009s\n",
            "| Adam | epoch: 951 | loss: 0.78619 - acc: 0.9070 -- iter: 16/36\n",
            "Training Step: 4753  | total loss: \u001b[1m\u001b[32m0.71309\u001b[0m\u001b[0m | time: 0.014s\n",
            "| Adam | epoch: 951 | loss: 0.71309 - acc: 0.9163 -- iter: 24/36\n",
            "Training Step: 4754  | total loss: \u001b[1m\u001b[32m0.64928\u001b[0m\u001b[0m | time: 0.018s\n",
            "| Adam | epoch: 951 | loss: 0.64928 - acc: 0.9246 -- iter: 32/36\n",
            "Training Step: 4755  | total loss: \u001b[1m\u001b[32m1.40776\u001b[0m\u001b[0m | time: 0.023s\n",
            "| Adam | epoch: 951 | loss: 1.40776 - acc: 0.9322 -- iter: 36/36\n",
            "--\n",
            "Training Step: 4756  | total loss: \u001b[1m\u001b[32m1.27158\u001b[0m\u001b[0m | time: 0.004s\n",
            "| Adam | epoch: 952 | loss: 1.27158 - acc: 0.8515 -- iter: 08/36\n",
            "Training Step: 4757  | total loss: \u001b[1m\u001b[32m1.15202\u001b[0m\u001b[0m | time: 0.009s\n",
            "| Adam | epoch: 952 | loss: 1.15202 - acc: 0.8663 -- iter: 16/36\n",
            "Training Step: 4758  | total loss: \u001b[1m\u001b[32m1.04451\u001b[0m\u001b[0m | time: 0.013s\n",
            "| Adam | epoch: 952 | loss: 1.04451 - acc: 0.8797 -- iter: 24/36\n",
            "Training Step: 4759  | total loss: \u001b[1m\u001b[32m0.94728\u001b[0m\u001b[0m | time: 0.018s\n",
            "| Adam | epoch: 952 | loss: 0.94728 - acc: 0.8917 -- iter: 32/36\n",
            "Training Step: 4760  | total loss: \u001b[1m\u001b[32m0.86006\u001b[0m\u001b[0m | time: 0.022s\n",
            "| Adam | epoch: 952 | loss: 0.86006 - acc: 0.9025 -- iter: 36/36\n",
            "--\n",
            "Training Step: 4761  | total loss: \u001b[1m\u001b[32m1.54114\u001b[0m\u001b[0m | time: 0.004s\n",
            "| Adam | epoch: 953 | loss: 1.54114 - acc: 0.9123 -- iter: 08/36\n",
            "Training Step: 4762  | total loss: \u001b[1m\u001b[32m1.39367\u001b[0m\u001b[0m | time: 0.009s\n",
            "| Adam | epoch: 953 | loss: 1.39367 - acc: 0.8336 -- iter: 16/36\n",
            "Training Step: 4763  | total loss: \u001b[1m\u001b[32m1.25933\u001b[0m\u001b[0m | time: 0.015s\n",
            "| Adam | epoch: 953 | loss: 1.25933 - acc: 0.8502 -- iter: 24/36\n",
            "Training Step: 4764  | total loss: \u001b[1m\u001b[32m1.13847\u001b[0m\u001b[0m | time: 0.020s\n",
            "| Adam | epoch: 953 | loss: 1.13847 - acc: 0.8652 -- iter: 32/36\n",
            "Training Step: 4765  | total loss: \u001b[1m\u001b[32m1.03290\u001b[0m\u001b[0m | time: 0.024s\n",
            "| Adam | epoch: 953 | loss: 1.03290 - acc: 0.8787 -- iter: 36/36\n",
            "--\n",
            "Training Step: 4766  | total loss: \u001b[1m\u001b[32m0.93723\u001b[0m\u001b[0m | time: 0.007s\n",
            "| Adam | epoch: 954 | loss: 0.93723 - acc: 0.8908 -- iter: 08/36\n",
            "Training Step: 4767  | total loss: \u001b[1m\u001b[32m0.85062\u001b[0m\u001b[0m | time: 0.012s\n",
            "| Adam | epoch: 954 | loss: 0.85062 - acc: 0.9017 -- iter: 16/36\n",
            "Training Step: 4768  | total loss: \u001b[1m\u001b[32m0.85062\u001b[0m\u001b[0m | time: 0.017s\n",
            "| Adam | epoch: 954 | loss: 0.85062 - acc: 0.9115 -- iter: 24/36\n",
            "Training Step: 4769  | total loss: \u001b[1m\u001b[32m0.77272\u001b[0m\u001b[0m | time: 0.022s\n",
            "| Adam | epoch: 954 | loss: 0.77272 - acc: 0.9204 -- iter: 32/36\n",
            "Training Step: 4770  | total loss: \u001b[1m\u001b[32m0.70226\u001b[0m\u001b[0m | time: 0.027s\n",
            "| Adam | epoch: 954 | loss: 0.70226 - acc: 0.9284 -- iter: 36/36\n",
            "--\n",
            "Training Step: 4771  | total loss: \u001b[1m\u001b[32m0.63885\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 955 | loss: 0.63885 - acc: 0.9420 -- iter: 08/36\n",
            "Training Step: 4772  | total loss: \u001b[1m\u001b[32m0.53092\u001b[0m\u001b[0m | time: 0.010s\n",
            "| Adam | epoch: 955 | loss: 0.53092 - acc: 0.9420 -- iter: 16/36\n",
            "Training Step: 4773  | total loss: \u001b[1m\u001b[32m0.48641\u001b[0m\u001b[0m | time: 0.017s\n",
            "| Adam | epoch: 955 | loss: 0.48641 - acc: 0.9478 -- iter: 24/36\n",
            "Training Step: 4774  | total loss: \u001b[1m\u001b[32m0.44625\u001b[0m\u001b[0m | time: 0.022s\n",
            "| Adam | epoch: 955 | loss: 0.44625 - acc: 0.9530 -- iter: 32/36\n",
            "Training Step: 4775  | total loss: \u001b[1m\u001b[32m0.40924\u001b[0m\u001b[0m | time: 0.027s\n",
            "| Adam | epoch: 955 | loss: 0.40924 - acc: 0.9577 -- iter: 36/36\n",
            "--\n",
            "Training Step: 4776  | total loss: \u001b[1m\u001b[32m0.37594\u001b[0m\u001b[0m | time: 0.004s\n",
            "| Adam | epoch: 956 | loss: 0.37594 - acc: 0.9619 -- iter: 08/36\n",
            "Training Step: 4777  | total loss: \u001b[1m\u001b[32m0.34504\u001b[0m\u001b[0m | time: 0.010s\n",
            "| Adam | epoch: 956 | loss: 0.34504 - acc: 0.9657 -- iter: 16/36\n",
            "Training Step: 4778  | total loss: \u001b[1m\u001b[32m0.31866\u001b[0m\u001b[0m | time: 0.015s\n",
            "| Adam | epoch: 956 | loss: 0.31866 - acc: 0.9692 -- iter: 24/36\n",
            "Training Step: 4779  | total loss: \u001b[1m\u001b[32m0.29350\u001b[0m\u001b[0m | time: 0.020s\n",
            "| Adam | epoch: 956 | loss: 0.29350 - acc: 0.9722 -- iter: 32/36\n",
            "Training Step: 4780  | total loss: \u001b[1m\u001b[32m0.27042\u001b[0m\u001b[0m | time: 0.024s\n",
            "| Adam | epoch: 956 | loss: 0.27042 - acc: 0.9750 -- iter: 36/36\n",
            "--\n",
            "Training Step: 4781  | total loss: \u001b[1m\u001b[32m0.25143\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 957 | loss: 0.25143 - acc: 0.9775 -- iter: 08/36\n",
            "Training Step: 4782  | total loss: \u001b[1m\u001b[32m0.23432\u001b[0m\u001b[0m | time: 0.012s\n",
            "| Adam | epoch: 957 | loss: 0.23432 - acc: 0.9798 -- iter: 16/36\n",
            "Training Step: 4783  | total loss: \u001b[1m\u001b[32m0.21833\u001b[0m\u001b[0m | time: 0.018s\n",
            "| Adam | epoch: 957 | loss: 0.21833 - acc: 0.9818 -- iter: 24/36\n",
            "Training Step: 4784  | total loss: \u001b[1m\u001b[32m0.20486\u001b[0m\u001b[0m | time: 0.023s\n",
            "| Adam | epoch: 957 | loss: 0.20486 - acc: 0.9836 -- iter: 32/36\n",
            "Training Step: 4785  | total loss: \u001b[1m\u001b[32m0.93706\u001b[0m\u001b[0m | time: 0.027s\n",
            "| Adam | epoch: 957 | loss: 0.93706 - acc: 0.9852 -- iter: 36/36\n",
            "--\n",
            "Training Step: 4786  | total loss: \u001b[1m\u001b[32m0.84974\u001b[0m\u001b[0m | time: 0.004s\n",
            "| Adam | epoch: 958 | loss: 0.84974 - acc: 0.8992 -- iter: 08/36\n",
            "Training Step: 4787  | total loss: \u001b[1m\u001b[32m0.77213\u001b[0m\u001b[0m | time: 0.009s\n",
            "| Adam | epoch: 958 | loss: 0.77213 - acc: 0.9093 -- iter: 16/36\n",
            "Training Step: 4788  | total loss: \u001b[1m\u001b[32m0.70232\u001b[0m\u001b[0m | time: 0.013s\n",
            "| Adam | epoch: 958 | loss: 0.70232 - acc: 0.9184 -- iter: 24/36\n",
            "Training Step: 4789  | total loss: \u001b[1m\u001b[32m0.63871\u001b[0m\u001b[0m | time: 0.018s\n",
            "| Adam | epoch: 958 | loss: 0.63871 - acc: 0.9265 -- iter: 32/36\n",
            "Training Step: 4790  | total loss: \u001b[1m\u001b[32m0.58197\u001b[0m\u001b[0m | time: 0.022s\n",
            "| Adam | epoch: 958 | loss: 0.58197 - acc: 0.9339 -- iter: 36/36\n",
            "--\n",
            "Training Step: 4791  | total loss: \u001b[1m\u001b[32m1.12255\u001b[0m\u001b[0m | time: 0.003s\n",
            "| Adam | epoch: 959 | loss: 1.12255 - acc: 0.9405 -- iter: 08/36\n",
            "Training Step: 4792  | total loss: \u001b[1m\u001b[32m1.01950\u001b[0m\u001b[0m | time: 0.008s\n",
            "| Adam | epoch: 959 | loss: 1.01950 - acc: 0.8589 -- iter: 16/36\n",
            "Training Step: 4793  | total loss: \u001b[1m\u001b[32m0.92373\u001b[0m\u001b[0m | time: 0.013s\n",
            "| Adam | epoch: 959 | loss: 0.92373 - acc: 0.8730 -- iter: 24/36\n",
            "Training Step: 4794  | total loss: \u001b[1m\u001b[32m0.83755\u001b[0m\u001b[0m | time: 0.018s\n",
            "| Adam | epoch: 959 | loss: 0.83755 - acc: 0.8857 -- iter: 32/36\n",
            "Training Step: 4795  | total loss: \u001b[1m\u001b[32m0.76246\u001b[0m\u001b[0m | time: 0.024s\n",
            "| Adam | epoch: 959 | loss: 0.76246 - acc: 0.8972 -- iter: 36/36\n",
            "--\n",
            "Training Step: 4796  | total loss: \u001b[1m\u001b[32m0.76246\u001b[0m\u001b[0m | time: 0.004s\n",
            "| Adam | epoch: 960 | loss: 0.76246 - acc: 0.9075 -- iter: 08/36\n",
            "Training Step: 4797  | total loss: \u001b[1m\u001b[32m1.35771\u001b[0m\u001b[0m | time: 0.009s\n",
            "| Adam | epoch: 960 | loss: 1.35771 - acc: 0.9167 -- iter: 16/36\n",
            "Training Step: 4798  | total loss: \u001b[1m\u001b[32m1.22827\u001b[0m\u001b[0m | time: 0.014s\n",
            "| Adam | epoch: 960 | loss: 1.22827 - acc: 0.8375 -- iter: 24/36\n",
            "Training Step: 4799  | total loss: \u001b[1m\u001b[32m1.11244\u001b[0m\u001b[0m | time: 0.018s\n",
            "| Adam | epoch: 960 | loss: 1.11244 - acc: 0.8538 -- iter: 32/36\n",
            "Training Step: 4800  | total loss: \u001b[1m\u001b[32m1.00824\u001b[0m\u001b[0m | time: 0.023s\n",
            "| Adam | epoch: 960 | loss: 1.00824 - acc: 0.8684 -- iter: 36/36\n",
            "--\n",
            "Training Step: 4801  | total loss: \u001b[1m\u001b[32m0.91571\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 961 | loss: 0.91571 - acc: 0.8816 -- iter: 08/36\n",
            "Training Step: 4802  | total loss: \u001b[1m\u001b[32m0.83410\u001b[0m\u001b[0m | time: 0.011s\n",
            "| Adam | epoch: 961 | loss: 0.83410 - acc: 0.8934 -- iter: 16/36\n",
            "Training Step: 4803  | total loss: \u001b[1m\u001b[32m1.53821\u001b[0m\u001b[0m | time: 0.016s\n",
            "| Adam | epoch: 961 | loss: 1.53821 - acc: 0.9041 -- iter: 24/36\n",
            "Training Step: 4804  | total loss: \u001b[1m\u001b[32m1.39114\u001b[0m\u001b[0m | time: 0.022s\n",
            "| Adam | epoch: 961 | loss: 1.39114 - acc: 0.8137 -- iter: 32/36\n",
            "Training Step: 4805  | total loss: \u001b[1m\u001b[32m1.39114\u001b[0m\u001b[0m | time: 0.027s\n",
            "| Adam | epoch: 961 | loss: 1.39114 - acc: 0.8323 -- iter: 36/36\n",
            "--\n",
            "Training Step: 4806  | total loss: \u001b[1m\u001b[32m1.26110\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 962 | loss: 1.26110 - acc: 0.8491 -- iter: 08/36\n",
            "Training Step: 4807  | total loss: \u001b[1m\u001b[32m1.14413\u001b[0m\u001b[0m | time: 0.010s\n",
            "| Adam | epoch: 962 | loss: 1.14413 - acc: 0.8642 -- iter: 16/36\n",
            "Training Step: 4808  | total loss: \u001b[1m\u001b[32m0.94482\u001b[0m\u001b[0m | time: 0.015s\n",
            "| Adam | epoch: 962 | loss: 0.94482 - acc: 0.8900 -- iter: 24/36\n",
            "Training Step: 4809  | total loss: \u001b[1m\u001b[32m1.31123\u001b[0m\u001b[0m | time: 0.019s\n",
            "| Adam | epoch: 962 | loss: 1.31123 - acc: 0.8260 -- iter: 32/36\n",
            "Training Step: 4810  | total loss: \u001b[1m\u001b[32m1.18747\u001b[0m\u001b[0m | time: 0.024s\n",
            "| Adam | epoch: 962 | loss: 1.18747 - acc: 0.8434 -- iter: 36/36\n",
            "--\n",
            "Training Step: 4811  | total loss: \u001b[1m\u001b[32m1.07658\u001b[0m\u001b[0m | time: 0.006s\n",
            "| Adam | epoch: 963 | loss: 1.07658 - acc: 0.8590 -- iter: 08/36\n",
            "Training Step: 4812  | total loss: \u001b[1m\u001b[32m0.97686\u001b[0m\u001b[0m | time: 0.010s\n",
            "| Adam | epoch: 963 | loss: 0.97686 - acc: 0.8731 -- iter: 16/36\n",
            "Training Step: 4813  | total loss: \u001b[1m\u001b[32m0.88921\u001b[0m\u001b[0m | time: 0.015s\n",
            "| Adam | epoch: 963 | loss: 0.88921 - acc: 0.8858 -- iter: 24/36\n",
            "Training Step: 4814  | total loss: \u001b[1m\u001b[32m0.80791\u001b[0m\u001b[0m | time: 0.023s\n",
            "| Adam | epoch: 963 | loss: 0.80791 - acc: 0.8858 -- iter: 32/36\n",
            "Training Step: 4815  | total loss: \u001b[1m\u001b[32m1.23605\u001b[0m\u001b[0m | time: 0.028s\n",
            "| Adam | epoch: 963 | loss: 1.23605 - acc: 0.8972 -- iter: 36/36\n",
            "--\n",
            "Training Step: 4816  | total loss: \u001b[1m\u001b[32m1.12043\u001b[0m\u001b[0m | time: 0.006s\n",
            "| Adam | epoch: 964 | loss: 1.12043 - acc: 0.8450 -- iter: 08/36\n",
            "Training Step: 4817  | total loss: \u001b[1m\u001b[32m1.01833\u001b[0m\u001b[0m | time: 0.010s\n",
            "| Adam | epoch: 964 | loss: 1.01833 - acc: 0.8605 -- iter: 16/36\n",
            "Training Step: 4818  | total loss: \u001b[1m\u001b[32m0.92654\u001b[0m\u001b[0m | time: 0.018s\n",
            "| Adam | epoch: 964 | loss: 0.92654 - acc: 0.8745 -- iter: 24/36\n",
            "Training Step: 4819  | total loss: \u001b[1m\u001b[32m0.84390\u001b[0m\u001b[0m | time: 0.024s\n",
            "| Adam | epoch: 964 | loss: 0.84390 - acc: 0.8870 -- iter: 32/36\n",
            "Training Step: 4820  | total loss: \u001b[1m\u001b[32m0.76924\u001b[0m\u001b[0m | time: 0.029s\n",
            "| Adam | epoch: 964 | loss: 0.76924 - acc: 0.8983 -- iter: 36/36\n",
            "--\n",
            "Training Step: 4821  | total loss: \u001b[1m\u001b[32m1.38226\u001b[0m\u001b[0m | time: 0.006s\n",
            "| Adam | epoch: 965 | loss: 1.38226 - acc: 0.9085 -- iter: 08/36\n",
            "Training Step: 4822  | total loss: \u001b[1m\u001b[32m1.25285\u001b[0m\u001b[0m | time: 0.012s\n",
            "| Adam | epoch: 965 | loss: 1.25285 - acc: 0.8426 -- iter: 16/36\n",
            "Training Step: 4823  | total loss: \u001b[1m\u001b[32m1.13550\u001b[0m\u001b[0m | time: 0.018s\n",
            "| Adam | epoch: 965 | loss: 1.13550 - acc: 0.8584 -- iter: 24/36\n",
            "Training Step: 4824  | total loss: \u001b[1m\u001b[32m1.13550\u001b[0m\u001b[0m | time: 0.025s\n",
            "| Adam | epoch: 965 | loss: 1.13550 - acc: 0.8725 -- iter: 32/36\n",
            "Training Step: 4825  | total loss: \u001b[1m\u001b[32m1.02995\u001b[0m\u001b[0m | time: 0.031s\n",
            "| Adam | epoch: 965 | loss: 1.02995 - acc: 0.8853 -- iter: 36/36\n",
            "--\n",
            "Training Step: 4826  | total loss: \u001b[1m\u001b[32m0.85385\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 966 | loss: 0.85385 - acc: 0.8968 -- iter: 08/36\n",
            "Training Step: 4827  | total loss: \u001b[1m\u001b[32m1.20113\u001b[0m\u001b[0m | time: 0.010s\n",
            "| Adam | epoch: 966 | loss: 1.20113 - acc: 0.9071 -- iter: 16/36\n",
            "Training Step: 4828  | total loss: \u001b[1m\u001b[32m1.20113\u001b[0m\u001b[0m | time: 0.015s\n",
            "| Adam | epoch: 966 | loss: 1.20113 - acc: 0.8539 -- iter: 24/36\n",
            "Training Step: 4829  | total loss: \u001b[1m\u001b[32m1.09115\u001b[0m\u001b[0m | time: 0.021s\n",
            "| Adam | epoch: 966 | loss: 1.09115 - acc: 0.8685 -- iter: 32/36\n",
            "Training Step: 4830  | total loss: \u001b[1m\u001b[32m0.90498\u001b[0m\u001b[0m | time: 0.027s\n",
            "| Adam | epoch: 966 | loss: 0.90498 - acc: 0.8816 -- iter: 36/36\n",
            "--\n",
            "Training Step: 4831  | total loss: \u001b[1m\u001b[32m0.82513\u001b[0m\u001b[0m | time: 0.007s\n",
            "| Adam | epoch: 967 | loss: 0.82513 - acc: 0.8935 -- iter: 08/36\n",
            "Training Step: 4832  | total loss: \u001b[1m\u001b[32m0.74944\u001b[0m\u001b[0m | time: 0.012s\n",
            "| Adam | epoch: 967 | loss: 0.74944 - acc: 0.9041 -- iter: 16/36\n",
            "Training Step: 4833  | total loss: \u001b[1m\u001b[32m0.68543\u001b[0m\u001b[0m | time: 0.020s\n",
            "| Adam | epoch: 967 | loss: 0.68543 - acc: 0.9137 -- iter: 24/36\n",
            "Training Step: 4834  | total loss: \u001b[1m\u001b[32m0.62617\u001b[0m\u001b[0m | time: 0.027s\n",
            "| Adam | epoch: 967 | loss: 0.62617 - acc: 0.9223 -- iter: 32/36\n",
            "Training Step: 4835  | total loss: \u001b[1m\u001b[32m0.57328\u001b[0m\u001b[0m | time: 0.035s\n",
            "| Adam | epoch: 967 | loss: 0.57328 - acc: 0.9301 -- iter: 36/36\n",
            "--\n",
            "Training Step: 4836  | total loss: \u001b[1m\u001b[32m0.52563\u001b[0m\u001b[0m | time: 0.008s\n",
            "| Adam | epoch: 968 | loss: 0.52563 - acc: 0.9371 -- iter: 08/36\n",
            "Training Step: 4837  | total loss: \u001b[1m\u001b[32m0.48422\u001b[0m\u001b[0m | time: 0.014s\n",
            "| Adam | epoch: 968 | loss: 0.48422 - acc: 0.9434 -- iter: 16/36\n",
            "Training Step: 4838  | total loss: \u001b[1m\u001b[32m0.44506\u001b[0m\u001b[0m | time: 0.018s\n",
            "| Adam | epoch: 968 | loss: 0.44506 - acc: 0.9490 -- iter: 24/36\n",
            "Training Step: 4839  | total loss: \u001b[1m\u001b[32m1.09803\u001b[0m\u001b[0m | time: 0.028s\n",
            "| Adam | epoch: 968 | loss: 1.09803 - acc: 0.9541 -- iter: 32/36\n",
            "Training Step: 4840  | total loss: \u001b[1m\u001b[32m1.00069\u001b[0m\u001b[0m | time: 0.036s\n",
            "| Adam | epoch: 968 | loss: 1.00069 - acc: 0.8587 -- iter: 36/36\n",
            "--\n",
            "Training Step: 4841  | total loss: \u001b[1m\u001b[32m0.90835\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 969 | loss: 0.90835 - acc: 0.8729 -- iter: 08/36\n",
            "Training Step: 4842  | total loss: \u001b[1m\u001b[32m0.82528\u001b[0m\u001b[0m | time: 0.012s\n",
            "| Adam | epoch: 969 | loss: 0.82528 - acc: 0.8856 -- iter: 16/36\n",
            "Training Step: 4843  | total loss: \u001b[1m\u001b[32m0.82528\u001b[0m\u001b[0m | time: 0.018s\n",
            "| Adam | epoch: 969 | loss: 0.82528 - acc: 0.8970 -- iter: 24/36\n",
            "Training Step: 4844  | total loss: \u001b[1m\u001b[32m0.68668\u001b[0m\u001b[0m | time: 0.026s\n",
            "| Adam | epoch: 969 | loss: 0.68668 - acc: 0.9073 -- iter: 32/36\n",
            "Training Step: 4845  | total loss: \u001b[1m\u001b[32m1.29300\u001b[0m\u001b[0m | time: 0.031s\n",
            "| Adam | epoch: 969 | loss: 1.29300 - acc: 0.9166 -- iter: 36/36\n",
            "--\n",
            "Training Step: 4846  | total loss: \u001b[1m\u001b[32m1.17299\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 970 | loss: 1.17299 - acc: 0.8374 -- iter: 08/36\n",
            "Training Step: 4847  | total loss: \u001b[1m\u001b[32m1.17299\u001b[0m\u001b[0m | time: 0.010s\n",
            "| Adam | epoch: 970 | loss: 1.17299 - acc: 0.8537 -- iter: 16/36\n",
            "Training Step: 4848  | total loss: \u001b[1m\u001b[32m0.96722\u001b[0m\u001b[0m | time: 0.015s\n",
            "| Adam | epoch: 970 | loss: 0.96722 - acc: 0.8683 -- iter: 24/36\n",
            "Training Step: 4849  | total loss: \u001b[1m\u001b[32m0.88387\u001b[0m\u001b[0m | time: 0.023s\n",
            "| Adam | epoch: 970 | loss: 0.88387 - acc: 0.8815 -- iter: 32/36\n",
            "Training Step: 4850  | total loss: \u001b[1m\u001b[32m0.80382\u001b[0m\u001b[0m | time: 0.029s\n",
            "| Adam | epoch: 970 | loss: 0.80382 - acc: 0.8933 -- iter: 36/36\n",
            "--\n",
            "Training Step: 4851  | total loss: \u001b[1m\u001b[32m1.56817\u001b[0m\u001b[0m | time: 0.004s\n",
            "| Adam | epoch: 971 | loss: 1.56817 - acc: 0.9040 -- iter: 08/36\n",
            "Training Step: 4852  | total loss: \u001b[1m\u001b[32m1.42300\u001b[0m\u001b[0m | time: 0.009s\n",
            "| Adam | epoch: 971 | loss: 1.42300 - acc: 0.8261 -- iter: 16/36\n",
            "Training Step: 4853  | total loss: \u001b[1m\u001b[32m1.29008\u001b[0m\u001b[0m | time: 0.015s\n",
            "| Adam | epoch: 971 | loss: 1.29008 - acc: 0.8435 -- iter: 24/36\n",
            "Training Step: 4854  | total loss: \u001b[1m\u001b[32m1.17056\u001b[0m\u001b[0m | time: 0.021s\n",
            "| Adam | epoch: 971 | loss: 1.17056 - acc: 0.8591 -- iter: 32/36\n",
            "Training Step: 4855  | total loss: \u001b[1m\u001b[32m1.06584\u001b[0m\u001b[0m | time: 0.027s\n",
            "| Adam | epoch: 971 | loss: 1.06584 - acc: 0.8732 -- iter: 36/36\n",
            "--\n",
            "Training Step: 4856  | total loss: \u001b[1m\u001b[32m1.06584\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 972 | loss: 1.06584 - acc: 0.8859 -- iter: 08/36\n",
            "Training Step: 4857  | total loss: \u001b[1m\u001b[32m0.96776\u001b[0m\u001b[0m | time: 0.011s\n",
            "| Adam | epoch: 972 | loss: 0.96776 - acc: 0.8973 -- iter: 16/36\n",
            "Training Step: 4858  | total loss: \u001b[1m\u001b[32m1.38045\u001b[0m\u001b[0m | time: 0.016s\n",
            "| Adam | epoch: 972 | loss: 1.38045 - acc: 0.8326 -- iter: 24/36\n",
            "Training Step: 4859  | total loss: \u001b[1m\u001b[32m1.25268\u001b[0m\u001b[0m | time: 0.021s\n",
            "| Adam | epoch: 972 | loss: 1.25268 - acc: 0.8493 -- iter: 32/36\n",
            "Training Step: 4860  | total loss: \u001b[1m\u001b[32m1.03208\u001b[0m\u001b[0m | time: 0.026s\n",
            "| Adam | epoch: 972 | loss: 1.03208 - acc: 0.8644 -- iter: 36/36\n",
            "--\n",
            "Training Step: 4861  | total loss: \u001b[1m\u001b[32m0.94080\u001b[0m\u001b[0m | time: 0.006s\n",
            "| Adam | epoch: 973 | loss: 0.94080 - acc: 0.8780 -- iter: 08/36\n",
            "Training Step: 4862  | total loss: \u001b[1m\u001b[32m0.85711\u001b[0m\u001b[0m | time: 0.014s\n",
            "| Adam | epoch: 973 | loss: 0.85711 - acc: 0.8902 -- iter: 16/36\n",
            "Training Step: 4863  | total loss: \u001b[1m\u001b[32m1.40424\u001b[0m\u001b[0m | time: 0.021s\n",
            "| Adam | epoch: 973 | loss: 1.40424 - acc: 0.9011 -- iter: 24/36\n",
            "Training Step: 4864  | total loss: \u001b[1m\u001b[32m1.27695\u001b[0m\u001b[0m | time: 0.027s\n",
            "| Adam | epoch: 973 | loss: 1.27695 - acc: 0.8235 -- iter: 32/36\n",
            "Training Step: 4865  | total loss: \u001b[1m\u001b[32m1.16034\u001b[0m\u001b[0m | time: 0.034s\n",
            "| Adam | epoch: 973 | loss: 1.16034 - acc: 0.8412 -- iter: 36/36\n",
            "--\n",
            "Training Step: 4866  | total loss: \u001b[1m\u001b[32m1.05552\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 974 | loss: 1.05552 - acc: 0.8571 -- iter: 08/36\n",
            "Training Step: 4867  | total loss: \u001b[1m\u001b[32m0.96298\u001b[0m\u001b[0m | time: 0.010s\n",
            "| Adam | epoch: 974 | loss: 0.96298 - acc: 0.8714 -- iter: 16/36\n",
            "Training Step: 4868  | total loss: \u001b[1m\u001b[32m0.87776\u001b[0m\u001b[0m | time: 0.016s\n",
            "| Adam | epoch: 974 | loss: 0.87776 - acc: 0.8842 -- iter: 24/36\n",
            "Training Step: 4869  | total loss: \u001b[1m\u001b[32m1.50494\u001b[0m\u001b[0m | time: 0.021s\n",
            "| Adam | epoch: 974 | loss: 1.50494 - acc: 0.8958 -- iter: 32/36\n",
            "Training Step: 4870  | total loss: \u001b[1m\u001b[32m1.36627\u001b[0m\u001b[0m | time: 0.028s\n",
            "| Adam | epoch: 974 | loss: 1.36627 - acc: 0.8062 -- iter: 36/36\n",
            "--\n",
            "Training Step: 4871  | total loss: \u001b[1m\u001b[32m1.23911\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 975 | loss: 1.23911 - acc: 0.8256 -- iter: 08/36\n",
            "Training Step: 4872  | total loss: \u001b[1m\u001b[32m1.12467\u001b[0m\u001b[0m | time: 0.011s\n",
            "| Adam | epoch: 975 | loss: 1.12467 - acc: 0.8430 -- iter: 16/36\n",
            "Training Step: 4873  | total loss: \u001b[1m\u001b[32m1.02292\u001b[0m\u001b[0m | time: 0.016s\n",
            "| Adam | epoch: 975 | loss: 1.02292 - acc: 0.8587 -- iter: 24/36\n",
            "Training Step: 4874  | total loss: \u001b[1m\u001b[32m0.93471\u001b[0m\u001b[0m | time: 0.022s\n",
            "| Adam | epoch: 975 | loss: 0.93471 - acc: 0.8729 -- iter: 32/36\n",
            "Training Step: 4875  | total loss: \u001b[1m\u001b[32m1.22748\u001b[0m\u001b[0m | time: 0.027s\n",
            "| Adam | epoch: 975 | loss: 1.22748 - acc: 0.8856 -- iter: 36/36\n",
            "--\n",
            "Training Step: 4876  | total loss: \u001b[1m\u001b[32m1.11702\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 976 | loss: 1.11702 - acc: 0.8345 -- iter: 08/36\n",
            "Training Step: 4877  | total loss: \u001b[1m\u001b[32m1.01911\u001b[0m\u001b[0m | time: 0.010s\n",
            "| Adam | epoch: 976 | loss: 1.01911 - acc: 0.8511 -- iter: 16/36\n",
            "Training Step: 4878  | total loss: \u001b[1m\u001b[32m0.93105\u001b[0m\u001b[0m | time: 0.015s\n",
            "| Adam | epoch: 976 | loss: 0.93105 - acc: 0.8660 -- iter: 24/36\n",
            "Training Step: 4879  | total loss: \u001b[1m\u001b[32m0.84981\u001b[0m\u001b[0m | time: 0.019s\n",
            "| Adam | epoch: 976 | loss: 0.84981 - acc: 0.8794 -- iter: 32/36\n",
            "Training Step: 4880  | total loss: \u001b[1m\u001b[32m0.84981\u001b[0m\u001b[0m | time: 0.025s\n",
            "| Adam | epoch: 976 | loss: 0.84981 - acc: 0.8914 -- iter: 36/36\n",
            "--\n",
            "Training Step: 4881  | total loss: \u001b[1m\u001b[32m1.24589\u001b[0m\u001b[0m | time: 0.006s\n",
            "| Adam | epoch: 977 | loss: 1.24589 - acc: 0.9023 -- iter: 08/36\n",
            "Training Step: 4882  | total loss: \u001b[1m\u001b[32m1.13282\u001b[0m\u001b[0m | time: 0.014s\n",
            "| Adam | epoch: 977 | loss: 1.13282 - acc: 0.8246 -- iter: 16/36\n",
            "Training Step: 4883  | total loss: \u001b[1m\u001b[32m1.03298\u001b[0m\u001b[0m | time: 0.019s\n",
            "| Adam | epoch: 977 | loss: 1.03298 - acc: 0.8421 -- iter: 24/36\n",
            "Training Step: 4884  | total loss: \u001b[1m\u001b[32m0.94325\u001b[0m\u001b[0m | time: 0.023s\n",
            "| Adam | epoch: 977 | loss: 0.94325 - acc: 0.8579 -- iter: 32/36\n",
            "Training Step: 4885  | total loss: \u001b[1m\u001b[32m0.94325\u001b[0m\u001b[0m | time: 0.029s\n",
            "| Adam | epoch: 977 | loss: 0.94325 - acc: 0.8721 -- iter: 36/36\n",
            "--\n",
            "Training Step: 4886  | total loss: \u001b[1m\u001b[32m0.86079\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 978 | loss: 0.86079 - acc: 0.8849 -- iter: 08/36\n",
            "Training Step: 4887  | total loss: \u001b[1m\u001b[32m1.13896\u001b[0m\u001b[0m | time: 0.013s\n",
            "| Adam | epoch: 978 | loss: 1.13896 - acc: 0.8964 -- iter: 16/36\n",
            "Training Step: 4888  | total loss: \u001b[1m\u001b[32m1.03806\u001b[0m\u001b[0m | time: 0.018s\n",
            "| Adam | epoch: 978 | loss: 1.03806 - acc: 0.8318 -- iter: 24/36\n",
            "Training Step: 4889  | total loss: \u001b[1m\u001b[32m0.94566\u001b[0m\u001b[0m | time: 0.023s\n",
            "| Adam | epoch: 978 | loss: 0.94566 - acc: 0.8486 -- iter: 32/36\n",
            "Training Step: 4890  | total loss: \u001b[1m\u001b[32m0.86253\u001b[0m\u001b[0m | time: 0.028s\n",
            "| Adam | epoch: 978 | loss: 0.86253 - acc: 0.8637 -- iter: 36/36\n",
            "--\n",
            "Training Step: 4891  | total loss: \u001b[1m\u001b[32m0.78869\u001b[0m\u001b[0m | time: 0.006s\n",
            "| Adam | epoch: 979 | loss: 0.78869 - acc: 0.8774 -- iter: 08/36\n",
            "Training Step: 4892  | total loss: \u001b[1m\u001b[32m0.72462\u001b[0m\u001b[0m | time: 0.012s\n",
            "| Adam | epoch: 979 | loss: 0.72462 - acc: 0.8896 -- iter: 16/36\n",
            "Training Step: 4893  | total loss: \u001b[1m\u001b[32m0.66597\u001b[0m\u001b[0m | time: 0.018s\n",
            "| Adam | epoch: 979 | loss: 0.66597 - acc: 0.9007 -- iter: 24/36\n",
            "Training Step: 4894  | total loss: \u001b[1m\u001b[32m0.61223\u001b[0m\u001b[0m | time: 0.023s\n",
            "| Adam | epoch: 979 | loss: 0.61223 - acc: 0.9106 -- iter: 32/36\n",
            "Training Step: 4895  | total loss: \u001b[1m\u001b[32m0.61223\u001b[0m\u001b[0m | time: 0.030s\n",
            "| Adam | epoch: 979 | loss: 0.61223 - acc: 0.9195 -- iter: 36/36\n",
            "--\n",
            "Training Step: 4896  | total loss: \u001b[1m\u001b[32m0.53171\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 980 | loss: 0.53171 - acc: 0.9276 -- iter: 08/36\n",
            "Training Step: 4897  | total loss: \u001b[1m\u001b[32m0.53171\u001b[0m\u001b[0m | time: 0.012s\n",
            "| Adam | epoch: 980 | loss: 0.53171 - acc: 0.9348 -- iter: 16/36\n",
            "Training Step: 4898  | total loss: \u001b[1m\u001b[32m0.49135\u001b[0m\u001b[0m | time: 0.019s\n",
            "| Adam | epoch: 980 | loss: 0.49135 - acc: 0.9413 -- iter: 24/36\n",
            "Training Step: 4899  | total loss: \u001b[1m\u001b[32m0.45467\u001b[0m\u001b[0m | time: 0.025s\n",
            "| Adam | epoch: 980 | loss: 0.45467 - acc: 0.9472 -- iter: 32/36\n",
            "Training Step: 4900  | total loss: \u001b[1m\u001b[32m0.98734\u001b[0m\u001b[0m | time: 0.030s\n",
            "| Adam | epoch: 980 | loss: 0.98734 - acc: 0.8650 -- iter: 36/36\n",
            "--\n",
            "Training Step: 4901  | total loss: \u001b[1m\u001b[32m0.90110\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 981 | loss: 0.90110 - acc: 0.8785 -- iter: 08/36\n",
            "Training Step: 4902  | total loss: \u001b[1m\u001b[32m0.82008\u001b[0m\u001b[0m | time: 0.010s\n",
            "| Adam | epoch: 981 | loss: 0.82008 - acc: 0.8906 -- iter: 16/36\n",
            "Training Step: 4903  | total loss: \u001b[1m\u001b[32m0.74722\u001b[0m\u001b[0m | time: 0.015s\n",
            "| Adam | epoch: 981 | loss: 0.74722 - acc: 0.9016 -- iter: 24/36\n",
            "Training Step: 4904  | total loss: \u001b[1m\u001b[32m0.63264\u001b[0m\u001b[0m | time: 0.020s\n",
            "| Adam | epoch: 981 | loss: 0.63264 - acc: 0.9114 -- iter: 32/36\n",
            "Training Step: 4905  | total loss: \u001b[1m\u001b[32m1.07148\u001b[0m\u001b[0m | time: 0.023s\n",
            "| Adam | epoch: 981 | loss: 1.07148 - acc: 0.9203 -- iter: 36/36\n",
            "--\n",
            "Training Step: 4906  | total loss: \u001b[1m\u001b[32m0.97481\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 982 | loss: 0.97481 - acc: 0.8407 -- iter: 08/36\n",
            "Training Step: 4907  | total loss: \u001b[1m\u001b[32m0.88973\u001b[0m\u001b[0m | time: 0.009s\n",
            "| Adam | epoch: 982 | loss: 0.88973 - acc: 0.8567 -- iter: 16/36\n",
            "Training Step: 4908  | total loss: \u001b[1m\u001b[32m0.81314\u001b[0m\u001b[0m | time: 0.016s\n",
            "| Adam | epoch: 982 | loss: 0.81314 - acc: 0.8710 -- iter: 24/36\n",
            "Training Step: 4909  | total loss: \u001b[1m\u001b[32m0.75084\u001b[0m\u001b[0m | time: 0.022s\n",
            "| Adam | epoch: 982 | loss: 0.75084 - acc: 0.8839 -- iter: 32/36\n",
            "Training Step: 4910  | total loss: \u001b[1m\u001b[32m0.69059\u001b[0m\u001b[0m | time: 0.027s\n",
            "| Adam | epoch: 982 | loss: 0.69059 - acc: 0.8955 -- iter: 36/36\n",
            "--\n",
            "Training Step: 4911  | total loss: \u001b[1m\u001b[32m1.12575\u001b[0m\u001b[0m | time: 0.006s\n",
            "| Adam | epoch: 983 | loss: 1.12575 - acc: 0.9060 -- iter: 08/36\n",
            "Training Step: 4912  | total loss: \u001b[1m\u001b[32m1.02788\u001b[0m\u001b[0m | time: 0.011s\n",
            "| Adam | epoch: 983 | loss: 1.02788 - acc: 0.8279 -- iter: 16/36\n",
            "Training Step: 4913  | total loss: \u001b[1m\u001b[32m0.93576\u001b[0m\u001b[0m | time: 0.016s\n",
            "| Adam | epoch: 983 | loss: 0.93576 - acc: 0.8451 -- iter: 24/36\n",
            "Training Step: 4914  | total loss: \u001b[1m\u001b[32m0.85291\u001b[0m\u001b[0m | time: 0.021s\n",
            "| Adam | epoch: 983 | loss: 0.85291 - acc: 0.8606 -- iter: 32/36\n",
            "Training Step: 4915  | total loss: \u001b[1m\u001b[32m0.77747\u001b[0m\u001b[0m | time: 0.030s\n",
            "| Adam | epoch: 983 | loss: 0.77747 - acc: 0.8745 -- iter: 36/36\n",
            "--\n",
            "Training Step: 4916  | total loss: \u001b[1m\u001b[32m0.71962\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 984 | loss: 0.71962 - acc: 0.8871 -- iter: 08/36\n",
            "Training Step: 4917  | total loss: \u001b[1m\u001b[32m1.24550\u001b[0m\u001b[0m | time: 0.010s\n",
            "| Adam | epoch: 984 | loss: 1.24550 - acc: 0.8984 -- iter: 16/36\n",
            "Training Step: 4918  | total loss: \u001b[1m\u001b[32m1.13563\u001b[0m\u001b[0m | time: 0.016s\n",
            "| Adam | epoch: 984 | loss: 1.13563 - acc: 0.8335 -- iter: 24/36\n"
          ]
        }
      ],
      "source": [
        "model.fit(training, output, n_epoch=1000, batch_size=8, show_metric=True)\n",
        "model.save(\"model.tflearn\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IERxrGzkdvKI"
      },
      "source": [
        "Prediction"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zpAe3X9Edn-c",
        "outputId": "24851b73-7dc1-4ac5-e79a-9caae9bf32be"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Start talking with the bot\n",
            "Bot: Sorry, I could not understand.\n",
            "Bot: Here is the list of required documents to apply for home loan: 1. 3 photographs passport sized 2. Identify proof 3. Residence proof 4. Bank Account Statement/Passbook for last 6 months 5. Signature verification by bankers of the applicant 6. Liabilities statement and Personal Assets 7. Property detailed documents.\n",
            "User: bro movie\n",
            "Bot: Sorry, I could not understand.\n"
          ]
        }
      ],
      "source": [
        "def bag_of_words(s, words):\n",
        "    bag = [0 for _ in range(len(words))]\n",
        "\n",
        "    s_words = nltk.word_tokenize(s)\n",
        "    s_words = [stemmer.stem(word.lower()) for word in s_words]\n",
        "\n",
        "    for se in s_words:\n",
        "        for i, w in enumerate(words):\n",
        "            if w == se:\n",
        "                bag[i] = 1\n",
        "\n",
        "    return numpy.array(bag)\n",
        "\n",
        "def chat():\n",
        "    print(\"Start talking with the bot\")\n",
        "    while True:\n",
        "        inp = input(\"User: \")\n",
        "        if inp.lower() in [\"bye\", \"quit\", \"exit\"]:\n",
        "            break\n",
        "\n",
        "        results = model.predict([bag_of_words(inp, words)])\n",
        "        results_index = numpy.argmax(results)\n",
        "        tag = labels[results_index]\n",
        "\n",
        "        if results[0][results_index] > 0.7:  # Adjust the confidence threshold as needed\n",
        "            for tg in data[\"intents\"]:\n",
        "                if tg['tag'] == tag:\n",
        "                    responses = tg['responses']\n",
        "            print(\"Bot: \" + random.choice(responses))\n",
        "        else:\n",
        "            print(\"Bot: Sorry, I could not understand.\")\n",
        "\n",
        "chat()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UKCpDc2f7Btb"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BAk6SoVCKOmr"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}